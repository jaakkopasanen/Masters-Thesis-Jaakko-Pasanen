\documentclass[12pt,a4paper,english
% ,twoside,openright
]{tutthesis}
%\documentclass[12pt,a4paper,finnish]{tutthesis}

% Note that you must choose either Finnish or English here and there in this
% file.
% Other options for document class
  % ,twoside,openright   % If printing on both sides (>80 pages)
  % ,twocolumn           % Can be used in lab reports, not in theses

% Ensure the correct Pdf size (not needed in all environments)
\special{papersize=210mm,297mm}


% LaTeX file for BSC/MSc theses and lab reports.
% Requires the class file (=template) tutthesis.cls and figure files,
% either tut-logo, exampleFig (as pdf or eps) and example_code.c
% Author: Sami Paavilainen (2006)
% Modified: Heikki Huttunen (heikki.huttunen@tut.fi) 31.7.2012.
%           Erno Salminen, @tut.fi, 2014-08-15
%             - added text snippets from the writing guide
%             - added lots of comments: both tips and alternative styles
%             - added an example table
%             - and so on...

%
% Define your basic information
%
\author{Jaakko Pasanen}
\title{Natural Language Syntactic Parsing with Deep Learning} % primary title (for front page)
\titleB{Luonnollisen kielen syntaksin parsiminen syväoppimisella}     % translated title for abstract
\thesistype{Master of Science thesis} % or Bachelor of Science, Laboratory Report... 
\examiner{Ari Visa} % without title Prof., Dr., MSc or such

% Put your thesis' main language last
% http://mirrors.ctan.org/macros/latex/required/babel/base/babel.pdf
\usepackage[finnish, main=english]{babel}

% http://www.ctan.org/pkg/biblatex
\usepackage[
  style=authoryear,
  maxcitenames=2,
  backend=biber,
  firstinits=true
]{biblatex}
\bibliography{thesis_refs.bib}
%% Note that option style=numeric works as well
\usepackage{fontspec}
\usepackage{amsfonts}


% You can also add your own commands
\newcommand\todo[1]{{\color{red}!!!TODO: #1}} % Remark text in braces appears in red
\newcommand{\angs}{\textsl{\AA}}              % , e.g. slanted symbol for Ångstöm

% Preparatory content ends here


\pagenumbering{roman} % was: {Roman}
\pagestyle{headings}
\begin{document}

% Special trick so that internal macros (denoted with @ in their name)
% can be used outside the cls file (e.g. \@author)
\makeatletter

%
% Create the title page.
% First the logo. Check its language.
\thispagestyle{empty}
\vspace*{-.5cm}\noindent
\includegraphics[width=8cm]{tty_tut_logo}   % Bilingual logo

% Then lay out the author, title and type to the center of page.
\vspace{6.8cm}
\maketitle
\vspace{6.7cm} % -> 6.7cm if thesis title needs two lines

% Last some additional info to the bottom-right corner
\begin{flushright}  
  \begin{minipage}[c]{6.8cm}
    \begin{spacing}{1.0}
      %\textsf{Tarkastaja: Prof. \@examiner}\\
      %\textsf{Tarkastaja ja aihe hyväksytty}\\ 
      %\textsf{xxxxxxx tiedekuntaneuvoston}\\
      %\textsf{kokouksessa dd.mm.yyyy}\\
      \textsf{Examiner: Prof. \@examiner}\\
      \textsf{Examiner and topic approved by the}\\ 
      \textsf{Faculty Council of the Faculty of}\\
      \textsf{Engineering Sciences}\\
      \textsf{on 31st December 2017}\\
    \end{spacing}
  \end{minipage}
\end{flushright}

% Leave the backside of title page empty in twoside mode
\if@twoside
\clearpage
\fi

%
% Use Roman numbering I,II,III... for the first pages (abstract, TOC,
% termlist etc)
\pagenumbering{Roman} 
\setcounter{page}{0} % Start numbering from zero because command 'chapter*' does page break

% Some fields in abstract are automated, namely those with \@ (author,
% title in the main language, thesis type, examiner).
\chapter*{Abstract}

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@title\\   % use \@titleB when thesis is in Finnish
         \textsf{Tampere University of Technology}\\
         \textsf{\@thesistype, xx pages, x Appendix pages} \\
         \textsf{December 2016}\\
         \textsf{Master's Degree Programme in Automation Technology}\\
         \textsf{Major: Learning and Intelligent Systems}\\
         \textsf{Examiner: Prof. \@examiner}\\ % 
         \textsf{Keywords: Hype}\\
\end{spacing}


The abstract is a concise 1-page description of the work: what was the
problem, what was done, and what are the results. Do not include
charts or tables in the abstract.

Put the abstract in the primary language of your thesis first and then
the translation (when that is needed).


% Foreign students do not need Fininsh abstract (tiivistelmä). Move
% this before English abstract if thesis is in Finnish. Move also the
% otherlanguage command to the English abstract (if needed).

\begin{otherlanguage}{finnish} %  Following text in in 2nd language
\chapter*{Tiivistelmä} % Asterisk * turns numbering off

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@titleB\\  % or use \@title when thesis is in Finnish
         \textsf{Tampereen teknillinen yliopisto}\\
         \textsf{Diplomityö, xx sivua, x liitesivua}\\ %
         \textsf{Joulukuu 2016}\\
         \textsf{Automaatiotekniikan koulutusohjelma}\\
         \textsf{Pääaine: Oppivat ja älykkäät järjestelmät}\\
         \textsf{Tarkastajat:  Prof. \@examiner}\\ % automated, if just 1 examiner
         \textsf{Avainsanat: Hype}\\
\end{spacing}

The abstract in Finnish. Foreign students do not need this page.

Suomenkieliseen diplomityöhön kirjoitetaan tiivistelmä sekä suomeksi
että englanniksi.

Kandidaatintyön tiivistelmä kirjoitetaan ainoastaan kerran, samalla
kielellä kuin työ. Kuitenkin myös suomenkielisillä kandidaatintöillä
pitää olla englanninkielinen otsikko arkistointia varten.

\end{otherlanguage} % End on 2nd language part


\chapter*{Preface}

This document template conforms to Guide to Writing a Thesis at
Tampere University of Technology (2014) and is based on the previous
template. The main purpose is to show how the theses are formatted
using LaTeX (or \LaTeX ~ to be extra fancy) .

The thesis text is written into file \texttt{d\_tyo.tex}, whereas
\texttt{tutthesis.cls} contains the formatting instructions. Both
files include lots of comments (start with \%) that should help in
using LaTeX. TUT specific formatting is done by additional settings on
top of the original \texttt{report.cls} class file. This example needs
few additional files: TUT logo, example figure, example code, as well
as example bibliography and its formatting (\texttt{.bst}) An example
makefile is provided for those preferring command line. You are
encouraged to comment your work and to keep the length of lines
moderate, e.g. <80 characters. In Emacs, you can use \texttt{Alt-Q} to
break long lines in a paragraph and \texttt{Tab} to indent commands
(e.g. inside figure and table environments). Moreover, tex files are
well suited for versioning systems, such as Subversion or Git.  
% \url{http://www.ctan.org/tex-archive/info/lshort/english/lshort.pdf}

Acknowledgements to those who contributed to the thesis are generally
presented in the preface. It is not appropriate to criticize anyone in
the preface, even though the preface will not affect your grade. The
preface must fit on one page. Add the date, after which you have not
made any revisions to the text, at the end of the preface.

~ 
% Tilde ~ makes an non-breakable spce in LaTeX. Here it is used to get
% two consecutive paragraph breaks

Tampere, 11.8.2014

~

On behalf of the working group, Erno Salminen


% Add the table of contents, optioanlly also the lists of figures,
% tables and codes.

%\renewcommand\contentsname{Sisällys} % Set Finnish name, remove this if using English
\setcounter{tocdepth}{3}              % How many header level are included
\tableofcontents                      % Create TOC


%
% Term and symbol exaplanations use a special list type
%
\chapter*{List of abbreviations and symbols}
\markboth{}{}                                % no headers
%\chapter*{Lyhenteet ja merkinnät}

% You don't have to align these with whitespaces, but it makes the
% .tex file more readable
\begin{termlist}
\item[ANN] Artificial Neural Network
\item[LAS] Labelled Attachment Score (\% of tokens with correct dependency head and relation)
\item[LDA] Latent Dirilecht Allocation
\item[LSA] Latent Semantic Analysis
\item[LSTM] Long short term memory; type of RNN with short term memory.
\item[NER] Named entity recognition
\item[NLP] Natural Language Processing
\item[PMI] Pointwise mutual information
\item[POS] Part-of-speech; also called lexical category
\item[RNN] Recurrent neural network
\item[S-LSTM] Stack long short term memory
\item[TUT] Tampere University of Technology
\item[UAS] Unlabelled Attachment Score (\% of tokens with correct dependency head)
\end{termlist}


\chapter*{Notes}

\section{Reading}
These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003). \cite{Pennington2014}

See first paragraph of section 1 of \cite{Liang2016a} for sources on machine translation

Encoder-decoder model


\section{Terms for Computational Linguistics}
\begin{description}
\item[1-of-V Coding] Representing words as sparse binary vectors which have 1 at the word's vocabulary index and 0 all others. With vocabulary \{dog, cat, mouse\}, dog becomes [1, 0, 0], cat becomes [0, 1, 0] and mouse [0, 0, 1].

\item[Bag-of-words] Multiset of words appearing in a text with occurrence counts for each word. Used as a tool for feature generation. Does not preserve word order or grammar. Can implemented as a dictionary (or associative array) where words are the keys and counts are the values.

\item[Conditional Random Field]

\item[Constituent] In syntactic analysis, a constituent is a word or a group of words that function(s) as a single unit within a hierarchical structure. Many constituents are phrases. \textit{Yesterday I saw \textbf{an orange bird with a white neck}}

\item[Corpus] A collection of texts with linguistic annotations.

\item[Dimensionality] When discussing word embeddings and word vector spaces the dimensionality refers to definition in linear algebra. Dimensionality of arrays in computing means the number of indices required to specify an element in the array. Word vector in 50 dimensional vector space $\mathbb{R}^{50}$ would be represented in computing as one dimensional array of length 50 [d1, d2, d3, ..., d50]

\item[Distributional Hypothesis] Words that are used and occur in the same contexts tend to purport similar meanings. \cite{Harris1954}

\item[Feature] Numeric data representation that can be effectively exploited in machine learning tasks. E.g. Word occurrence frequencies.

\item[Feature Vector] Vector containing all the features. For an image a feature vector could be all the raw values of pixels as a single sequence. For a trigram model with 300 dimensional word embeddings a feature vector would be a 900 dimensional vector formed by concatenating all the separate word embedding vectors.

\item[Gazzetteer] In Named Entity Recognition a gazzetteer is a dictionary of known named entities.

\item[Language Model] Probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $P(w_{1},\ldots ,w_{m})$ to the whole sequence. Problems caused by growing vocabulary can be addressed with continuous language models such as neural net language models (NNML). Word2Vec by \cite{Mikolov2013} addresses this problem with Continuous Bag-of-words and Skip-gram models.

\item[Lemmatisation] Process of finding the base form of a word, e.g. flew -> fly

\item[Lexeme] A basic lexical unit of a language consisting of one word or several words, the elements of which do not separately convey the meaning of the whole.

\item[n-gram] Probabilistic language model where probability of current word is the joint probability of previous \textit{n} words. Bigram example: $P(I, saw, the, red, house) \approx P(I | ^\wedge)P(saw | I)P(red | the)P(house | red)P(\$ | house)$. The words unigram, bigram and trigram language model denote n-gram model language models with n = 1, n = 2 and n = 3, respectively.

\item[One-hot] Group of bits which the legal comibations of values are only those with a single high (1) and all the others low (0). See also 1-of-V Coding.

\item[Parsing] Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.

\item[PMI] Word co-occurence probability metric. High values for words that occur often together.

\item[POS-tagging] Process of marking up a word to particular part-of-speech (nouns, verbs, etc...) based on both its definition and its context.

\item[ReLU] Rectified Linear Unit. $h(x) = max\{0, x\}$. Used as non-linear activation function in neural nets particularly in convolutional net

\item[Skip-gram] Language model which predicts the context (previous and next \textit{n} words) of a current word from the current word instead of traditional way of predicting current word from the context.

\item[Structured Prediction] Predicting structured objects, rather than scalar discrete or real values. Translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.

\item[Token] A structure representing a lexeme that explicitly indicates its categorization for the purpose of parsing. In plain words tokens are instances of words in a text. Not to be confused with word type.

\item[Tree bank] Parsed text corpus that annotates syntactic or semantic sentence structure. Contains trees for sentences where phrases in a sentence are structured in a tree of syntactic or semantic relations. Very useful for training POS-taggers etc...

\item[Tri-Training] Parsing unlabeled data with two different parses and selecting only the sentences for which the two parsers produce the same trees \cite{Weiss2015}

\item[Word Lookup Table] Matrix $\textbf{P} \in \mathbb{R}^{d \times |V|}$ of d rows and |V| columns, where d is the word vector dimensionality and |V| is the size of vocalbulary. I.e. each column represent a single word and each row represent single dimension in vector space.

\item[Word Type] Unique words in a text. \textit{Good wine is good} has 4 tokens but only 3 word types.

\item[Word vector] N-dimensional vector representation of a word with interesting properties such as: vector('Paris') - vector('France') + Vector('Italy') -> vector('Rome')
\end{description}

\section{Universal Dependecies}

\subsection{CoNNL-U format}
Universal dependencies use CoNNL-U format for treebanks, CoNNL-U is revised version of CoNNL-X. Annotations are encoded in text files with word lines, blank lines for sentence boundaries and comments starting with hash (\#).

Word lines consist of following columns:
\begin{termlist}
\item[ID] Word ID in sentence
\item[FORM] Word form or punctuation symbol
\item[LEMMA] Lemma or stem of word form
\item[UPOSTAG] Universal part-of-speech tag
\item[XPOSTAG] Language specific part-of-speech tag
\item[FEATS] List of morphological features
\item[HEAD] Head of the curren token, value of ID or zero (0)
\item[DEPREL] Universal dependecy relation to the HEAD
\item[DEPS] List of secondary dependencies
\item[MISC] Any other annotation
\end{termlist}

Example in Finnish: Jäällä kävely avaa aina hauskoja ja erikoisia näkökulmia kaupunkiin

\begin{tabular}{l l l l l} 
ID & FORM & LEMMA & UPOSTAG & XPOSTAG \\
\hline
1 & Jäällä & jää & NOUN & N \\
2 & kävely & kävely & NOUN & N \\
3 & avaa & avata & VERB & V \\
4 & aina & aina & ADV & Adv \\
5 & hauskoja & hauska & ADJ & A \\
6 & ja & ja & CONJ & C \\
7 & erikoisia & erikoinen & ADJ & A \\
8 & näkökulmia & näkö\#kulma & NOUN & N \\
9 & kaupunkiin & kaupunki & NOUN & N \\
10 & . & . & PUNCT & Punct
\end{tabular}

\begin{tabular}{l}
FEATS \\
\hline
Case=Ade|Number=Sing \\
Case=Nom|Number=Sing \\
Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
Case=Par|Number=Plur \\
Case=Ill|Number=Sing \\
\_
\end{tabular}

\begin{tabular}{l l l l}
HEAD & DEPREL & DEPS & MISC \\
\hline
2 & nmod & \_ & \_ \\
3 & nsubj & \_ & \_ \\
0 & root & \_ & \_ \\
3 & advmod & \_ & \_ \\
8 & amod & \_ & \_ \\
5 & cc & \_ & \_ \\
5 & conj & 8:amod & \_ \\
3 & dobj & \_ & \_ \\
8 & nmod & \_ & SpaceAfter=No
\end{tabular}

\subsection{Universal POS tags}
\begin{termlist}
\item[ADJ] Adjective. Describing word qualifying noun or noun phrase. \textbf{deep}, \textbf{intelligent}
\item[ADP] Adposition. Word expressing spatial or temporal relations \textbf{under}, \textbf{around}, \textbf{before} or mark various semantic roles \textbf{of}, \textbf{for}
\item[ADV] Adverb. Modifies another word. Typically express manner, place, time, frequency etc. She sang \textbf{loudly}. You are \textbf{quite} right.
\item[AUX] Auxiliary verb. A verb used in forming the tenses, moods, and voices of other verbs. \textbf{Do} you want tea?. He \textbf{has} given his all.
\item[CONJ] Coordinating conjunction. Conjunction placed between words, phrases, clauses or sentences of equal rank. \textbf{and}, \textbf{but}, \textbf{or}.
\item[DET] Determiner. Expresses reference of a noun (group). \textbf{The} girl is \textbf{a} student. \textbf{Which} book is that?
\item[INTJ] Interjection. Shows emotion or feeling of the author, includes exclamations, curses, greetings and such. \textbf{Ouch!}, \textbf{hey}, \textbf{huh?}.
\item[NOUN] Noun. Denotes a person, animal, place thing or idea. The \textbf{cat} sat on a \textbf{mat}.
\item[NUM] Numeral. Number, written with digits or letters. \textbf{12}, \textbf{eleven}.
\item[PART] Particle. Cannot be inflected. Interjections and conjunctions. In finnish also \textbf{että}, \textbf{jotta}, \textbf{koska}, \textbf{kun} etc...
\item[PRON] Pronoun. Replaces (often previously introduced) noun. Joe saw Jill, and \textbf{he} waved at \textbf{her}.
\item[PUNCT] Punctuation. Full stop, comma, bracket etc.
\item[SCONJ] Subordinating conjunction. A conjunction that introduces a subordinating clause, e.g. \textbf{although}, \textbf{because}, \textbf{whenever}.
\item[SYM] Symbol.
\item[VERB] Verb. Conveys an action \textbf{bring}, \textbf{read}, an occurrence \textbf{happen}, \textbf{become}, or a state of being \textbf{be}, \textbf{exist}.
\item[X] Other
\end{termlist}

\section{Machine Translation}
\label{se:machine_translation}
\begin{itemize}
\item See section 2 of \cite{Kestemont2016} for sources on Internet to standard language translitteration
\item NLP tools suffer with texts with a lot of ortographical variation, one solution is to translate them. \cite{Kestemont2016}
\end{itemize}


\section{Language Modeling}
\label{se:language_modeling}
\begin{itemize}
\item A language model is a probability distribution over a sequence of words, traditionally performed with n-th order Markov assumption or n-gram counting and smoothing (Chen and Goodman, 1998) \textit{see \cite{Kim2016} Introduction for citation.}
\item \cite{Kim2016} have character level encoding and word-level predicting model for language modeling.
\item \cite{Kim2016} noticed that character inputs are sufficient for language modeling.
\item Character level model of \cite{Kim2016} outperfom word-level and morpheme-level models on morphologically rich languages Arabic, Czech, French, German, Spanish and Russian.
\item Neural Language Models NLM are blind to sub-word level information (e.g. morphemes). \cite{Kim2016}
\item 
\end{itemize}

\section{Intent Recognition and Slot Detection}
\label{se:intetn_prediction_and_slot_detection}
\begin{itemize}
\item Intent prediction is determining user's intents from their utterances (messages). Intents can be seen as functions to call in traditional programming.
\item Slot detection is identifying relevant actionable pieces of utterance \cite{Bhargava2013}. Slots can be seen as function parameters.
\item \cite{Bhargava2013} reduce error rates of intent prediction by 6,7\% and 8.7\% for transcribed data and automatically recognized data respectively when using intents from previous messages.
\item \cite{Bhargava2013} find no significant difference for slot detection by using context information.
\item Performance of \cite{Bhargava2013} for intent prediction is increased from 97,1\% to 97.3\% on transcribed data. However they use Viterbi algorithm with full access to future of the dialog, not realistic in production.
\item Most systems assume a single intent per utterance leading to unnatural dialogue experience. \cite{Xu2013}
\item Approaching multi-intent recognition by selecting top-K hypotheses from a single intent classifier yields poor results. \cite{Xu2013}
\item Multi-label learning works better. Splitting into a K binary classifiers, or combining multiple labels into a single label classification problem. \cite{Xu2013}
\item Usually K is a system design choice. \cite{Xu2013}
\end{itemize}

\section{GloVe}
\begin{itemize}
\item GloVe by \cite{Pennington2014} capture global corpus statistics with log-bilinear co-occurence count model.
\item Memory requirements for GloVe are substantial since global co-occurence matrix for entire vocabulary is required, even though GloVe eliminates the need for zero occurence elements. Problem becomes worse for inflectional languages such as Finnish were vocabulary requires word type for each infliction for each word.
\item GloVe outperforms other methods on almost all tested tasks. All tasks are English only. \cite{Pennington2014}
\end{itemize}

\section{Deep Learning}

\section{Vanishing Gradients}
\begin{itemize}
\item Activation function with plateu will introduce gradients vanishing to zero
\item RNNs have vanishing gradients problem when sequences are long.
\end{itemize}

\section{Activation Functions}
\begin{itemize}
\item Sigmoid has a problem with vanishing gradients when using multiple hidden layers
\item Add sigmoid derivative plot for visual evidence.
\item ReLU fixes this problem (still has exploding gradients problem)
\item Softplus is the smooth version of ReLU but computationally more expensive.
\item ELU is like ReLU but doesn't die off to zero. \cite{Clevert2015}
\end{itemize}

\section{Recurrent Neural Networks}
\begin{itemize}
\item Multi-layer perceptrons cannot be used to map sequences to sequences since they require the dimensionality of inputs and outputs to be fixed. \cite{Sutskever2014}
\item RNN is suited for medling sequential phenomena. \cite{Kim2016}
\item RNN is a neural network architechture to which input sequence is fed one timestep at a time. RNN predicts output after each timestep and also feeds the output of previous timestep as an input in current iteration.
\item In theory RNN can summarize all historical information, but in practice vanilla RNN performs poorly with long sequences due to vanishing/exploding gradients. (Bengio, Simard and Frasconi 1994), \textit{see Model chapter of \cite{Kim2016}}
\item Long short-term memory (LSTM) networks address the problem of vanishing gradients with long sequences by adding a memory cell. (Hochreiter and Schmidhuber 1997), \textit{see Model chapter of \cite{Kim2016}}
\item LSTM architecture addresses the vanishing gradients problem by introducing internal memory to RNN cell which is updated on every timestep via forget and update gates.
\item On each timestep the LSTM forgets things from the sequence which it deems unnecessary and adds new relevant things from current example.
\item Gradient exploding remains a problem, but is easily addressable in practice by simple strategies such as gradient clipping. \cite{Kim2016}
\item Adding more layers such that input of a layer is the hidden state of previous layer is often crucial for significant performance increase. (Pascanu et al. 2013), \textit{see Model chapter of \cite{Kim2016}}
\item Deep LSTM of \cite{Sutskever2014} significantly outperformed their shallow LSTM, each layer decreasing the perplexity by nearly 10\%.
\item Bi-directional RNN is composed of two RNN of which the first reads the sequence in forward direction and the second reads the sequence in reverse direction, hidden states are concatenated. \cite{Chung2016}
\end{itemize}

\section{Encoder-Decoder}
\begin{itemize}
\item Introduced by \cite{Sutskever2014} and \cite{Cho2014}
\item Used in machine translation \cite{Chung2016}
\item Dual RNN architechture where 1st RNN encodes a sequence of tokens to fixed length vector and 2nd RNN decodeds that vector representation to a target sequence of tokens. \cite{Cho2014}, \cite{Bahdanau2014}, \cite{Sutskever2014}
\item Both RNNs are jointly trained to maximize conditional probability of a target sequence given a source sequence. \cite{Cho2014}, \cite{Bahdanau2014}
\item Encoder creates a summary of the entire input sequence. \cite{Cho2014}
\item Decoder samples a token at a time using input sequence summary, it's own RNN hidden state(s) and/or previously generated sample(s). \cite{Cho2014}, \cite{Bahdanau2014}
\item See figure 1 on page 2 of \cite{Cho2014} for architecture depiction.
\item Can be used to generate a target sequence based on input sequence. Can also be used to score a given pair of input-output sequences. \cite{Cho2014}
\item RNN Encoder-decoder captures semantic and syntactic structures of phrases. \cite{Cho2014}
\item Encoder-decoder needs to compress all the relevant information of the sentece in a single fixed length vector. This becomes a problem when sentence length increases, larger model is required. \cite{Bahdanau2014}.
\item Encoder-decoder have no explicit alignment. \cite{Liu2016a}
\item Input and output sequences can be of different length. \textbf{\textcolor{red}{Citation?}}
\item \cite{Chung2016} have character level encoder-decoder sequence-to-sequence machine translation system. Using sub-word level symbols in source side, full character level only in decoder.
\item \cite{Chung2016} use novel RNN (bi-scale RNN) on the target side for better handling of multiple timescales++++++
\item Character level decoder relaxes the problem with computational complexity (of softmax function) with large target vocabulary. \cite{Chung2016}
\item Using character level only encoding and decoding removes the need to know how to do segmentation of characters into words, which is a problem in models which use character level word encodings such as C2W. \cite{Chung2016}
\item All inflectional forms of word result in very large vocabulary, more efficient encoding can achieved with lexeme (lemma) and morphemes, but requires a lemmatizer and morphological analyser. \cite{Chung2016}
\item Furthermore model may not perform well with common words if the morphological form is rare. \cite{Chung2016}
\item Encoder-decoder used in translation from English to French gains significant performance increase when reversing the source sentence word order. \cite{Sutskever2014}
\item \cite{Sutskever2014} speculate that reversing the source sentence helps by making backpropagation work better with shorter dependencies of the sentences' first words. Reversing the source sentence did not deteriorate the translation performance of later parts of the sentence, as was initially believed by \cite{Sutskever2014}
\end{itemize}

\section{Attention Mechanism}
\begin{itemize}
\item \cite{Bahdanau2014} introduced attention mechanism in neural machine translation as a solution to deteriorating performance with long input sentences. \cite{Sutskever2014} speculate that similar improvement could have been gained with simply reversing the source sentence word order.
\item System of \cite{Bahdanau2014} soft searches words in source sentence for each word in target sentence.
\item System of \cite{Bahdanau2014} does not try to encode the whole sentence as a fixed size vector, but instead input sentence is encoded as sequence of vectors which are weighted at the decoding time.
\item See section 3.1 of \cite{Bahdanau2014} for description of decoder with attention.
\item \cite{Bahdanau2014} use beam search to approximate maximum conditional probability on the trained model.
\item Attention allows encoder-decoder to learn soft alignment of input and output sequences. \cite{Liu2016a}
\end{itemize}

\section{Hyperparameter Optimization}
\label{se:hyperparameter_optimization}
\begin{itemize}
\item Random search often works well
\end{itemize}



\newpage

% The actual text begins here and page numbering changes to 1,2...
% Leave the backside of title empty in twoside mode
\if@twoside
%\newpage
\cleardoublepage
\fi

\pagenumbering{arabic}
\setcounter{page}{1} % Start numbering from zero because command
                     % 'chapter*' does page break
\renewcommand{\chaptername}{} % This disables the prefix 'Chapter' or
                              % 'Luku' in page headers (in 'twoside'
                              % mode)


\chapter{Introduction}
\label{ch:intro}
Testing citation \cite{Andor2016}


\chapter{Natural Language Processing}
\label{ch:natural_language_processing}
\begin{itemize}
\item Natural Language Processing is vastly wide field, this thesis discusses only on the sections of NLP relevant to the experiments.
\item Subfields such as sentence segmentation and sentiment analysis are out of scope of this thesis.
\item Most of the NLP work has been for english.
\item Cross-linguistic annotation and parsing has been a reality only after introduction of The Universal Dependencies project and SyntaxNet.
\item Similarly Finnish parsing has been unreachable until the first Finnish corpus Turku Dependency Treebank \cite{Haverinen2014} and cross-linguistic parsers.
\item Traditionally NLP systems are tailored to the single problem at hand with hand engineered features suited for the problem. Recently general approach has received interest where feature engineering and task specific architectures are not needed. \cite{Collobert2011}, \cite{Zhang2015}
\item See Chapter 2.1 for Finnish Language quirks in \cite{Korenius2004}
\end{itemize}

\section{Feature Engingeering in NLP}
\label{ch:feature_engineering_in_nlp}
\begin{itemize}
\item Machine learning algorithms require words to be represented quantifiable features such as IDs or real number vectors.
\item Traditional feature selection requires hand engineered features.
\item Engineered features hog 95\% of the computation time. \cite{Chen2014}
\item Traditionally words have been represented by indices. \cite{Mikolov2013}
\item Next step was to use 1-of-V coding.
\item Index representation is simple as computationally cheap, making use of huge datasets possible. Simple models with huge data outperform complex models with less data. \cite{Mikolov2013}
\item See LSA and LDA for previous systems. Neural networks significantly outperform LSA in preserving linearities. LDA doesn't scale for large datasets. \cite{Mikolov2013}
\end{itemize}

\section{Word Embeddings}
\begin{itemize}
\item \textbf{\textcolor{red}{see section 1.2 of \cite{Mikolov2013} for previous work and history of word embeddings}}
\item Word embeddings represent words as n-dimensional vectors. \cite{Mikolov2013}
\item LSA leverages statistical information of a corpus but performs poorly on word analogy task. \cite{Pennington2014}
\item Skip-gram is good for word analogies but doesn't utilize corpus statistics well since vectors are trained on local context. \cite{Pennington2014}
\item Word embeddings try to map words with semantic similarities close to each other. Words may have several types of similarities such as \textit{France} and \textit{Italy} are countries but \textit{dogs} and \textit{triangles} are both in plural form. \cite{Mikolov2013a}
\item \cite{Chen2014} use 50 dimensional word embeddings created with Word2vec.
\item \cite{Chen2014} also use embeddings for POS tags and dependecy arcs. Only embedding POS tags has clear benefit, \cite{Chen2014} suspect that embedding arc labels have no effect since POS tags already contain the relational information.
\item Word embeddings with lookup table generalize poorly with morpohlogically rich languages such as Finnish. \cite{Takala2016}
\item Morphologically rich languages benefit from breaking the word into sub-parts, RNN based character level model is not compared with Stem+ending. \cite{Takala2016}
\item Word embeddings obtained through neural language models exhibit the property whereby semantically close words are close in the embedding vector space. \cite{Kim2016}
\item Most of the word embedding libraries work on principle of fixed vocabulary where embeddings are computed for all words in vocabulary. It's difficult to handle out-of-vocabulary words since word spelling contains only a small part of the word's semantic meaning.
\item Problem for morphologically rich languages can be relaxed by lemmatizing all words because lemmas don't suffer as much from the vocabulary explosion.
\end{itemize}

\subsection{Word2vec}
\begin{itemize}
\item \cite{Mikolov2013}
\item Can be used with datasets of billions of words
\item Has two models: Continuous bag-of-words and continuous skip-gram
\item Continuous bag-of-words predicts current word from the context (surrounding words)
\item Continuous skip-gram predicts context (surrounding words) from current word.
\item Continuous Bag-of-Words is better for small datasets, continuous skip-gram is better for large datasets.
\item CBOW is better for syntax, Skip-gram is better for semantics.
\item Can be used to find semantic relationships like vector('biggest') - vector('big') + vector('small') => vector('smallest') 
\item State of the art (as of 2013). Since several new architectures have proposed improvements such as FastText by Facebook and GloVE by \cite{Pennington2014}.
\end{itemize}

\subsection{Charater to Word}
\begin{itemize}
\item \cite{Ling2015}
\item Word embeddings can be generated from character sequences with significantly better performance for morphological languages.
\item Requires only single vector for each character type. Particularly good for morphological languages where word type count may be infinite.
\item Orthographical and functional (syntactic and semantic) relationships are non-trivial: \textit{butter} and \textit{batter} are orthographically close but functionally distant, \textit{rich} and \textit{affluent} are orthographically distant but functionally close. \cite{Ling2015} resort to LSTM networks for learning the relationships.
\item Word lookup tables are unable to generate representations for previously unseen words, as is required for morphology. \cite{Ling2015}
\item C2W can generate embeddings for unseen words.
\item C2W is computationally more expensive than word lookup tables, but can be eased by saving word embeddings for most frequent words since the words embedding for a character sequence (word) does not change.
\item During training word embeddings change but not inside a single batch, thus it is computationally cheaper to use large batches for training.
\item C2W can be replaced with word lookup tables for downstream processing since input and output of both methods are the same.
\end{itemize}

\section{Annotations}
\label{se:annotations}
\begin{itemize}
\item Stanford Dependencies by \cite{DeMarneffe2006}
\item Stanford Dependencies emerged as de facto annotation scheme for english, but has been adapted to several other languages including Finnish. \cite{Nivre2016}, \cite{Haverinen2014}.
\item Turku Dependency Treebank has been tranformed into universal dependencies. \cite{Pyysalo2015}
\item Unified annotation scheme reduces need for cross-language adaptations in downstream development. \cite{Petrov2012}
\item Universal Dependencies project started from the requirement for cross-linguistically consistent treebank annotations even for morphological languages. \cite{Nivre2016}.
\item Universal Dependencies project was born from merging several previous attempts to form a cross-linguistically sound dependency annotation schemes. \cite{Nivre2016}
\item UD data has been encoded in the CoNLL-U format, a revision of the popular CoNLL-X format. \cite{Nivre2016}
\item UD treebanks released in November 2015. \cite{Nivre2016}
\end{itemize}

\subsection{Turku Dependecy Treebank}
\begin{itemize}
\item \cite{Haverinen2014}
\item Treebanks are needed in computational linguistics.
\item First Finnish treebank.
\item Open licence, including for text annotated
\item 204339 tokens, 15126 sentences
\item Based on Stanford Dependency scheme with minor modifications to exclude phenomena not present in Finnish and to include new annotations not present in English.
\item Transposed to CoNNL-U scheme by universal dependencies project
\item Connexor Machinese Syntax is the only currenty available Finnish full dependency parser.
\item Texts from 10 different categories ranging from news and legal text to blog entries and fiction.
\item Dependency parsing is done manually with full double annotation process.
\item Uses Omorfi for morphological analysis. Ambiguous tokens are handled partly manually, partly rule based and partly with machine learning.
\item FTB uses 3 different taggers for morphology, check them out!
\item FTB is 97\% grammar examples, meant for rule based POS tagger development
\end{itemize}

\section{Tokenization}
\begin{itemize}
\item Rule based approach to tokenization would mostly split sentence into words from spaces and separate punctuation from the words.
\item Symbols and codes are more challenging for rule based tokenizers.
\item Neural net based approach to tokenization can be done with seq2seq model which inserts linefeeds.
\item Another neural net approach to tokenization is to do character classification where each character is classified to be first character of a word.
\item Good tokenization is very important for good downstream processing results; very small errors in tokenization can lead to extremely large errors in subsequent tasks (ask sitation from Honain). 
\item This thesis uses gold standard tokenization of the data files and therefore tokenization is not included in the experiments. 
\end{itemize}

\section{POS-tagging}
\begin{itemize}
\item Started from rule based taggers
\item Tagger by \cite{Brill1992} (known as Brill tagger) learns the rules and as such can be considered as a hybrid approach
\item Contemporary research is focused on statistical and NN based taggers
\item Rest of this section focuses on statistical parsers
\item \cite{Ling2015} introduced S-LSTM based State-of-the-art tagger
\item \cite{Andor2016} Improved accuracy with transition based tagger
\item \cite{Chen2014} were first to represent POS-tag and arc labels as embeddings
\item \cite{Andor2016} and \cite{Weiss2015} built their solutions based on \cite{Chen2014}
\item \cite{Nivre2004} introduced system for transition based taggers known as arc-starndard system. \cite{Chen2014}
\end{itemize}

\section{Lemmatisation}
\label{se:lemmatisation}
\begin{itemize}
\item Lemmatization is the process of finding a base form for a word.
\item Lemmatization is a normalization technique. \cite{Korenius2004}
\item Homographic and inflectional word forms cause ambiquity. \cite{Korenius2004}
\item Compound words cause problems. \cite{Korenius2004}
\item Lemmatization is better than stemming for clustering of documents written in Finnish because of it's highly inflectional nature. \cite{Korenius2004}
\item Lemmatization catches better the semantic meaning of a word, as can be deducted from a better clustering performance.
\item Omorfi does lemmatization based on morphological analysis
\item Omorfi produces multiple lemmas which need to be disambiguated
\item Disambiguation can be done with selecting most probable word, given the context, by language model
\item \cite{Kestemont2016} try to solve lemmatization as a neural net classification problem, where lemmas are the class labels
\item Method of \cite{Kestemont2016} cannot produce lemmas not seen on training time.
\item Lemmatization has received a lot of research attention for highly inflectional languages, see \cite{Kestemont2016}
\item Lemmatization of english is considered a solved problem, rule based or hybrid approaches can do practically flawless job.
\item There has been almost none previous work using deep learning for lemmatization before \cite{Kestemont2016}
\end{itemize}

\section{Structural Parsing}
\label{se:structural_parsing}
\begin{itemize}
\item Aims to find structure of a sentence.
\item Commonly divided to two different tasks: constituency parsing and dependency parsing.
\item Constituency parser creates a parse tree of constituencies.
\item Dependency parser creates a parse tree of word token dependencies.
\item Constituency parsers are slower but more informal than dependency parsers. \cite{Fernandez-Gonzalez2015}
\item \cite{Fernandez-Gonzalez2015} show that it is possible to build constituency parser with dependency parser by reducing constituents to dependency parsing.
\item CoNLL uses dependency parse trees.
\end{itemize}

\subsection{Transition Based Parsers}
\begin{itemize}
\item Good balance between efficiency and accuracy \cite{Weiss2015}
\item Parsed left to right; at each position the parses chooses action from a set of possible actions.
\item Greedy models are fast but error prone and need hand engineered features \cite{Weiss2015}
\item Actions can be chosen by ANN to avoid hand engineering \cite{Chen2014}, \cite{Weiss2015}
\end{itemize}

\subsubsection{Syntaxnet}
\begin{itemize}
\item \cite{Andor2016}
\item Transition based
\item Locally and globally normalized
\item Backpropagation through entire net
\item State-of-the-Art
\end{itemize}


\chapter{Experiements on Joint Model for POS-tagging and Lemmatization}
\label{ch:experiements_on_joint_model_for_pos_tagging_and_lemmatization}

\section{Why Joint model?}
\begin{itemize}
\item \cite{Liu2016a} did joint model for intents and slots on ATIS dataset achieving 22\% decrease on intent error on their Attention based encoder-decorder network.
\item Ask Honain for ref for joint POS-tagging and lemmatization paper.
\item Lemmas and POS-tags have strictly linked relationship
\item Lemma identifies a single unique word which has single POS
\item Lemmas as written form may reference multiple words (eg. nail), this thesis simply omits the possible problems associated with written forms with multiple meanings.
\item Hypothesis of this thesis is that doing joint learning for lemmas and POS-tags will improve the classification performance of either or both.
\end{itemize}

\section{Architechture}
\label{se:architecture}
\begin{itemize}
\item Architecture used for experiements is a multi-layer deep neural network consisting not only from multiple stacked layers but multiple architectural layers.
\item Architectural layers form an end-to-end pipeline for representing characters, words and contexes as well as doing the actual classification
\item POS-tags are limited set of 31 lables and are therefore easily quantified for classification task
\item It is not possible to classify all possible lemmas because of extremely flexible system of forming compound words in certain languages such as Finnish
\item Out-of-vocabulary lemmas are classified as \verb£ <UNK>£ as token representing unknown words
\item One approach for lemmatizing all possible lemmas would be to have encoder-decoder model generating the lemmas one character at a time.
\item For the sake of simplicity and focus this thesis considers lemmatization only as a classification problem.
\item Decoder adds a lot of complexity to network and fuzzies the metrics; as a character level model, the decoder is not very natural for word level accuracy evaluation.
\item Observing effect of joint model training on classification only should satisfy the hypothesis on mutual information without doing generation with decoder.
\end{itemize}

\subsection{Word Embedding}
\label{ss:word_embedding}
\begin{itemize}
\item C2W
\item Character vocabulary needs to fixed for character embedding layer.
\item We selected \verb£ !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ£ \verb£[\]^_`abcdefghijklmnopqrstuvwxyz{|}~ÄÅÖäåö€£ as our character vocabulary.
\item This contains ASCII characters from 32 to 127, scandic characters in lower and upper case and € sign.
\item Selected character vocabulary covers 99,933\% of character usages in Finnish internet parsebank n-gram dataset.
\item ASCII end-of-text (ETX) and substition (SUB) characters were added to character vocabulary to handle sequence length padding and out of vocabulary characters respectively.
\item Characters were represented as character embedding vectors obtained during the training and implemented as simple trainable Tensor in tensorflow.
\item C2W RNN takes a mini-batch of words represented as character tensors (one character per row) as an input
\item Output of C2W is word embedding vectors for all words in the mini-batch.
\end{itemize}

\subsection{Context Encoding}
\label{ss:context_encoding}
\begin{itemize}
\item Bi-directional RNN
\item Word vectors obtained from word embedding layer used as input
\item Input for each word also contains it's right and left side contexes.
\item Each word has effectively only uni-directional RNN for each side context because sentence is fed once through bi-directional RNN. Forward pass encodes left side contexes and backward pass encodes right side contexes.
\item Uni-directional RNN encoding for contexes should not be a problem; sentence lengths are usually very limited (averaging 13 words) and most important words in the context are the words closest to current word for whom context is being encoded.
\item Output is word embedding vector which also contains information about the word's context.
\end{itemize}

\subsection{Classification}
\label{ss:classification}
\begin{itemize}
\item Add fully connected layers and output layer (MLP)
\item Decision to add fully connected layers was made with brief experiments comparing results with and without the fully connected layers. Fully connected layers added almost no computational complexity while improving learning.
\item All experiments were done with fully connected layers and further experimenting how they affect different tasks was left for future research.
\item Lemma vocabulary consists of most frequent lemmas in Universal Dependencies dataset covering at least 90\% of use cases in the dataset.
\item Output layers are a simple fully connected layers with linear activation functions. Size of output lemma classification and POS classification layers are the size of lemma vocabulary and size of POS-tag vocabulary respectively.
\end{itemize}

\section{Implementation}
\label{se:implementation}
\begin{itemize}
\item Tensorflow on Python
\end{itemize}

\section{Experiements}
\label{se:experiements}
\begin{itemize}
\item Testing with Universal Dependencies data
\item POS-tagging without lemmatization
\item Lemmatization without POS-tagging
\item Lemmatization with classified POS-tags
\item Lemmatization with gold standard POS-tags
\item Joint model
\item POS-tagging with lemmas as input features in not very meaningful task because POS can be read from a dictionary if lemma is known.
\end{itemize}

\subsection{Test Methods}
\label{ss:test_methods}
\begin{itemize}
\item Using accuracy. Percentage of correct classification labels.
\item Academia does not seem to agree what F1 score is
\item Accuracy is simpler and easier for readers to understand
\item Reporting accuracies and losses for training, validation and test datasets separately
\item Each experiement was executed by training the network on the training dataset with early stopping based on observations of validation dataset loss.
\item Architecture was modified as little as possible between the experiements, only disabling or enabling either lemma classification or POS-tag classification layers.
\item Same hyperparameters were used for all experiements, tuned with Optunity implemented particle swarm algorithm for joint model.
\end{itemize}

\section{Results}
\label{se:results}
\begin{itemize}
\item UDPipe achieves 94,5\% on POS-tagging on Finnish Universal Dependencies 1.4 and 86,5\% on lemmatization.
\item UDPipe lemmatizer seems to be some sort of generative model so comparison is not fair or even meaningful.
\item POS-tagging without lemmatization = 94,30\%. Extremely close to UDPipe results with only 0,2\%-points advantage to UDPipe.
\item Lemmatization without POS-tagging = 94,25\%
\item Lemmatization with classified POS-tags = 94,40\%
\item Lemmatization with gold standard POS-tags = 96,22\%
\item Joint model. Lemmas = 95,24\%, POS = 94,14\%
\item Joint model achieves best practical results for lemmatization, with 0,99\%-points increase, since gold-standard POS-tags are not available at production.
\item Joint model POS-tags are slightly inferior (0,16\%-points) to baseline
\item Lemmatization results with classified POS-tags are slightly better (0,1\%-points) than the baseline
\item Lemmatization with gold-standard POS-tags achieves clearly the best results with 1,97\%-points increase over baseline.
\end{itemize}


\chapter{Discussion}
\label{ch:discussion}
\section{How well results generalize?}
\begin{itemize}
\item Do these results generalize to other languages needs to be verified with additional experiments.
\item Universal Dependencies datasets are created to provide as uniform language modeling as possible across all languages and therefore it is probable that the same mutual information between lemmas and POS-tags in other languages would prove beneficial in joint learning of lemma and POS-tag classification.
\item Speculating how well these results generalize to other tasks in natural language processing is more complicated because other tasks may not share as strong mutual information as lemmas and POS-tags do.
\item \cite{Liang2016a} proved that neural networks benefit from jointly learning two tasks different from lemmatization and POS-tagging. \cite{Liu2016a} even had tasks on different architectural level, slot filling being word level task and intent classification being sentence or message level task.
\item With results provided in this work and results from \cite{Liu2016a} it's failry safe to say that neural networks can obtain better results when learning jointly two tasks that share mutual information.
\item Another benefit of joint learning is of course smaller computational complexity since both tasks can share same character and word level encoders requiring only one pass through the encoders in live inferencing.
\end{itemize}

\section{What was assumed?}
\begin{itemize}
\item What was assumed?
\item It was assumed that training, validation and test datasets would represent Finnish language well.
\item It was also assumed that different datasets would have fairly similar data distribution and training done on training dataset would generalize to test dataset.
\item It was noticed that selecting lemma vocabulary which covers 100\% of uses in training set only covers about 75\% of uses in validation and test datasets.
\item Lemma classifier learned remarkably well to classifiy lemmas not seen during training time as unknown tokens. Maybe priori bias explains this?
\end{itemize}

\section{What was simplified?}
\begin{itemize}
\item Slightly different results would be optained by optimizing hyperparameters separately for every different experiement. Hyperparameter optimization requires about 100 to 200 training runs and as such takes significant time budget. Using time required for hyperparameter optimization before every experiement proved inconvenient for this work.
\item Lemmatization as word level classification task only is also a pretty significant simplification. Decoder model for lemma generation is able to always produce some kind of answer without ever producing unknown tokens.
\item Whether having unknown tokens is fatal flaw depends on the problem at hand and on the down stream processing which uses lemmas obtained with lemmatization. For some tasks not having generated lemmas with high probability of typographical errors produced might be significantly better option.
\end{itemize}


\chapter{Conclusions}
\label{ch:conclusions}
\begin{itemize}
\item Hypothesis proved to be true
\item Interestingly we couldn't obtain increase in POS-tagging performance.
\item Maybe lemmatization as a more information rich task dominates the learning
\item POS-tagging on joint model could be improved by giving more weight to POS-tagging task but that quickly deteriorates the lemmatization perfomance drastically.
\item What was learned
\end{itemize}

%
% The bibliography, i.e the list of references (3 options available)
%
\newpage


% Extra for Finnish theses

\renewcommand{\bibname}{Bibliography}     % Bilingual babel puts Finnish ``Kirjallisuttaa'' otherwise. Strange...
%\renewcommand{\bibname}{Lähteet}         % Set Finnish header, remove this if using English
%\addcontentsline{toc}{chapter}{Lähteet}  % Include this in TOC
\addcontentsline{toc}{chapter}{\bibname}  % Include this in TOC


\printbibliography                  % a) heading in English
%\printbibliography[title=Lähteet]   % b) heading in Finnish
%\addtocontents{toc}{%               % b) add Finnish heading to table of contents
%\protect\noindent Lähteet\protect\par
%} 


%
% Appendices are optional. 
% This part is semi-ugly at the moment. Please give feedback if can
% improve it.
\appendix
\pagestyle{headings}
% \renewcommand{\appendixname}{Liite} % Extra. Set Finnish prefix for page header

%
% a) Not-so-handy way, but at least it works
% 
\def\appA{APPENDIX A. Something extra} % Define the name and numbering manually
\chapter*{\appA}                       % Create chapter heading
\markboth{\appA}{\appA}                % Set page header
\addcontentsline{toc}{chapter}{\appA}  % Include this in TOC
% Note that \label does not work with unnumbered chapter

Appendices are purely optional.  All appendices must be referred to in
the body text

\def\appB{APPENDIX B. Something completely different} % Define another new command
\chapter*{\appB}                       % As above, but use \appB instead of \appA
\label{app:B}
\markboth{\appB}{\appB}                     
\addcontentsline{toc}{chapter}{\appB}  


You can append to your thesis, for example, lengthy mathematical
derivations, an important algorithm in a programming language, input
and output listings, an extract of a standard relating to your thesis,
a user manual, empirical knowledge produced while preparing the
thesis, the results of a survey, lists, pictures, drawings, maps,
complex charts (conceptual schema, circuit diagrams, structure charts)
and so on.


%
% b) The other option is to use numbered chapter and our baseline
% template report.cls numbers them as A, B... The heading and TOC do
% not include prefix 'Appendix' although the page header does.
%\chapter{name of the appendix}
%\label{app:A}                          % For cross-references



\end{document}

