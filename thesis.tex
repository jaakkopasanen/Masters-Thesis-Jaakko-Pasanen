\documentclass[12pt,a4paper,english
% ,twoside,openright
]{tutthesis}
%\documentclass[12pt,a4paper,finnish]{tutthesis}

% Note that you must choose either Finnish or English here and there in this
% file.
% Other options for document class
  % ,twoside,openright   % If printing on both sides (>80 pages)
  % ,twocolumn           % Can be used in lab reports, not in theses

% Ensure the correct Pdf size (not needed in all environments)
\special{papersize=210mm,297mm}


% LaTeX file for BSC/MSc theses and lab reports.
% Requires the class file (=template) tutthesis.cls and figure files,
% either tut-logo, exampleFig (as pdf or eps) and example_code.c
% Author: Sami Paavilainen (2006)
% Modified: Heikki Huttunen (heikki.huttunen@tut.fi) 31.7.2012.
%           Erno Salminen, @tut.fi, 2014-08-15
%             - added text snippets from the writing guide
%             - added lots of comments: both tips and alternative styles
%             - added an example table
%             - and so on...

%
% Define your basic information
%
\author{Jaakko Pasanen}
\title{Natural Language Understanding for Information Retrieval in Chat Based Customer Support} % primary title (for front page)
\titleB{Luonnollisen kielen ymmärrys tiedon hakua varten chat-pohjaisessa asiakaspalvelussa}     % translated title for abstract
\thesistype{Master of Science thesis} % or Bachelor of Science, Laboratory Report... 
\examiner{Ari Visa} % without title Prof., Dr., MSc or such

% Put your thesis' main language last
% http://mirrors.ctan.org/macros/latex/required/babel/base/babel.pdf
\usepackage[finnish, main=english]{babel}

% http://www.ctan.org/pkg/biblatex
\usepackage[
  style=authoryear,
  maxcitenames=2,
  backend=biber,
  firstinits=true
]{biblatex}
\bibliography{thesis_refs.bib}
%% Note that option style=numeric works as well
\usepackage{fontspec}
\usepackage{amsfonts}


% You can also add your own commands
\newcommand\todo[1]{{\color{red}!!!TODO: #1}} % Remark text in braces appears in red
\newcommand{\angs}{\textsl{\AA}}              % , e.g. slanted symbol for Ångstöm

% Preparatory content ends here


\pagenumbering{roman} % was: {Roman}
\pagestyle{headings}
\begin{document}

% Special trick so that internal macros (denoted with @ in their name)
% can be used outside the cls file (e.g. \@author)
\makeatletter

%
% Create the title page.
% First the logo. Check its language.
\thispagestyle{empty}
\vspace*{-.5cm}\noindent
\includegraphics[width=8cm]{tty_tut_logo}   % Bilingual logo

% Then lay out the author, title and type to the center of page.
\vspace{6.8cm}
\maketitle
\vspace{6.7cm} % -> 6.7cm if thesis title needs two lines

% Last some additional info to the bottom-right corner
\begin{flushright}  
  \begin{minipage}[c]{6.8cm}
    \begin{spacing}{1.0}
      %\textsf{Tarkastaja: Prof. \@examiner}\\
      %\textsf{Tarkastaja ja aihe hyväksytty}\\ 
      %\textsf{xxxxxxx tiedekuntaneuvoston}\\
      %\textsf{kokouksessa dd.mm.yyyy}\\
      \textsf{Examiner: Prof. \@examiner}\\
      \textsf{Examiner and topic approved by the}\\ 
      \textsf{Faculty Council of the Faculty of}\\
      \textsf{Engineering Sciences}\\
      \textsf{on 31st December 2016}\\
    \end{spacing}
  \end{minipage}
\end{flushright}

% Leave the backside of title page empty in twoside mode
\if@twoside
\clearpage
\fi

%
% Use Roman numbering I,II,III... for the first pages (abstract, TOC,
% termlist etc)
\pagenumbering{Roman} 
\setcounter{page}{0} % Start numbering from zero because command 'chapter*' does page break

% Some fields in abstract are automated, namely those with \@ (author,
% title in the main language, thesis type, examiner).
\chapter*{Abstract}

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@title\\   % use \@titleB when thesis is in Finnish
         \textsf{Tampere University of Technology}\\
         \textsf{\@thesistype, xx pages, x Appendix pages} \\
         \textsf{December 2016}\\
         \textsf{Master's Degree Programme in Automation Technology}\\
         \textsf{Major: Learning and Intelligent Systems}\\
         \textsf{Examiner: Prof. \@examiner}\\ % 
         \textsf{Keywords: Hype}\\
\end{spacing}


The abstract is a concise 1-page description of the work: what was the
problem, what was done, and what are the results. Do not include
charts or tables in the abstract.

Put the abstract in the primary language of your thesis first and then
the translation (when that is needed).


% Foreign students do not need Fininsh abstract (tiivistelmä). Move
% this before English abstract if thesis is in Finnish. Move also the
% otherlanguage command to the English abstract (if needed).

\begin{otherlanguage}{finnish} %  Following text in in 2nd language
\chapter*{Tiivistelmä} % Asterisk * turns numbering off

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@titleB\\  % or use \@title when thesis is in Finnish
         \textsf{Tampereen teknillinen yliopisto}\\
         \textsf{Diplomityö, xx sivua, x liitesivua}\\ %
         \textsf{Joulukuu 2016}\\
         \textsf{Automaatiotekniikan koulutusohjelma}\\
         \textsf{Pääaine: Oppivat ja älykkäät järjestelmät}\\
         \textsf{Tarkastajat:  Prof. \@examiner}\\ % automated, if just 1 examiner
         \textsf{Avainsanat: Hype}\\
\end{spacing}

The abstract in Finnish. Foreign students do not need this page.

Suomenkieliseen diplomityöhön kirjoitetaan tiivistelmä sekä suomeksi
että englanniksi.

Kandidaatintyön tiivistelmä kirjoitetaan ainoastaan kerran, samalla
kielellä kuin työ. Kuitenkin myös suomenkielisillä kandidaatintöillä
pitää olla englanninkielinen otsikko arkistointia varten.

\end{otherlanguage} % End on 2nd language part


\chapter*{Preface}

This document template conforms to Guide to Writing a Thesis at
Tampere University of Technology (2014) and is based on the previous
template. The main purpose is to show how the theses are formatted
using LaTeX (or \LaTeX ~ to be extra fancy) .

The thesis text is written into file \texttt{d\_tyo.tex}, whereas
\texttt{tutthesis.cls} contains the formatting instructions. Both
files include lots of comments (start with \%) that should help in
using LaTeX. TUT specific formatting is done by additional settings on
top of the original \texttt{report.cls} class file. This example needs
few additional files: TUT logo, example figure, example code, as well
as example bibliography and its formatting (\texttt{.bst}) An example
makefile is provided for those preferring command line. You are
encouraged to comment your work and to keep the length of lines
moderate, e.g. <80 characters. In Emacs, you can use \texttt{Alt-Q} to
break long lines in a paragraph and \texttt{Tab} to indent commands
(e.g. inside figure and table environments). Moreover, tex files are
well suited for versioning systems, such as Subversion or Git.  
% \url{http://www.ctan.org/tex-archive/info/lshort/english/lshort.pdf}

Acknowledgements to those who contributed to the thesis are generally
presented in the preface. It is not appropriate to criticize anyone in
the preface, even though the preface will not affect your grade. The
preface must fit on one page. Add the date, after which you have not
made any revisions to the text, at the end of the preface.

~ 
% Tilde ~ makes an non-breakable spce in LaTeX. Here it is used to get
% two consecutive paragraph breaks

Tampere, 11.8.2014

~

On behalf of the working group, Erno Salminen


% Add the table of contents, optioanlly also the lists of figures,
% tables and codes.

%\renewcommand\contentsname{Sisällys} % Set Finnish name, remove this if using English
\setcounter{tocdepth}{3}              % How many header level are included
\tableofcontents                      % Create TOC


%
% Term and symbol exaplanations use a special list type
%
\chapter*{List of abbreviations and symbols}
\markboth{}{}                                % no headers
%\chapter*{Lyhenteet ja merkinnät}

% You don't have to align these with whitespaces, but it makes the
% .tex file more readable
\begin{termlist}
\item[ANN] Artificial Neural Network
\item[LAS] Labelled Attachment Score
\item[LDA] Latent Dirilecht Allocation
\item[LSA] Latent Semantic Analysis
\item[LSTM] Long short term memory; type of RNN with short term memory.
\item[NER] Named entity recognition
\item[NLP] Natural Language Processing
\item[PMI] Pointwise mutual information
\item[POS] Part-of-speech; also called lexical category
\item[RNN] Recurrent neural network
\item[S-LSTM] Stack long short term memory
\item[TUT] Tampere University of Technology
\item[UAS] Unlabelled Attachment Score
\end{termlist}


\chapter*{Notes}

\section{Reading}
These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003). \cite{Pennington2014}

See first paragraph of section 1 of \cite{Liang2016a} for sources on machine translation

Encoder-decoder model


\section{Terms for Computational Linguistics}
\begin{description}
\item[1-of-V Coding] Representing words as sparse binary vectors which have 1 at the word's vocabulary index and 0 all others. With vocabulary \{dog, cat, mouse\}, dog becomes [1, 0, 0], cat becomes [0, 1, 0] and mouse [0, 0, 1].

\item[Bag-of-words] Multiset of words appearing in a text with occurrence counts for each word. Used as a tool for feature generation. Does not preserve word order or grammar. Can implemented as a dictionary (or associative array) where words are the keys and counts are the values.

\item[Conditional Random Field]

\item[Constituent] In syntactic analysis, a constituent is a word or a group of words that function(s) as a single unit within a hierarchical structure. Many constituents are phrases. \textit{Yesterday I saw \textbf{an orange bird with a white neck}}

\item[Corpus] A collection of texts with linguistic annotations.

\item[Dimensionality] When discussing word embeddings and word vector spaces the dimensionality refers to definition in linear algebra. Dimensionality of arrays in computing means the number of indices required to specify an element in the array. Word vector in 50 dimensional vector space $\mathbb{R}^{50}$ would be represented in computing as one dimensional array of length 50 [d1, d2, d3, ..., d50]

\item[Distributional Hypothesis] Words that are used and occur in the same contexts tend to purport similar meanings. \cite{Harris1954}

\item[Feature] Numeric data representation that can be effectively exploited in machine learning tasks. E.g. Word occurrence frequencies.

\item[Feature Vector] Vector containing all the features. For an image a feature vector could be all the raw values of pixels as a single sequence. For a trigram model with 300 dimensional word embeddings a feature vector would be a 900 dimensional vector formed by concatenating all the separate word embedding vectors.

\item[Gazzetteer] In Named Entity Recognition a gazzetteer is a dictionary of known named entities.

\item[Language Model] Probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $P(w_{1},\ldots ,w_{m})$ to the whole sequence. Problems caused by growing vocabulary can be addressed with continuous language models such as neural net language models (NNML). Word2Vec by \cite{Mikolov2013} addresses this problem with Continuous Bag-of-words and Skip-gram models.

\item[Lemmatisation] Process of finding the base form of a word, e.g. flew -> fly

\item[Lexeme] A basic lexical unit of a language consisting of one word or several words, the elements of which do not separately convey the meaning of the whole.

\item[n-gram] Probabilistic language model where probability of current word is the joint probability of previous \textit{n} words. Bigram example: $P(I, saw, the, red, house) \approx P(I | ^\wedge)P(saw | I)P(red | the)P(house | red)P(\$ | house)$. The words unigram, bigram and trigram language model denote n-gram model language models with n = 1, n = 2 and n = 3, respectively.

\item[One-hot] Group of bits which the legal comibations of values are only those with a single high (1) and all the others low (0). See also 1-of-V Coding.

\item[Parsing] Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.

\item[PMI] Word co-occurence probability metric. High values for words that occur often together.

\item[POS-tagging] Process of marking up a word to particular part-of-speech (nouns, verbs, etc...) based on both its definition and its context.

\item[ReLU] Rectified Linear Unit. $h(x) = max\{0, x\}$. Used as non-linear activation function in neural nets particularly in convolutional net

\item[Skip-gram] Language model which predicts the context (previous and next \textit{n} words) of a current word from the current word instead of traditional way of predicting current word from the context.

\item[Structured Prediction] Predicting structured objects, rather than scalar discrete or real values. Translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.

\item[Token] A structure representing a lexeme that explicitly indicates its categorization for the purpose of parsing. In plain words tokens are instances of words in a text. Not to be confused with word type.

\item[Tree bank] Parsed text corpus that annotates syntactic or semantic sentence structure. Contains trees for sentences where phrases in a sentence are structured in a tree of syntactic or semantic relations. Very useful for training POS-taggers etc...

\item[Tri-Training] Parsing unlabeled data with two different parses and selecting only the sentences for which the two parsers produce the same trees \cite{Weiss2015}

\item[Word Lookup Table] Matrix $\textbf{P} \in \mathbb{R}^{d \times |V|}$ of d rows and |V| columns, where d is the word vector dimensionality and |V| is the size of vocalbulary. I.e. each column represent a single word and each row represent single dimension in vector space.

\item[Word Type] Unique words in a text. \textit{Good wine is good} has 4 tokens but only 3 word types.

\item[Word vector] N-dimensional vector representation of a word with interesting properties such as: vector('Paris') - vector('France') + Vector('Italy') -> vector('Rome')
\end{description}

\section{Universal Dependecies}

\subsection{CoNNL-U format}
Universal dependencies use CoNNL-U format for treebanks, CoNNL-U is revised version of CoNNL-X. Annotations are encoded in text files with word lines, blank lines for sentence boundaries and comments starting with hash (\#).

Word lines consist of following columns:
\begin{termlist}
\item[ID] Word ID in sentence
\item[FORM] Word form or punctuation symbol
\item[LEMMA] Lemma or stem of word form
\item[UPOSTAG] Universal part-of-speech tag
\item[XPOSTAG] Language specific part-of-speech tag
\item[FEATS] List of morphological features
\item[HEAD] Head of the curren token, value of ID or zero (0)
\item[DEPREL] Universal dependecy relation to the HEAD
\item[DEPS] List of secondary dependencies
\item[MISC] Any other annotation
\end{termlist}

Example in Finnish: Jäällä kävely avaa aina hauskoja ja erikoisia näkökulmia kaupunkiin

\begin{tabular}{l l l l l} 
ID & FORM & LEMMA & UPOSTAG & XPOSTAG \\
\hline
1 & Jäällä & jää & NOUN & N \\
2 & kävely & kävely & NOUN & N \\
3 & avaa & avata & VERB & V \\
4 & aina & aina & ADV & Adv \\
5 & hauskoja & hauska & ADJ & A \\
6 & ja & ja & CONJ & C \\
7 & erikoisia & erikoinen & ADJ & A \\
8 & näkökulmia & näkö\#kulma & NOUN & N \\
9 & kaupunkiin & kaupunki & NOUN & N \\
10 & . & . & PUNCT & Punct
\end{tabular}

\begin{tabular}{l}
FEATS \\
\hline
Case=Ade|Number=Sing \\
Case=Nom|Number=Sing \\
Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
Case=Par|Number=Plur \\
Case=Ill|Number=Sing \\
\_
\end{tabular}

\begin{tabular}{l l l l}
HEAD & DEPREL & DEPS & MISC \\
\hline
2 & nmod & \_ & \_ \\
3 & nsubj & \_ & \_ \\
0 & root & \_ & \_ \\
3 & advmod & \_ & \_ \\
8 & amod & \_ & \_ \\
5 & cc & \_ & \_ \\
5 & conj & 8:amod & \_ \\
3 & dobj & \_ & \_ \\
8 & nmod & \_ & SpaceAfter=No
\end{tabular}

\subsection{Universal POS tags}
\begin{termlist}
\item[ADJ] Adjective. Describing word qualifying noun or noun phrase. \textbf{deep}, \textbf{intelligent}
\item[ADP] Adposition. Word expressing spatial or temporal relations \textbf{under}, \textbf{around}, \textbf{before} or mark various semantic roles \textbf{of}, \textbf{for}
\item[ADV] Adverb. Modifies another word. Typically express manner, place, time, frequency etc. She sang \textbf{loudly}. You are \textbf{quite} right.
\item[AUX] Auxiliary verb. A verb used in forming the tenses, moods, and voices of other verbs. \textbf{Do} you want tea?. He \textbf{has} given his all.
\item[CONJ] Coordinating conjunction. Conjunction placed between words, phrases, clauses or sentences of equal rank. \textbf{and}, \textbf{but}, \textbf{or}.
\item[DET] Determiner. Expresses reference of a noun (group). \textbf{The} girl is \textbf{a} student. \textbf{Which} book is that?
\item[INTJ] Interjection. Shows emotion or feeling of the author, includes exclamations, curses, greetings and such. \textbf{Ouch!}, \textbf{hey}, \textbf{huh?}.
\item[NOUN] Noun. Denotes a person, animal, place thing or idea. The \textbf{cat} sat on a \textbf{mat}.
\item[NUM] Numeral. Number, written with digits or letters. \textbf{12}, \textbf{eleven}.
\item[PART] Particle. Cannot be inflected. Interjections and conjunctions. In finnish also \textbf{että}, \textbf{jotta}, \textbf{koska}, \textbf{kun} etc...
\item[PRON] Pronoun. Replaces (often previously introduced) noun. Joe saw Jill, and \textbf{he} waved at \textbf{her}.
\item[PUNCT] Punctuation. Full stop, comma, bracket etc.
\item[SCONJ] Subordinating conjunction. A conjunction that introduces a subordinating clause, e.g. \textbf{although}, \textbf{because}, \textbf{whenever}.
\item[SYM] Symbol.
\item[VERB] Verb. Conveys an action \textbf{bring}, \textbf{read}, an occurrence \textbf{happen}, \textbf{become}, or a state of being \textbf{be}, \textbf{exist}.
\item[X] Other
\end{termlist}

\newpage

% The actual text begins here and page numbering changes to 1,2...
% Leave the backside of title empty in twoside mode
\if@twoside
%\newpage
\cleardoublepage
\fi

\pagenumbering{arabic}
\setcounter{page}{1} % Start numbering from zero because command
                     % 'chapter*' does page break
\renewcommand{\chaptername}{} % This disables the prefix 'Chapter' or
                              % 'Luku' in page headers (in 'twoside'
                              % mode)


\chapter{Introduction}
\label{ch:intro}
Testing citation \cite{Andor2016}


\chapter{Natural Language Processing}
\label{ch:natural_language_processing}
\begin{itemize}
\item Natural Language Processing is vastly wide field, this thesis discusses only on the sections of NLP relevant to the experiments.
\item Subfields such as sentence segmentation and sentiment analysis are out of scope of this thesis.
\item Most of the NLP work has been for english.
\item Cross-linguistic annotation and parsing has been a reality only after introduction of The Universal Dependencies project and SyntaxNet.
\item Similarly Finnish parsing has been unreachable until the first Finnish corpus Turku Dependency Treebank \cite{Haverinen2014} and cross-linguistic parsers.
\item Traditionally NLP systems are tailored to the single problem at hand with hand engineered features suited for the problem. Recently general approach has received interest where feature engineering and task specific architectures are not needed. \cite{Collobert2011}, \cite{Zhang2015}
\item This thesis focuses on task specific systems because general language understanding systems are still lacking a good support for production systems and depend on vast amounts of good data \cite{Zhang2015} unobtainable within our time frame and resources.
\item See Chapter 2.1 for Finnish Language quirks in \cite{Korenius2004}
\end{itemize}

\section{Pre-processing}
\label{ch:pre-processing}

\section{Feature Engingeering in NLP}
\label{ch:feature_engineering_in_nlp}
\begin{itemize}
\item Machine learning algorithms require words to be represented quantifiable features such as IDs or real number vectors.
\item Traditional feature selection requires hand engineered features.
\item Engineered features hog 95\% of the computation time. \cite{Chen2014}
\item Traditionally words have been represented by indices. \cite{Mikolov2013}
\item Next step was to use 1-of-V coding.
\item Index representation is simple as computationally cheap, making use of huge datasets possible. Simple models with huge data outperform complex models with less data. \cite{Mikolov2013}
\item See LSA and LDA for previous systems. Neural networks significantly outperform LSA in preserving linearities. LDA doesn't scale for large datasets. \cite{Mikolov2013}
\end{itemize}

\subsection{Word Embeddings}
\begin{itemize}
\item \textbf{\textcolor{red}{see section 1.2 of \cite{Mikolov2013} for previous work and history of word embeddings}}
\item Word embeddings represent words as n-dimensional vectors. \cite{Mikolov2013}
\item LSA leverages statistical information of a corpus but performs poorly on word analogy task. \cite{Pennington2014}
\item Skip-gram is good for word analogies but doesn't utilize corpus statistics well since vectors are trained on local context. \cite{Pennington2014}
\item Word embeddings try to map words with semantic similarities close to each other. Words may have several types of similarities such as \textit{France} and \textit{Italy} are countries but \textit{dogs} and \textit{triangles} are both in plural form. \cite{Mikolov2013a}
\item \cite{Chen2014} use 50 dimensional word embeddings created with Word2vec.
\item \cite{Chen2014} also use embeddings for POS tags and dependecy arcs. Only embedding POS tags has clear benefit, \cite{Chen2014} suspect that embedding arc labels have no effect since POS tags already contain the relational information.
\item Word embeddings with lookup table generalize poorly with morpohlogically rich languages such as Finnish. \cite{Takala2016}
\item Morphologically rich languages benefit from breaking the word into sub-parts, RNN based character level model is not compared with Stem+ending. \cite{Takala2016}
\item Word embeddings obtained through neural language models exhibit the property whereby semantically close words are close in the embedding vector space. \cite{Kim2016}
\end{itemize}

\subsection{Word2vec}
\begin{itemize}
\item \cite{Mikolov2013}
\item Can be used with datasets of billions of words
\item Has two models: Continuous bag-of-words and continuous skip-gram
\item Continuous bag-of-words predicts current word from the context (surrounding words)
\item Continuous skip-gram predicts context (surrounding words) from current word.
\item Continuous Bag-of-Words is better for small datasets, continuous skip-gram is better for large datasets.
\item CBOW is better for syntax, Skip-gram is better for semantics.
\item Can be used to find semantic relationships like vector('biggest') - vector('big') + vector('small') => vector('smallest') 
\item State of the art (as of 2013)
\end{itemize}

\subsection{GloVe}
\begin{itemize}
\item GloVe by \cite{Pennington2014} capture global corpus statistics with log-bilinear co-occurence count model.
\item Memory requirements for GloVe are substantial since global co-occurence matrix for entire vocabulary is required, even though GloVe eliminates the need for zero occurence elements. Problem becomes worse for inflectional languages such as Finnish were vocabulary requires word type for each infliction for each word.
\item GloVe outperforms other methods on almost all tested tasks. All tasks are English only. \cite{Pennington2014}
\end{itemize}

\subsection{Charater to Word}
\begin{itemize}
\item \cite{Ling2015}
\item Word embeddings can be generated from character sequences with significantly better performance for morphological languages.
\item Requires only single vector for each character type. Particularly good for morphological languages where word type count may be infinite.
\item Orthographical and functional (syntactic and semantic) relationships are non-trivial: \textit{butter} and \textit{batter} are orthographically close but functionally distant, \textit{rich} and \textit{affluent} are orthographically distant but functionally close. \cite{Ling2015} resort to LSTM networks for learning the relationships.
\item Word lookup tables are unable to generate representations for previously unseen words, as is required for morphology. \cite{Ling2015}
\item C2W can generate embeddings for unseen words.
\item C2W is computationally more expensive than word lookup tables, but can be eased by saving word embeddings for most frequent words since the words embedding for a character sequence (word) does not change.
\item During training word embeddings change but not inside a single batch, thus it is computationally cheaper to use large batches for training.
\item C2W can be replaced with word lookup tables for downstream processing since input and output of both methods are the same.
\end{itemize}

\section{Annotations}
\label{se:annotations}
\begin{itemize}
\item Stanford Dependencies by \cite{DeMarneffe2006}
\item Stanford Dependencies emerged as de facto annotation scheme for english, but has been adapted to several other languages including Finnish. \cite{Nivre2016}, \cite{Haverinen2014}.
\item Turku Dependency Treebank has been tranformed into universal dependencies. \cite{Pyysalo2015}
\item Unified annotation scheme reduces need for cross-language adaptations in downstream development. \cite{Petrov2012}
\item Universal Dependencies project started from the requirement for cross-linguistically consistent treebank annotations even for morphological languages. \cite{Nivre2016}.
\item Universal Dependencies project was born from merging several previous attempts to form a cross-linguistically sound dependency annotation schemes. \cite{Nivre2016}
\item UD data has been encoded in the CoNLL-U format, a revision of the popular CoNLL-X format. \cite{Nivre2016}
\item UD treebanks released in November 2015. \cite{Nivre2016}
\end{itemize}

\subsection{Turku Dependecy Treebank}
\begin{itemize}
\item \cite{Haverinen2014}
\item Treebanks are needed in computational linguistics.
\item First Finnish treebank.
\item Open licence, including for text annotated
\item 204339 tokens, 15126 sentences
\item Based on Stanford Dependency scheme with minor modifications to exclude phenomena not present in Finnish and to include new annotations not present in English.
\item Transposed to CoNNL-U scheme by universal dependencies project
\item Connexor Machinese Syntax is the only currenty available Finnish full dependency parser.
\item Texts from 10 different categories ranging from news and legal text to blog entries and fiction.
\item Dependency parsing is done manually with full double annotation process.
\item Uses Omorfi for morphological analysis. Ambiguous tokens are handled partly manually, partly rule based and partly with machine learning.
\item FTB uses 3 different taggers for morphology, check them out!
\item FTB is 97\% grammar examples, meant for rule based POS tagger development
\end{itemize}

\section{Syntactic Parsing}
\label{se:syntactic_parsing}
\begin{itemize}
\item Commonly divided to constituency parsing and dependency parsing.
\item Constituency parser creates a parse tree of constituencies.
\item Dependency parser creates a parse tree of word token dependencies.
\item Constituency parsers are slower but more informal than dependency parsers. \cite{Fernandez-Gonzalez2015}
\item \cite{Fernandez-Gonzalez2015} show that it is possible to build constituency parser with dependency parser by reducing constituents to dependency parsing.
\end{itemize}

\subsection{POS-tagging}
\begin{itemize}
\item Started from rule based taggers
\item Tagger by \cite{Brill1992} (known as Brill tagger) learns the rules and as such can be considered as a hybrid approach
\item Contemporary research is focused on statistical and ANN based taggers
\item Rest of this section focuses on statistical parsers
\item \cite{Ling2015} introduced S-LSTM based State-of-the-art tagger
\item \cite{Andor2016} Improved accuracy with transition based tagger
\item \cite{Chen2014} were first to represent POS-tag and arc labels as embeddings
\item \cite{Andor2016} and \cite{Weiss2015} built their solutions based on \cite{Chen2014}
\item \cite{Nivre2004} introduced system for transition based taggers known as arc-starndard system. \cite{Chen2014}
\end{itemize}

\subsection{Transition Based Parsers}
\begin{itemize}
\item Good balance between efficiency and accuracy \cite{Weiss2015}
\item Parsed left to right; at each position the parses chooses action from a set of possible actions.
\item Greedy models are fast but error prone and need hand engineered features \cite{Weiss2015}
\item Actions can be chosen by ANN to avoid hand engineering \cite{Chen2014}, \cite{Weiss2015}
\end{itemize}

\subsection{Syntaxnet}
\begin{itemize}
\item \cite{Andor2016}
\item Transition based
\item Locally and globally normalized
\item Backpropagation through entire net
\item State-of-the-Art
\end{itemize}

\section{Language Modeling}
\label{se:language_modeling}
\begin{itemize}
\item A language model is a probability distribution over a sequence of words, traditionally performed with n-th order Markov assumption or n-gram counting and smoothing (Chen and Goodman, 1998) \textit{see \cite{Kim2016} Introduction for citation.}
\item \cite{Kim2016} have character level encoding and word-level predicting model for language modeling.
\item \cite{Kim2016} noticed that character inputs are sufficient for language modeling.
\item Character level model of \cite{Kim2016} outperfom word-level and morpheme-level models on morphologically rich languages Arabic, Czech, French, German, Spanish and Russian.
\item Neural Language Models NLM are blind to sub-word level information (e.g. morphemes). \cite{Kim2016}
\item 
\end{itemize}

\section{Machine Translation}
\label{se:machine_translation}
See section \ref{ss:translating_colloquial_speech} for more info on our implementation.

\section{Lemmatisation}
\label{se:lemmatisation}
\begin{itemize}
\item Lemmatization is the process of finding a base form for a word.
\item Lemmatization is a normalization technique. \cite{Korenius2004}
\item Homographic and inflectional word forms cause ambiquity. \cite{Korenius2004}
\item Compound words cause problems. \cite{Korenius2004}
\item Lemmatization is better than stemming for clustering of documents written in Finnish because of it's highly inflectional nature. \cite{Korenius2004}
\item Lemmatization catches better the semantic meaning of a word, as can be deducted from a better clustering performance.
\item Omorfi does lemmatization based on morphological analysis
\item Omorfi produces multiple lemmas which need to be disambiguated
\item Disambiguation can be done with selecting most probable word, given the context, by language model
\item \cite{Kestemont2016} try to solve lemmatization as a neural net classification problem, where lemmas are the class labels
\item Method of \cite{Kestemont2016} cannot produce lemmas not seen on training time. This restriction also applies to Word2vec by \cite{Mikolov2013}
\item Lemmatization has received a lot of research attention for highly inflectional languages, see \cite{Kestermont2016}
\item Unknown token can be taken as a variation of known token, search with e.g. Levenshtein edit distance
\item There has been almost none previous work using deep learning for lemmatization before \cite{Kestemont2016}
\end{itemize}

\section{Machine Translation}
\label{se:machine_translation}
\begin{itemize}
\item See section 2 of \cite{Kestemont2016} for sources on Internet to standard language translitteration
\item NLP tools suffer with texts with a lot of ortographical variation, one solution is to translate them. \cite{Kestemont2016}
\end{itemize}

\section{Intent Recognition and Slot Detection}
\label{se:intetn_prediction_and_slot_detection}
\begin{itemize}
\item Intent prediction is determining user's intents from their utterances (messages). Intents can be seen as functions to call in traditional programming.
\item Slot detection is identifying relevant actionable pieces of utterance \cite{Bhargava2013}. Slots can be seen as function parameters.
\item \cite{Bhargava2013} reduce error rates of intent prediction by 6,7\% and 8.7\% for transcribed data and automatically recognized data respectively when using intents from previous messages.
\item \cite{Bhargava2013} find no significant difference for slot detection by using context information.
\item Performance of \cite{Bhargava2013} for intent prediction is increased from 97,1\% to 97.3\% on transcribed data. However they use Viterbi algorithm with full access to future of the dialog, not realistic in production.
\item Most systems assume a single intent per utterance leading to unnatural dialogue experience. \cite{Xu2013}
\item Approaching multi-intent recognition by selecting top-K hypotheses from a single intent classifier yields poor results. \cite{Xu2013}
\item Multi-label learning works better. Splitting into a K binary classifiers, or combining multiple labels into a single label classification problem. \cite{Xu2013}
\item Usually K is a system design choice. \cite{Xu2013}
\end{itemize}


\chapter{Deep Learning}
\label{ch:deep_learning}

\section{Activation Functions}
\begin{itemize}
\item Sigmoid has a problem with vanishing gradients when using multiple hidden layers
\item ReLU fixes this problem (still has exploding gradients problem)
\item ReLU6 solves expoloding gradients also, but introduce vanishing gradients when x > 6?
\item Softplus is the smooth version of ReLU but computationally more expensive.
\item ELU is like ReLU but doesn't die off to zero. \cite{Clevert2015}
\end{itemize}

\section{Recurrent Neural Networks}
\label{se:recurrent_neural_networks}
\begin{itemize}
\item DNNs (Deep Neural Networks) cannot be used to map sequences to sequences since they require the dimensionality of inputs and outputs to be fixed. \cite{Sutskever2014}
\item RNN is suted for medling sequential phenomena. \cite{Kim2016}
\item In theory RNN can summarize all historical information, but in practice vanilla RNN performs poorly with long sequences due to vanishing/exploding gradients. (Bengio, Simard and Frasconi 1994), \textit{see Model chapter of \cite{Kim2016}}
\item Long short-term memory (LSTM) networks address the problem of vanishing gradients with long sequences by adding a memory cell. (Hochreiter and Schmidhuber 1997), \textit{see Model chapter of \cite{Kim2016}}
\item Gradient exploding remains a problem, but is easily addressable in practice by simple strategies such as gradient clipping. \cite{Kim2016}
\item Adding more layers such that input of a layer is the hidden state of previous layer is often crucial for significant performance increase. (Pascanu et al. 2013), \textit{see Model chapter of \cite{Kim2016}}
\item Deep LSTM of \cite{Sutskever2014} significantly outperformed their shallow LSTM, each layer decreasing the perplexity by nearly 10\%.
\item Encoder-decoder used in translation from English to French gains significant performance increase when reversing the source sentence word order. \cite{Sutskever2014}
\item \cite{Sutskever2014} speculate that reversing the source sentence helps by making backpropagation work better with shorter dependencies of the sentences' first words. Reversing the source sentence did not deteriorate the translation performance of later parts of the sentence, as was initially believed by \cite{Sutskever2014}
\item Bi-directional RNN is composed of two RNN of which the first reads the sequence in forward direction and the second reads the sequence in reverse direction, hidden states are concatenated. \cite{Chung2016}
\end{itemize}

\section{Encoder-Decoder}
\begin{itemize}
\item Introduced by \cite{Sutskever2014} and \cite{Cho2014}
\item Used in machine translation \cite{Chung2016}
\item Dual RNN architechture where 1st RNN encodes a sequence of tokens to fixed length vector and 2nd RNN decodeds that vector representation to a target sequence of tokens. \cite{Cho2014}, \cite{Bahdanau2014}, \cite{Sutskever2014}
\item Both RNNs are jointly trained to maximize conditional probability of a target sequence given a source sequence. \cite{Cho2014}, \cite{Bahdanau2014}
\item Encoder creates a summary of the entire input sequence. \cite{Cho2014}
\item Decoder samples a token at a time using input sequence summary, it's own RNN hidden state(s) and/or previously generated sample(s). \cite{Cho2014}, \cite{Bahdanau2014}
\item See figure 1 on page 2 of \cite{Cho2014} for architecture depiction.
\item Can be used to generate a target sequence based on input sequence. Can also be used to score a given pair of input-output sequences. \cite{Cho2014}
\item RNN Encoder-decoder captures semantic and syntactic structures of phrases. \cite{Cho2014}
\item Encoder-decoder needs to compress all the relevant information of the sentece in a single fixed length vector. This becomes a problem when sentence length increases, larger model is required. \cite{Bahdanau2014}.
\item Encoder-decoder have no explicit alignment. \cite{Liu2016a}
\item Input and output sequences can be of different length. \textbf{\textcolor{red}{Citation?}}
\item \cite{Chung2016} have character level encoder-decoder sequence-to-sequence machine translation system. Using sub-word level symbols in source side, full character level only in decoder.
\item \cite{Chung2016} use novel RNN (bi-scale RNN) on the target side for better handling of multiple timescales
\item Character level decoder relaxes the problem with computational complexity (of softmax function) with large target vocabulary. \cite{Chung2016}
\item Using character level only encoding and decoding removes the need to know how to do segmentation of characters into words, which is a problem in models which use character level word encodings such as C2W. \cite{Chung2016}
\item All inflectional forms of word result in very large vocabulary, more efficient encoding can achieved with lexeme (lemma) and morphemes, but requires a lemmatizer and morphological analyser. \cite{Chung2016}
\item Furthermore model may not perform well with common words if the morphological form is rare. \cite{Chung2016}
\end{itemize}

\section{Attention Mechanism}
\begin{itemize}
\item \cite{Bahdanau2014} introduced attention mechanism in neural machine translation as a solution to deteriorating performance with long input sentences. \cite{Sutskever2014} speculate that similar improvement could have been gained with simply reversing the source sentence word order.
\item System of \cite{Bahdanau2014} soft searches words in source sentence for each word in target sentence.
\item System of \cite{Bahdanau2014} does not try to encode the whole sentence as a fixed size vector, but instead input sentence is encoded as sequence of vectors which are weighted at the decoding time.
\item See section 3.1 of \cite{Bahdanau2014} for description of decoder with attention.
\item \cite{Bahdanau2014} use beam search to approximate maximum conditional probability on the trained model.
\item Attention allows encoder-decoder to learn soft alignment of input and output sequences. \cite{Liu2016a}
\end{itemize}


\chapter{Our System}
\label{ch:our_system}
\begin{itemize}
\item This thesis only considers customer message understanding.
\item Greetings, Chatting, information retrieval and response generation are important part of a dialogue system but have been left out of scope of this thesis.
\item Large portion of the research on the conversational models focus on chatter systems where utterance responses are retrieved or generated from response corpus. These systems do not aim to solve customer problem by incorporating relevant information retrieved from a specific knowledge base.
\item Utterance systems are not suitable for customer support out side of simple frequently asked questions.
\item Furthermore large portion of responses generated by neural conversation models are safe responses e.g. \textit{I don't know} \cite{Li2015}.
\item Conversational models trained on customer support history incorporate data from long time period and as such data may contain outdated information which cannot be corrected unless sentences containing outdated information are replaced. Finding and replacing such information from customer support dialogues requires reading much of the dialogues by humans and rewriting conversation history; task both time consuming, error prone and expensive.
\item Also incorporating new information into such dataset requires manual conversation synthesis where humans write new simulated conversations into a dialogue history used for training the system.
\item Other ways for updating old and adding new information are required for real world customer support. Our system draws it's information from a knowledge base with traditional user interface for updating and adding new knowledge.
\end{itemize}

\section{Pipeline}
\label{se:pipeline}
\begin{enumerate}
\item Spell checking and correction (Omorfi)
\item Machine translation from colloquial speech to standard dialect
\item Co-Reference replacement
\item Named Entity Recognition and replacement
\item POS-tagging (Syntaxnet)
\item Dependency parsing (Syntaxnet)
\item Lemmatization (Omorfi)
\item Character to word with \cite{Ling2015} for word lemmas and POS-tags
\item 1-of-V coding for features
\item Dependencies?
\item Query generation
\item Querying knowledge base with generated query
\item Response rendering with templates (sprintf?)
\end{enumerate}

\section{C2W}
\label{se:c2w}
\begin{itemize}
\item Character vocabulary needs to fixed for character embedding layer.
\item We selected \verb£ !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ£ \verb£[\]^_`abcdefghijklmnopqrstuvwxyz{|}~ÄÅÖäåö€£ as our character vocabulary.
\item This contains ASCII characters from 32 to 127, scandic characters in lower and upper case and € sign.
\item Selected character vocabulary covers 99,933\% of character usages in Finnish internet parsebank n-gram dataset.
\item ASCII end-of-text (ETX) and substition (SUB) characters were added to character vocabulary to handle sequence length padding and out of vocabulary characters respectively.
\end{itemize}

\section{C2W2V}
\begin{itemize}
\item Learning word2vec embeddings from character level sequences by mapping character embeddings sequentially with RNN to pre-trained word2vec vector representations.
\item Training with word2vec vocabulary of 2208293 words
\item Word2vec embeddings trained by \cite{Ginter2014} from 5-gram data.
\item character level sequences are embedded with C2W architecture of \cite{Ling2015}.
\item Using additive inverse of cosine similarity of word2vec embedding and predicted C2W embedding as loss function.
\item Using mean squared error as loss function yielded even worse results when validating with cosine similarities of different words and comparing to cosine similarities of the same words calculated from word2vec embeddings.
\item Optimizing with Adam gradient descent optimizer.
\item Training converged to cosine similarity of 0.3680 and would not get past that with varying hyperparameters and increasing model size.
\item Fail to learn word2vec representations from character level sequences is probably due to word2vec representations capturing distributional sematics which don't seem to be captured in character level sequences.
\item In other words lemma, stem, morphology and inflections convey only a part of word's semantic meaning.
\item Purpose is to use C2W2V to create vector embeddings for words not found in word2vec vocabulary.
\item Cosine similarity of 0.3680 conveys some meaning and might function as a alternative to using unknown token representation.
\end{itemize}

\section{Evaluation}
\label{se:evaluation}
\begin{itemize}
\item Recently end-to-end dialogue system have adopted metrics from amchine translation and text summarization. These don't work so well. \cite{Liu2016}
\item \cite{Liu2016} considers unsupervised utterance systems where response is generated or selected from a set of possible responses. This is irrelevant since we are building a problem solving dialogue system instead of chattering system.
\end{itemize}


\chapter{Conclusions and Future Work}
\label{ch:conclusions_and_future_work}


%
% The bibliography, i.e the list of references (3 options available)
%
\newpage


% Extra for Finnish theses

\renewcommand{\bibname}{Bibliography}     % Bilingual babel puts Finnish ``Kirjallisuttaa'' otherwise. Strange...
%\renewcommand{\bibname}{Lähteet}         % Set Finnish header, remove this if using English
%\addcontentsline{toc}{chapter}{Lähteet}  % Include this in TOC
\addcontentsline{toc}{chapter}{\bibname}  % Include this in TOC


\printbibliography                  % a) heading in English
%\printbibliography[title=Lähteet]   % b) heading in Finnish
%\addtocontents{toc}{%               % b) add Finnish heading to table of contents
%\protect\noindent Lähteet\protect\par
%} 


%
% Appendices are optional. 
% This part is semi-ugly at the moment. Please give feedback if can
% improve it.
\appendix
\pagestyle{headings}
% \renewcommand{\appendixname}{Liite} % Extra. Set Finnish prefix for page header

%
% a) Not-so-handy way, but at least it works
% 
\def\appA{APPENDIX A. Something extra} % Define the name and numbering manually
\chapter*{\appA}                       % Create chapter heading
\markboth{\appA}{\appA}                % Set page header
\addcontentsline{toc}{chapter}{\appA}  % Include this in TOC
% Note that \label does not work with unnumbered chapter

Appendices are purely optional.  All appendices must be referred to in
the body text

\def\appB{APPENDIX B. Something completely different} % Define another new command
\chapter*{\appB}                       % As above, but use \appB instead of \appA
\label{app:B}
\markboth{\appB}{\appB}                     
\addcontentsline{toc}{chapter}{\appB}  


You can append to your thesis, for example, lengthy mathematical
derivations, an important algorithm in a programming language, input
and output listings, an extract of a standard relating to your thesis,
a user manual, empirical knowledge produced while preparing the
thesis, the results of a survey, lists, pictures, drawings, maps,
complex charts (conceptual schema, circuit diagrams, structure charts)
and so on.


%
% b) The other option is to use numbered chapter and our baseline
% template report.cls numbers them as A, B... The heading and TOC do
% not include prefix 'Appendix' although the page header does.
%\chapter{name of the appendix}
%\label{app:A}                          % For cross-references



\end{document}

