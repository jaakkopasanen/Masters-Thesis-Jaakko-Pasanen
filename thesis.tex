\documentclass[12pt,a4paper,english
% ,twoside,openright
]{tutthesis}
%\documentclass[12pt,a4paper,finnish]{tutthesis}

% Note that you must choose either Finnish or English here and there in this
% file.
% Other options for document class
  % ,twoside,openright   % If printing on both sides (>80 pages)
  % ,twocolumn           % Can be used in lab reports, not in theses

% Ensure the correct Pdf size (not needed in all environments)
\special{papersize=210mm,297mm}


% LaTeX file for BSC/MSc theses and lab reports.
% Requires the class file (=template) tutthesis.cls and figure files,
% either tut-logo, exampleFig (as pdf or eps) and example_code.c
% Author: Sami Paavilainen (2006)
% Modified: Heikki Huttunen (heikki.huttunen@tut.fi) 31.7.2012.
%           Erno Salminen, @tut.fi, 2014-08-15
%             - added text snippets from the writing guide
%             - added lots of comments: both tips and alternative styles
%             - added an example table
%             - and so on...

%
% Define your basic information
%
\author{Jaakko Pasanen}
\title{Natural Language Syntactic Parsing with Neural Networks} % primary title (for front page)
\titleB{Luonnollisen kielen syntaksin parsiminen neuroverkoilla}     % translated title for abstract
\thesistype{Master of Science thesis} % or Bachelor of Science, Laboratory Report... 
\examiner{Ari Visa} % without title Prof., Dr., MSc or such

% Put your thesis' main language last
% http://mirrors.ctan.org/macros/latex/required/babel/base/babel.pdf
\usepackage[finnish, main=english]{babel}

% http://www.ctan.org/pkg/biblatex
\usepackage[
  style=authoryear,
  maxcitenames=2,
  backend=biber,
  firstinits=true
]{biblatex}
\bibliography{thesis_refs.bib}
%% Note that option style=numeric works as well
\usepackage{fontspec}
\usepackage{amsfonts}


% You can also add your own commands
\newcommand\todo[1]{{\color{red}TODO: #1}} % Remark text in braces appears in red
\newcommand{\angs}{\textsl{\AA}}              % , e.g. slanted symbol for Ångstöm

% Preparatory content ends here


\pagenumbering{roman} % was: {Roman}
\pagestyle{headings}
\begin{document}

% Special trick so that internal macros (denoted with @ in their name)
% can be used outside the cls file (e.g. \@author)
\makeatletter

%
% Create the title page.
% First the logo. Check its language.
\thispagestyle{empty}
\vspace*{-.5cm}\noindent
\includegraphics[width=8cm]{tty_tut_logo}   % Bilingual logo

% Then lay out the author, title and type to the center of page.
\vspace{6.8cm}
\maketitle
\vspace{6.7cm} % -> 6.7cm if thesis title needs two lines

% Last some additional info to the bottom-right corner
\begin{flushright}  
  \begin{minipage}[c]{6.8cm}
    \begin{spacing}{1.0}
      %\textsf{Tarkastaja: Prof. \@examiner}\\
      %\textsf{Tarkastaja ja aihe hyväksytty}\\ 
      %\textsf{xxxxxxx tiedekuntaneuvoston}\\
      %\textsf{kokouksessa dd.mm.yyyy}\\
      \textsf{Examiner: Prof. \@examiner}\\
      \textsf{Examiner and topic approved on}\\ 
      \textsf{11th October 2017}\\
    \end{spacing}
  \end{minipage}
\end{flushright}

% Leave the backside of title page empty in twoside mode
\if@twoside
\clearpage
\fi

%
% Use Roman numbering I,II,III... for the first pages (abstract, TOC,
% termlist etc)
\pagenumbering{Roman} 
\setcounter{page}{0} % Start numbering from zero because command 'chapter*' does page break

% Some fields in abstract are automated, namely those with \@ (author,
% title in the main language, thesis type, examiner).
\chapter*{Abstract}

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@title\\   % use \@titleB when thesis is in Finnish
         \textsf{Tampere University of Technology}\\
         \textsf{\@thesistype, 42 pages, 0 Appendix pages} \\
         \textsf{October 2017}\\
         \textsf{Master's Degree Programme in Automation Technology}\\
         \textsf{Major: Learning and Intelligent Systems}\\
         \textsf{Examiner: Prof. \@examiner}\\ % 
         \textsf{Keywords: Natural Language Processing, Syntactic Parsing, Neural Networks, Deep Learning}\\
\end{spacing}

Convesational user interfaces have made strong appearance during the last couple of years. Most growth with conversational UIs can be seen with customer service moving from phone to chat. As big as the hype surrounding the conversational user interfaces is, they often cannot surpass traditional alternatives for the use cases they are used without actually understanding the language user speaks. This has crated a large demand for artificial intelligence which can understand users in their natural language.

Natural language used by humans is tremendously complex without people actually realizing that. Understanding something this complex often requires system which divedes the problem into smaller sub-problems are tries to tackle those, a divide and conquer paradigm. Natural language processing tools are often built as pipelines where more information is mined from the text in each step. Finding syntactic features, such as part of speech and lemma,  is one such step, and is the focus of this thesis.

Main objective for this thesis was to build a neural network architecture which can classify lemmas and parts of speech for the input text. Research hypothesis was then to determine if such architecture could be modified to do the both tasks at the same time and if such change would improve classification performance of the model.

Doing experiments with Finnish Universal Dependencies dataset revealed that lemmatization benefits from jointly learning to POS-tag, but POS-tagging performance could not be improved. Best absolute lemmatization results were gained by using correct POS-tags as input features, but since they are not available for live predictions the result has no practical meaning.

 


% Foreign students do not need Fininsh abstract (tiivistelmä). Move
% this before English abstract if thesis is in Finnish. Move also the
% otherlanguage command to the English abstract (if needed).

\begin{otherlanguage}{finnish} %  Following text in in 2nd language
\chapter*{Tiivistelmä} % Asterisk * turns numbering off

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@titleB\\  % or use \@title when thesis is in Finnish
         \textsf{Tampereen teknillinen yliopisto}\\
         \textsf{Diplomityö, 42 sivua, 0 liitesivua}\\ %
         \textsf{Lokakuu 2017}\\
         \textsf{Automaatiotekniikan koulutusohjelma}\\
         \textsf{Pääaine: Oppivat ja älykkäät järjestelmät}\\
         \textsf{Tarkastajat: Prof. \@examiner}\\ % automated, if just 1 examiner
         \textsf{Avainsanat: Luonnollisen kielen käsittely, syntaksin parsiminen, neuroverkot, syväoppiminen}\\
\end{spacing}

Keskustelevat käyttöliittymät ovat nousseet vahvasti perinteisten käyttöliittymien rinnalle viimeisen parin vuoden aikana. Suurin kasvu tällä hetkellä on nähtävissä asiakaspalvelun siirtymisessä puhelimesta chattiin. Vaikka hype keskustelevien käyttöliittymien ympärillä on valtavaa, eivät ne useinkaan pysty päihittämään perinteisiä käyttöliittymiä ilman että käyttäjää ymmärretään sen omalla kielellä. Tästä on syntynyt suuri kysyntä tekoälylle joka ymmärtää käyttäjiä heidän luonnollisella kielellään.

Ihmisten käyttämä luonnollinen kieli on valtavan monimutkaista ilman että ihmiset useinkaan ymmärtävät sitä. Näin monimutkaisen asian ymmärtäminen usin vaatii että ongelma jaetaan pienenpiin osaongelmiin ja pyritään ratkaisemaan ne. Työkalut luonnollisen kielen käsittelyyn rakennetaan usein sarjaksi askelia joista jokainen kaivaa lisää informaatiota tekstistä. Syntaktisten piirteiden, kuten sanaluokkien ja perusmuotojen, etsiminen yksi tällainen askel, ja onkin tämän opinnäytetyön painopiste.

Ensisijainen tavoite tälle opinnäytetyölle oli rakentaa neuroverkkoarkkitehtuuri joka pystyy luokittelemaan sanoille sanaluokat ja perusmuodot. Tutkimushypoteesi oli selvittää voiko arkkitehtuurin muokata sellaiseksi että se suorittaa molemmat tehtävät samanaikaisesti ja saavutetaanko muutoksella parannus verkon suorituskykyyn.

Suomenkielisellä Universal Dependencies datasetillä tehdyt kokeet paljastivat että perusmuotojen luokittelu hyötyy samanaikaisesta sanaluokkien luokittelusta, mutta kuitenkaan suorituskykyä sanaluokkien luokitteluun ei pystytty parantamaan. Paras absoluuttinen tulos perusmuotojen luokittelulle saavutettiin käyttämällä oikeita sanaluokkia lisäpiirteenä verkolle, mutta koska täysin oikeita sanaluokkia ei ole käytettävissä todellisessa tilanteessa ei tällä tuloksella ole juuri käytännön merkitystä.



\end{otherlanguage} % End on 2nd language part


\chapter*{Preface}

This thesis was done at ultimate.ai, a customer service software company, as part of ongoing research into natural language understanding during 2017.

I would like to thank my examiner professor Ari Visa for his guidance and valuable feedback in formulating the hypothesis, selecting thesis scope and writing process.

~ 
% Tilde ~ makes an non-breakable spce in LaTeX. Here it is used to get
% two consecutive paragraph breaks

Tampere, 15.8.2017

~

Jaakko Pasanen


% Add the table of contents, optioanlly also the lists of figures,
% tables and codes.

%\renewcommand\contentsname{Sisällys} % Set Finnish name, remove this if using English
\setcounter{tocdepth}{3}              % How many header level are included
\tableofcontents                      % Create TOC


%
% Term and symbol exaplanations use a special list type
%
\chapter*{List of abbreviations and symbols}
\markboth{}{}                                % no headers
%\chapter*{Lyhenteet ja merkinnät}

% You don't have to align these with whitespaces, but it makes the
% .tex file more readable
\begin{termlist}
\item[ANN] Artificial Neural Network
\item[CPU] Central Processing Unit
\item[GPU] Graphics Processing Unit
\item[MLP] Multi-layer perceptron
\item[NLP] Natural Language Processing
\item[POS] Part-of-speech; also called lexical category
\item[RNN] Recurrent neural network
\item[TUT] Tampere University of Technology
\item[UI] User Interface
\end{termlist}



\newpage

% The actual text begins here and page numbering changes to 1,2...
% Leave the backside of title empty in twoside mode
\if@twoside
%\newpage
\cleardoublepage
\fi

\pagenumbering{arabic}
\setcounter{page}{1} % Start numbering from zero because command
                     % 'chapter*' does page break
\renewcommand{\chaptername}{} % This disables the prefix 'Chapter' or
                              % 'Luku' in page headers (in 'twoside'
                              % mode)


\chapter{Introduction}
\label{ch:intro}
2-4 pages


\chapter{Natural Language Processing}
\label{ch:natural_language_processing}
\begin{itemize}
\item Natural Language Processing is vastly wide field, this thesis discusses only on the sections of NLP relevant to the experiments.
\item Subfields such as sentence segmentation and sentiment analysis are out of scope of this thesis.
\item Most of the NLP work has been for english.
\item Cross-linguistic annotation and parsing has been a reality only after introduction of The Universal Dependencies project and SyntaxNet.
\item Similarly Finnish parsing has been unreachable until the first Finnish corpus Turku Dependency Treebank \cite{Haverinen2014} and cross-linguistic parsers.
\item Traditionally NLP systems are tailored to the single problem at hand with hand engineered features suited for the problem. Recently general approach has received interest where feature engineering and task specific architectures are not needed. \cite{Collobert2011}, \cite{Zhang2015}
\item See Chapter 2.1 for Finnish Language quirks in \cite{Korenius2004}
\item This chapter focuses on syntactic parsing tasks
\item There are plenty of other tasks also
\end{itemize}

\section{Feature Engingeering in NLP}
\label{ch:feature_engineering_in_nlp}
\begin{itemize}
\item Machine learning algorithms require words to be represented quantifiable features such as IDs or real number vectors.
\item Traditional feature selection requires hand engineered features.
\item Engineered features hog 95\% of the computation time. \cite{Chen2014}
\item Traditionally words have been represented by indices. \cite{Mikolov2013}
\item Next step was to use 1-of-V coding.
\item Index representation is simple as computationally cheap, making use of huge datasets possible. Simple models with huge data outperform complex models with less data. \cite{Mikolov2013}
\item See LSA and LDA for previous systems. Neural networks significantly outperform LSA in preserving linearities. LDA doesn't scale for large datasets. \cite{Mikolov2013}
\end{itemize}

\section{Word Embeddings}
\label{se:word_embeddings}
\begin{itemize}
\item \textbf{\textcolor{red}{see section 1.2 of \cite{Mikolov2013} for previous work and history of word embeddings}}
\item Word embeddings represent words as n-dimensional vectors. \cite{Mikolov2013}
\item LSA leverages statistical information of a corpus but performs poorly on word analogy task. \cite{Pennington2014}
\item Skip-gram is good for word analogies but doesn't utilize corpus statistics well since vectors are trained on local context. \cite{Pennington2014}
\item Word embeddings try to map words with semantic similarities close to each other. Words may have several types of similarities such as \textit{France} and \textit{Italy} are countries but \textit{dogs} and \textit{triangles} are both in plural form. \cite{Mikolov2013a}
\item \cite{Chen2014} use 50 dimensional word embeddings created with Word2vec.
\item \cite{Chen2014} also use embeddings for POS tags and dependecy arcs. Only embedding POS tags has clear benefit, \cite{Chen2014} suspect that embedding arc labels have no effect since POS tags already contain the relational information.
\item Word embeddings with lookup table generalize poorly with morpohlogically rich languages such as Finnish. \cite{Takala2016}
\item Morphologically rich languages benefit from breaking the word into sub-parts, RNN based character level model is not compared with Stem+ending. \cite{Takala2016}
\item Word embeddings obtained through neural language models exhibit the property whereby semantically close words are close in the embedding vector space. \cite{Kim2016}
\item Most of the word embedding libraries work on principle of fixed vocabulary where embeddings are computed for all words in vocabulary. It's difficult to handle out-of-vocabulary words since word spelling contains only a small part of the word's semantic meaning.
\item Problem for morphologically rich languages can be relaxed by lemmatizing all words because lemmas don't suffer as much from the vocabulary explosion.
\end{itemize}

\subsection{Word2vec}
\begin{itemize}
\item \cite{Mikolov2013}
\item Can be used with datasets of billions of words
\item Has two models: Continuous bag-of-words and continuous skip-gram
\item Continuous bag-of-words predicts current word from the context (surrounding words)
\item Continuous skip-gram predicts context (surrounding words) from current word.
\item Continuous Bag-of-Words is better for small datasets, continuous skip-gram is better for large datasets.
\item CBOW is better for syntax, Skip-gram is better for semantics.
\item Can be used to find semantic relationships like vector('biggest') - vector('big') + vector('small') => vector('smallest') 
\item State of the art (as of 2013). Since several new architectures have proposed improvements such as FastText by Facebook and GloVE by \cite{Pennington2014}.
\end{itemize}

\subsection{Charater to Word}
\begin{itemize}
\item \cite{Ling2015}
\item Word embeddings can be generated from character sequences with significantly better performance for morphological languages.
\item Requires only single vector for each character type. Particularly good for morphological languages where word type count may be infinite.
\item Orthographical and functional (syntactic and semantic) relationships are non-trivial: \textit{butter} and \textit{batter} are orthographically close but functionally distant, \textit{rich} and \textit{affluent} are orthographically distant but functionally close. \cite{Ling2015} resort to LSTM networks for learning the relationships.
\item Word lookup tables are unable to generate representations for previously unseen words, as is required for morphology. \cite{Ling2015}
\item C2W can generate embeddings for unseen words.
\item C2W is computationally more expensive than word lookup tables, but can be eased by saving word embeddings for most frequent words since the words embedding for a character sequence (word) does not change.
\item During training word embeddings change but not inside a single batch, thus it is computationally cheaper to use large batches for training.
\item C2W can be replaced with word lookup tables for downstream processing since input and output of both methods are the same.
\end{itemize}

\section{Annotations}
\label{se:annotations}
\begin{itemize}
\item Stanford Dependencies by \cite{DeMarneffe2006}
\item Stanford Dependencies emerged as de facto annotation scheme for english, but has been adapted to several other languages including Finnish. \cite{Nivre2016}, \cite{Haverinen2014}.
\item Turku Dependency Treebank has been tranformed into universal dependencies. \cite{Pyysalo2015}
\item Unified annotation scheme reduces need for cross-language adaptations in downstream development. \cite{Petrov2012}
\item Universal Dependencies project started from the requirement for cross-linguistically consistent treebank annotations even for morphological languages. \cite{Nivre2016}.
\item Universal Dependencies project was born from merging several previous attempts to form a cross-linguistically sound dependency annotation schemes. \cite{Nivre2016}
\item UD data has been encoded in the CoNLL-U format, a revision of the popular CoNLL-X format. \cite{Nivre2016}
\item UD treebanks released in November 2015. \cite{Nivre2016}
\end{itemize}

\subsection{Turku Dependecy Treebank}
\label{ss:turku_dependency_treebank}
\begin{itemize}
\item \cite{Haverinen2014}
\item Treebanks are needed in computational linguistics.
\item First Finnish treebank.
\item Open licence, including for text annotated
\item 204339 tokens, 15126 sentences
\item Based on Stanford Dependency scheme with minor modifications to exclude phenomena not present in Finnish and to include new annotations not present in English.
\item Transposed to CoNNL-U scheme by universal dependencies project
\item Connexor Machinese Syntax is the only currenty available Finnish full dependency parser.
\item Texts from 10 different categories ranging from news and legal text to blog entries and fiction.
\item Dependency parsing is done manually with full double annotation process.
\item Uses Omorfi for morphological analysis. Ambiguous tokens are handled partly manually, partly rule based and partly with machine learning.
\item FTB uses 3 different taggers for morphology, check them out!
\item FTB is 97\% grammar examples, meant for rule based POS tagger development
\end{itemize}

\section{Tokenization}
\begin{itemize}
\item Rule based approach to tokenization would mostly split sentence into words from spaces and separate punctuation from the words.
\item Symbols and codes are more challenging for rule based tokenizers.
\item Neural net based approach to tokenization can be done with seq2seq model which inserts linefeeds.
\item Another neural net approach to tokenization is to do character classification where each character is classified to be first character of a word.
\item Good tokenization is very important for good downstream processing results; very small errors in tokenization can lead to extremely large errors in subsequent tasks (ask sitation from Honain). 
\item This thesis uses gold standard tokenization of the data files and therefore tokenization is not included in the experiments. 
\end{itemize}

\section{POS-tagging}
\begin{itemize}
\item Started from rule based taggers
\item Tagger by \cite{Brill1992} (known as Brill tagger) learns the rules and as such can be considered as a hybrid approach
\item Contemporary research is focused on statistical and NN based taggers
\item Rest of this section focuses on statistical parsers
\item \cite{Ling2015} introduced S-LSTM based State-of-the-art tagger
\item \cite{Andor2016} Improved accuracy with transition based tagger
\item \cite{Chen2014} were first to represent POS-tag and arc labels as embeddings
\item \cite{Andor2016} and \cite{Weiss2015} built their solutions based on \cite{Chen2014}
\item \cite{Nivre2004} introduced system for transition based taggers known as arc-starndard system. \cite{Chen2014}
\end{itemize}

\section{Lemmatisation}
\label{se:lemmatisation}
\begin{itemize}
\item Lemmatization is the process of finding a base form for a word.
\item Lemmatization is a normalization technique. \cite{Korenius2004}
\item Homographic and inflectional word forms cause ambiquity. \cite{Korenius2004}
\item Compound words cause problems. \cite{Korenius2004}
\item Lemmatization is better than stemming for clustering of documents written in Finnish because of it's highly inflectional nature. \cite{Korenius2004}
\item Lemmatization catches better the semantic meaning of a word, as can be deducted from a better clustering performance.
\item Omorfi does lemmatization based on morphological analysis
\item Omorfi produces multiple lemmas which need to be disambiguated
\item Disambiguation can be done with selecting most probable word, given the context, by language model
\item \cite{Kestemont2016} try to solve lemmatization as a neural net classification problem, where lemmas are the class labels
\item Method of \cite{Kestemont2016} cannot produce lemmas not seen on training time.
\item Lemmatization has received a lot of research attention for highly inflectional languages, see \cite{Kestemont2016}
\item Lemmatization of english is considered a solved problem, rule based or hybrid approaches can do practically flawless job.
\item There has been almost none previous work using deep learning for lemmatization before \cite{Kestemont2016}
\end{itemize}

\section{Morphological Parsing}
\begin{itemize}
\item Multi-class classification problem
\end{itemize}

\section{Structural Parsing}
\label{se:structural_parsing}
\begin{itemize}
\item Aims to find structure of a sentence.
\item Commonly divided to two different tasks: constituency parsing and dependency parsing.
\item Constituency parser creates a parse tree of constituencies.
\item Dependency parser creates a parse tree of word token dependencies.
\item Constituency parsers are slower but more informal than dependency parsers. \cite{Fernandez-Gonzalez2015}
\item \cite{Fernandez-Gonzalez2015} show that it is possible to build constituency parser with dependency parser by reducing constituents to dependency parsing.
\item CoNLL uses dependency parse trees.
\end{itemize}

\subsection{Transition Based Parsers}
\begin{itemize}
\item Good balance between efficiency and accuracy \cite{Weiss2015}
\item Parsed left to right; at each position the parses chooses action from a set of possible actions.
\item Greedy models are fast but error prone and need hand engineered features \cite{Weiss2015}
\item Actions can be chosen by ANN to avoid hand engineering \cite{Chen2014}, \cite{Weiss2015}
\end{itemize}


\chapter{Experiements on Joint Model for POS-tagging and Lemmatization}
\label{ch:experiements_on_joint_model_for_pos_tagging_and_lemmatization}
Focus of this thesis is to prove or disprove the hypothesis that joint learning of lemmatization and POS-tagging with neural networks can obtain better performing network for one of both tasks than learning the said tasks separately. Joint learning in the context of this thesis means learning and predicting both outputs with single neural network architecture and single forward pass. Joint learning model is compared to separate tasks baseline.

Lemmatization and POS-tagging were selected as tasks for this thesis because they are well studied and results from other research projects exist making baseline validation possible. Both tasks are also popular choises as input features for downstream processing. Although lemmatization is considered by some to be solved for morphologically poor languages, it is not for morphologically rich languages such as Finnish. Lemmatization is especially interesting for Finnish because properly done lemmatization would allow usage of word level features such as pre-computed word2vec vectors as input features in downstream tasks. All experiments of this thesis are performed with Finnish language.

Neural networks were selected as implementational approach for the problem mainly because of their flexible and architecturally general nature. Neural networks don't require architectural changes, other than maybe a hyper-parameter optimization, when adapting the network for new languages. Essentially same neural network can handle the natural language processing task at hand for any language. Possible exceptions are languages which are written on different level than european languages. Chinese has symbols only for words, has no letters at all, and as such might demand architectural changes.

Another convenient property of neural networks is their flexibility to adapt different tasks with sometimes very small architectural changes. Neural networks developed for this thesis share vast majority of components among lemma classifier and POS classifier, only the output layer is separate and has different number of nodes for said tasks. But even then both outputlayers are similar fully connected linear projection layers.

Lastly neural networks were selected because NLP has been researched for several decades and it appears that older, often statistical, methods have been already tuned close to their maximum. Neural networks on the other hand have shown very promising progress during the last couple of years, mainly due to ever decreasing price of computational resources and introduction or re-introduction of a few mathematical advancements which have made deep neural networks easier to train. 

Statistical methods such as bag-of-words might be trickier to implement as character-level models than neural networks. Counting words in a sentence of in a context of a word has for a years been the simplest baseline model for mulitude of NLP tasks. Bag-of-words works because words are essentially the basic sematic unit of a language. Character on the other hand convey very little meaning when not associated with other characters in order of appearance. Simply counting characters is therefore not going to reveal the underlying phenomena. Same explanation applies to some extent to other traditional models also.

Joint model approach was taken into inspection because joint learning models have not yet been studied very widly but results from the few studies have shown that joint models can achieve better results than separate models. Lemma and a part of speech of a word a tighly linked to each other. Lemma is the basic (almost) unique identifier of a word and each word has always a single fixed part-of-speech. This tight coupling of lemmas and part of speeches serve as a good foundation for building a joint learning model. 

Unfortunately lemmas do not identify a word in a completely unique way; multiple words may have same written base form. Nail is a written form for at least two different meanings, one being a fastener for attaching pieces of wood together with a hammer and the other being a keratin made envelope covering the tips of fingers and toes. Fortunately words with multiple meanings are more rare than not. This thesis simply omits the problems and implications which could and do arise from having shared tokens for multiple words. Such decision to omit the problems may not be as harmful as one might think; predicting correct lemma whether the nail is meant to be hit with a hammer or not does still produce a correct lemma, this becomes and issue only with the downstream tasks which might need the two to be separated.

Part of speech tagging can also still be done without too much hinderance. Often the two words have the same part of speech. In the case of fastener nail and finger nail problem does not exists since both are nouns. Separating "nails" as plural form of a noun "nail" from "nails" as colloquial form of a verb for love making can be done with the information provided by the context of the word.


\section{Neural Network Architechture}
\label{se:architecture}
Architecture used for experiments in this thesis is a multi-layer deep neural network composing an end to end pipeline handling everything needed from character representations to output classifications. Multiple layers in the architecture are not only layers of interconnected nodes as usually depicted by term layers in the context of neural networks. The architecture also contains multiple architectural layers such as character embedding layer, word embedding layer, context encoding layer and classification layer. To distinguish architectural layers from neural layers the former is going to be called components for the rest of this thesis. All the components, all layers which make the individual components and weights are learned at the same time. Learning all network parameters in a single training run makes training simpler and removes, or at least obfuscates, the possible compatibility issues and non-optimalities between the components and layers. Figure \ref{figure:architecture} shows high level architecture and data processing pipeline used in this work.

\begin{figure}[htbp]
\caption{High level architecture for neural network used}
\label{figure:architecture}
\centering
\includegraphics[width=15cm]{architecture.png}
\end{figure}

Neural network code was implemented with Python using popular computation libraries Tensorflow and Numpy. Tensorflow provides scripting APIs for other programming languages too but Python was selected because of it is fast to write and computational performance does not suffer compared to eg. C++ since all the expensive computations are made in the C++ based backend. Numpy is used in the data pre-processing phase where data is processed to be suitable for feeding as input to the actual neural network. Tensorflow was used for implementing the neural network as a computational graph. Tensorflow makes it fairly easy to implement compled multi-componen end to end architectures while abstracting the actual optimization work away from the developer.

Tensorflow also provides a layer of abstraction to execute the computational graph on either CPU or GPU. GPU computations for pleasantly parallel problems such as neural network optimization are significantly faster. Doing the network training and especially hyper-arameter optimization only on CPU would have not been feasible with the hardware resources available. If computations would have been forced to be ran on CPU, there would have not been enough resources to run sufficient number of hyper-parameter optimization runs. Neural network results are often very sensitive to having suitable hyper-parameters and therefore results obtained in the experiments in the worst case could have been unable to provide answer for the hypothesis.

\subsection{Lemmatizations as Classification Only Task}
Approach for lemmatization in this thesis is classification only. POS-tags are a limited set of 31 labels as defined in the Universal Dependencies project but such is not the case for lemmas. When classifying lemmas with neural network one needs an indexed vocabulary of all possible output labels. Having fixed and limited set of lemmas proves to be a challenge for lemmatization.

As discussed earlier the Finnish language suffers from vocabulary explosion even when considering only lemmas and omitting the inflections because Finnish language makes it possible to form compound words very freely. The number of possible combinations created by selecting two or more words for a compound word is way too large to be handled with a linear vocabulary. The word2vec vocabulary which is created from Finnish Internet Parse bank (\cite{Kanerva2014}) contains over 1,7 million unique lemmas. Learning to classify this number of lemmas with a corpus of 160 and some thousand tokens is obviously impossible task.

To circumvent the vocabulary explosion problem for output vocabulary fixed and limited set of lemmas were selected from the training set. Selected lemma vocabulary contains 90\% of the use cases in the Finnish Universal Dependencies 1.4 training dataset. The 90\% coverage is formed by selecting the most frequent words only. This lemma vocabulary contains less than nine thousand unique lemmas opposed to over 1,7 million in the word2vec vocabulary. All the lemmas that were left out of the selected vocabulary are treated as unknown tokens meaning that when classifying out of vocabulary lemma the neural network with output an unknown token "<UNK>". Since vocabulary covers 90\% of the uses in training set, 10\% of uses are left out making unknown token a most frequent label.

Having unknown tokens provides it's own set of challenges for downstream processing tasks: information that is supposed to be provided by the lemma is lost with unknown token. One approach for outputting all possible lemmas is to use a generative model such as encoder-decoder model popular in recent studies (\cite{Sutskever2014}, \cite{Cho2014}, \cite{Chung2016}, \cite{Bahdanau2014}, \cite{Liu2016a}, \cite{Chung2016}) for machine translation. Generative model does not classify indices to a fixed vocabulary but generates the lemma one character at a time.

Generative model does solve the problem with lemma vocabulary but introduces a myriad of other problems. Some of the most prominent problems being vastly increased architectural complexity and significantly increased computational complexity and memory requriements. Introducing a encoder-decoder model also introduces problems with observation metrics. POS-tagging and lemma classification are word level prediction tasks and are as such measured with word level metrics eg. accuracy or F1 score. However encoder-decoder model is a character level model which is also observed on character level. Mixing word level classifications and character level classifications fuzzies the meaning of used metrics for evaluation of network performance.

Because of the forementioned problems, the encoder-decoder model was not a part of the implementation used for this thesis and lemmatization is treated as a classification only task. Also it's worth noting that observing lemma classification with POS-tagging should prove to be sufficient for proving or disproving the hypothesis. 

\subsection{Word Embedding Component}
\label{ss:word_embedding_component}
Representing words with multi-dimensional real number vectors is required to encode semantic and syntactic meaning of the words in a way a neural network can understand them as is discussed in section \ref{se:word_embeddings}. Vector representations are created in this work at the same time as neural network is trained to classify lemmas and part of speeches. In other words no external word embeddings are used such as word2vec. Word embedding in this architecture is managed with character to word encoder similar to \cite{Ling2015}.

Word embedding process starts with representing characters as multi-dimensional real number vectors, called character embedding for the rest of this work. Character embeddings as a part of character to word encoder are also trained at the same time with rest of the network. Character embeddings are implemented as a single trainable Tensorflow variable, a two dimensional array where each row contains embedding vector for a single character in the character vocabulary. Character vocabulary is a fixed set of characters selected to represent majority of use cases in currently processed language, Finnish in this case.

Selected character vocabulary contains ASCII characters from index 32 to index 127, ie. all but ASCII control characters, as well as lower and upper case scandic letters used in Finnish and an euro sign €. This character vocabulary covers 99,933\% of character usages in Finnish internet parsebank. A fairly good representation for Finnish language with very reasonable vocabulary size of 103 characters. Characters not included in the selected character vocabulary were substituted with ASCII control character SUB which was added to character vocabulary. Character vocabulary also contained another ASCII control character ETX, which was used for padding all other words to length of longest word in the current mini-batch (Tensors are essentially arrays and as such do not tolerate variable lengths within a single dimension).

Words as input to word vector encoder are represented as Tensors of character embeddings, each row of a single input word contains character embedding for a single character in the word. Input words for the word vector encoder contain character embeddings for all characters in all words in all sentences selected for the mini-batch. If mini-batch size is selected to be 25, then word vector encoder input contains all characters and words for the selected 25 sentences.

\begin{table}[htbp]
\caption{Example input for Word vector encoder}
\label{table:word_vector_encoder_input}
\centering
\begin{tabular}{|r|c|c|c|c|l|}
  \hline
  & $d_0$ & $d_1$ & ... & $d_{299}$ & \\
  \hline
  \hline
  K & 0.43 & 0.05 & & 0.37 & $t_0$ \\
  i & 0.32 & 0.62 & & 0.80 & $t_1$ \\
  r & 0.45 & 0.69 & & 0.62 & $t_2$ \\
  a & 0.75 & 0.64 & ... & 0.01 & $t_3$ \\
  h & 0.24 & 0.93 & & 0.53 & $t_4$ \\
  v & 0.23 & 0.24 & & 0.15 & $t_5$ \\
  i & 0.32 & 0.62 & & 0.80 & $t_1$ \\
  \hline
\end{tabular}
\end{table}
Table \ref{table:word_vector_encoder_input} shows an example input to word vector encoder RNN with values rounded to two decimals. $d_0$ to $d_{299}$ are the dimensionalities of character embedding vectors and $t_0$ to $t_6$ are RNN input timesteps ie. characters of the word \textbf{\textit{Kirahvi}}. Example provided shows only a single word, in practice the input data contains multiple words all stacked into a single Tensor.

Word vector encoder itself is a bi-directional recurrent neural network which takes the words represented by character embeddings as input and produces word embedding Tensor as output. Characters of the input words are the timesteps data for the RNN. Bi-directional RNN goes through the input timesteps in both directions. Each direction has a single RNN cell and outputs of both cells are concatenated as one double sized Tensor. Word embedding Tensor ie. output Tensor contains a single word embedding on each row. These word embeddings produced by the word vector encoder contain the semantic and syntactic meanings that were obtained from reading the words as separate units without any context.

\begin{table}[htbp]
\caption{Example output of Word vector encoder}
\label{table:word_vector_encoder_output}
\centering
\begin{tabular}{|c|c|c|c|}
  \hline
  $d_0$ & $d_1$ & ... & $d_{299}$ \\
  \hline
  \hline
  0.96 & 0.12 & ... & 0.87 \\
  \hline
\end{tabular}
\end{table}

Table \ref{table:word_vector_encoder_output} shows an example output of word vector encoder RNN with values rounded to 2 decimals. $d_0$ to $d_{299}$ are the dimensionalities of word embedding vector for a word \textbf{\textit{Kirahvi}}. Example provided shows only a single word, in practice the output data would have several words in a single Tensor, one per row.


\subsection{Context Encoding Component}
\label{ss:context_encoding_component}
Word embedding vectors created with the word embedding components are the foundation for doing word level predictions. Using encoded information about semantic and syntactic information of the words for which predictions are to be made is far superior to simply using eg. one hot encoding as is discussed in section \ref{se:word_embeddings}. However word embeddings are not nearly perfect representation of all the information that is associated with the words because words represented this way are still only separate units without context in which they appear.

More information about words for doing predictions can be gained by encoding the context of the word into a vector representation to be used along with word embedding. Words are read from left to right in most languages forming a sequence, a sentence, in similar way as character sequences form words. Since words in a sentence are sequence, the context of a word is also a sequence. Sequence of a word in this work means word's preceeding succeeding words ie. words on the left and right side of the current word. Because context is a sequence, once again recurrent neural networks are a natural way to process the sequences.

Word's context can be divided into two parts: left side context, the preceeding words, and right side context, succeeding words. To encode both contexts the architecture uses bi-directional RNN. First cell is used process the left side context and second cell is used to process right side context. One could argue that using two bi-directional RNNs, one for each side, would yield better results. However since importance of word in a context is higher closer the contextual word is to current word and since RNNs "remember" the last timesteps the best, it was hypothesised that using only a single direction for each side would yield almost similar results. Sequence direction of a context is always towards the current word, from left to right for left side context and from right to left for right side context making the closest words always last in the context sequences.

Using only a single uni-directional cell for each side also has significant benefits in terms of implementational simplicity and computational complexity. If one were to use bi-directional RNN for each side, one would be forced to run contexts for each word in a sentence separately because in this scenario the backward passes start from different location, from the current word. When using only an uni-directional RNN for each side there are no backward passes and forward passes always start from the same location: first word of a sentence for left side context and last word of a sentence for right side context. Figure \ref{figure:uni_vs_bi} illustrates this difference between uni-directional and bi-directional context encoders. Using uni-directional context encoders makes it possible to do a single forward pass and a single backward pass for entire sentence. Contexts of different words are simply different timestep outputs of these passes.

\begin{figure}[htbp]
\caption{Uni-directional vs bi-directional RNNs for context encoding}
\label{figure:uni_vs_bi}
\centering
\includegraphics[width=15cm]{uni_vs_bi.png}
\end{figure}

Sharing context encoder passes among all the words in the sentence simplifies input data format of context encoder. Instead of having separate Tensor for each word as input data as is the case with bi-directional RNN, uni-directional can use a single Tensor which has all the words stacked, one word embedding vector per row. Having one simple input Tensor entails a single data pass through the RNN decreasing computational complexity drastically. For a 13 word sentence one would have to make 14 passes for left side (1 forward, 13 backward) and another 14 passes for right side if one were using bi-directional RNN. With uni-directional RNN this comes down to one pass per side, 14 fold decrease. Actual computation time decrease might smaller because it could be possible to parallelize this operation with GPU. Measuring actual prediction performance benefits and added computational costs associated with using bi-directional RNN for context encoder is out of scope for this thesis. Table \ref{table:context_encoder_input} shows an example of input data Tensor for context encoder when using batch size of one sentence. $d_0$ to $d_{299}$ are dimensionalities of word embedding vectors.

\begin{table}[htbp]
\caption{Example input of context encoder}
\label{table:context_encoder_input}
\centering
\begin{tabular}{|r|c|c|c|c|l|}
  \hline
  & $d_0$ & $d_1$ & ... & $d_{299}$ & \\
  \hline
  \hline
  Kirahvi & 0.96 & 0.12 & & 0.87 & $t_0$ \\
  yltää & 0.16 & 0.26 & & 0.65 & $t_1$ \\
  syömään & 0.24 & 0.51 & & 0.80 & $t_2$ \\
  lehtiä & 0.05 & 0.55 & ... & 0.96 & $t_3$ \\
  korkealta & 0.82 & 0.06 & & 0.39 & $t_4$ \\
  . & 0.03 & 0.14 & & 0.46 & $t_5$ \\
  \hline
\end{tabular}
\end{table}

Context encoder RNN produces embedding vectors of a similar form as is it's input ie. output of word embedding component. These vectors encode semantic, and some syntactic, meaning of the words' contexts. When used with word embedding vectors these two encodings express all the information that is available for the words when looking only at the sentence. Output of the context encoding component is vector formed by concatenating word embedding vectors and context embedding vectors for respective words. Table \ref{table:context_encoding_component_output} shows an example output for a single word from context encoding component. $d_0$ to $d_{299}$ are dimensionalities of word embedding vectors and $d{300}$ to $d_{599}$ are dimensionalities of context encoding RNN.

\begin{table}[htbp]
\caption{Example output of context encoding component}
\label{table:context_encoding_component_output}
\centering
\begin{tabular}{|r|c|c|c|c|c|c|c|c|l|}
  \hline
  & $d_0$ & $d_1$ & ... & $d_{299}$ & $d_{300}$ & $d_{301}$ & ... & $d_{599}$ & \\
  \hline
  \hline
  Kirahvi & 0.96 & 0.12 & & 0.87 & 0.69 & 0.88 & ... & 0.74 & $t_0$ \\
  \hline
\end{tabular}
\end{table}

Higher level contructs for information encoding, such as looking at preceeding and succeeding sentences in the corpus, are not used in this architecture. Adding contextual information for sentences would add yet another layer of complexity to the architecture increasing actual implementational difficulty significantly. Reshaping Tensors in Tensorflow can be very tricky at times, especially when splicing and stacking for batching has to be done on multiple architectural levels. Also using sentences' contexts could prove problematic when using the neural network for actual live production work because it is often the case that user wants lemmatization and POS-tagging done for a single sentence without having context at hand.


\subsection{Classification Component}
\label{ss:classification_component}
Output of context encoding component could very well be used for doing predictions given that output is projected to a suitable size, namely the number of classes to classify. However architecture used in this work has additional fully connected layers between the output projection layer and context encoding component. In this configuration the architecture can be divided into two logical sections: encoder and classifier. Figure \ref{figure:encoder_classifier} shows the two logical high level components of the architecture.

\begin{figure}[htbp]
\caption{High level logical components of the architecture}
\label{figure:encoder_classifier}
\centering
\includegraphics[width=15cm]{encoder_classifier.png}
\end{figure}

Encoder composes of all the layers and components up to the classification component and the classifier is a simple fully connected feed-forward network, also called multi-layer perceptron (MLP). When conseptualized in this way, the responsibility of the encoder is to, as name suggest, encode all available input information in a format which is easily understood by the classifier. Classifier is responsible to produce the actual predictions. Input of the classifier is the output of the encoder as is, without any projections or scaling. When the architecture is build by separating the encoder and the classifier, it is easier in the future works to reuse the encoder and create a new classifier suitable for the task at hand. On the other hand the classification component can be thought to be just a few fully connected layers on top of context encoding component for added expression power.

Regardless of how one wishes to approach the logical components of the architecture, the fact remains that the fully connected layers were added based on early experiments on the architecture's learning capabilities. With few training runs on different configurations it became clear that neural net converges to a better performing model when using the fully connected layers than without them. Without the fully connected layers the network seemed to converge faster, with less epochs, but couldn't obtain quite as high classification performance. It is also worth mentioning that adding even several fully connected layers added almost no extra computational cost. Multiple layers of RNNs seem to be a lot more expensive than even far greater number of fully connected layers.

The last two layers of the classification component are output projection layer and softmax layer. The output projection layer is fully connected layer with linear activation function and has one node for each class. The softmax layer takes the output of projection layer can computes a softmax function for it, this is done to scale the output values in such a way that all output values sum up to one. When outputs are scaled to sum up to one, the output can be treated as probability distribution even though it might not strictly be one. Scaled outputs are also easier for humans to understand, often a nice to have feature but certainly not critical. The main reason to add a softmax layer after the output prediction really lies with the softmaxes innate properties which make it suitable for optimization with cross entropy loss.

The softmax cross-entropy loss for the separate lemmatization and POS-tagging tasks is a straight forward computation from the single output projection layer values and one-hot encoded vector for correct labels. Softmax for $j$:th node of output layer is defined in equation \ref{equation:softmax}, where $z$ is the vector of output projection layer values and $K$ is the size of output projection layer.

\begin{equation}
\label{equation:softmax}
\sigma(z)_j=\frac{e^{z_j}}{\sum_{k=1}^{K}(e^{z_k})}
\end{equation}

Cross entropy between softmax vector $\sigma$ and one-hot encoded vector of correct lables $Y$ is defined in equation \ref{equation:cross_entropy}, where $K$ is the size of output layer.

\begin{equation}
\label{equation:cross_entropy}
H(\sigma, Y) = -\sum_{k=1}^{K}(\sigma_k \log(Y_k)
\end{equation}

However loss function for the joint model is a bit more complicated since there are two optimization targets to aim for. In order to optimize both tasks at the same time, a single joint loss function has to be defined for the optimization target. Joint loss function for this work was selected to be a weighted sum of separate loss functions for lemmatization $H_l$ and POS-tagging $H_p$

\begin{equation}
\label{equation:joint_loss}
H_{joint}=\alpha H_l + H_p
\end{equation}

As a matter of fact the softmax layer only exists in the training pipeline. Softmax scaling is not necessary for doing the predictions because the prediction of a classifier is the output node with highest activation value. Softmax is a monotonic function ie. higher input values will always give higher output values and since the classification is only about selecting the highest value, the softmax is not required. In addition the softmax can be computationally relatively expensive especially when the output size is large, as is the case with large lemma vocabulary.

To produce the actual lemmas and part of speeches, the indices of the highest values in the output layer need to be used for indexing the label vocabularies. If the third node on the lemma output layer has highest value, one must select third item in the lemma vocabulary as the final prediction. Vocabularies are not needed for testing and validating because the ground truth lemmas and part of speeches have also been turned into indices and comparison can be done with the indices.

\subsection{Training and Optimization}
All neural network trainings in this work were done with stochastic gradient descent back-propagation algorigthm. The optmizer algorithm and back-propagation through the computation graphs are handled by Tensorflow's built in features. First the input data is fed to neural network and propagated in forward direction through the whole network. When the last network layer, the softmax layer, has been computed, the softmax values are used to compute cross entropy loss with one-hot coded ground truth vector. Error gradient is then propagated through the entire network in backward direction, calculating error gradient for each layer. Error gradient in each layer is used to update the layer weights and biases.

Gradient descent update was done with mini-batches of 25 sentences. This means that entire mini-batch is back-propagated through the network before adjusting the network weights and biases. Alternative is to update the network after each example but that has serious drawbacks for computational efficiency. Mini-batch training can be done in a pleasantly parallel manner, meaning that the exapmles of the mini-batch have no cross-dependencies and thus can be all trained at the same time, parallel. Parallel computations are very suitable for GPU workloads and as a matter of fact mini-batch training is critical component for enabling the significant, often an order of magnitude or more, increases in computation speed on GPU. Figure \ref{figure:single_vs_batch} illustrates the difference between the propagation sequences in single example training and mini-batch training.

\begin{figure}[htbp]
\caption{Propagation sequences in single exmaple and mini-batch training}
\label{figure:single_vs_batch}
\centering
\includegraphics[width=12cm]{single_vs_batch.png}
\end{figure}

Also a true single example training is not even possible, at minimum a mini-batch contains all the words of a single sentence. The reasons for this are the facts that the single example of input-output value pair is a single word and it's lemma of POS-tag and a word depends on it's context ie. other words in the sentence. Therefore it's mandatory to use at least a single sentence mini-batching. If architecture didn't contain the context encoding component, all words of the sentence could be managed as separate units and could be trained independently.

Mini-batches of 25 sentences used for network optimization are formed by dividing the entire training dataset randomly into the said mini-batches. Training procedure goes through all the mini-batches in an epoch. In the end of the epoch after all of the training data has been used, the network classification performance is evaluated on the validation dataset by calculating loss. Loss is the compared to the best obtained loss value from all the previous epochs, if the validation loss has decreased training procedure continues onto the next epoch. If validation loss doesn't decrease training can be stopped to avoid over-fitting. However classification performance on validation dataset does not typically improve on every epoch, especially after the network is starting to converge. Therefore it's wise to allow the training to continue for a fixed number of epochs even if there is no improvement. When the maximum number of epochs without improvement has been reached, the training is stopped and classification performance is tested on the testing dataset.

The network training procedure described above is only a single training run. Single training run allows the optimization of network parameters (weights and biases) but not hyperparameters. Hyperparameters are all the parameters that determine the network architecture, such as number of layers on each component, number of nodes on each layer, activation functions used, dropout etc. Traditionally hyperparameters have been optimized by manual process of training with certain cofiguration, inspecting the results, adjusting hyperparameters and training again. While manual hyperparameter optimization can provide good enough results, it is still very tedious work and often requires deep understanding of neural networks and of the task at hand. Recently automatic hyperparameter optimization has gained popularity, mainly because computation resources have increased and thus it's more feasible to run a lot of training runs while tuning the hyperparameters. Automatic hyperparameter tuning is particularly beneficial when the number of hyperparameters to optmize gets large, humans are typically poor at processing more than 3 dimensional data.

Hyperparameter optimization in this work was done automatically using a optimization library called Optunity (\cite{optunity}). Optunity provides a convenient way to abstract the optimization work for basically any given task. There are many different optimization algorithms for hyperparameter tuning and not one has gained similar de facto status as gradient descent has for the neural net optimization. Algorithm used in this work for hyperparameter tuning is particle swarm which is a meta heuristic optimization algorithm. Particle swarm is said by the Optunity authors to be most versatile of all the algorithms available in Optunity. Hyperparameter optimization algorithm comparisons were not done since particle swarm provided reasonable results with the time results of this project. Besides hyperparameter optimization algorithm selection is not in the scope of this work and has very little relevance with proving the hypothesis.

Hyperparameter optimization was done by running a fixed number of training runs and tuning the hyperparameters with particle swarm between each training run. Since automatic hyperparameter selection may ramp up all the hyperparameters to their maximum allowed values, can training times scale up to unsustainable durations. To avoid dealing with single training runs that take tens of hours, maximum time budget was forced for the training runs. If training run reached the maximum allowed time, best model obtained with that training run was selected for final comparison and hyperparameter tuning. Training runs in the hyperparameter optimization also used early stopping based on validation loss, this helps to cut the training with some hyperparameter configurations to just few training epochs, speeding up the hyperparameter optimization drastically.

After all of the training runs for hyperparameter optimization were finished, hyperparameter configuration which provided the best model was saved for using in experiments.

\section{Experiments}
\label{se:experiments}
Experiments done for this thesis consist of minimal set of tests that can prove or disprove the tested hypothesis. To test whether jointly learning lemmatization and POS-tagging in a single neural network architecture the following experiments were done. Firstly a baseline was established by training and testing the neural net with lemmatization and POS-tagging as separate tasks. Having a well established baseline allows the comparison of different style shared information tasks and their benefits and drawbacks. Figure \ref{figure:baseline_experiment} shows the setup for separate tasks.

\begin{figure}[htbp]
\caption{Separate lemmatization and POS-tagging as a baseline}
\label{figure:baseline_experiment}
\centering
\includegraphics[width=15cm]{baseline_experiment.png}
\end{figure}

First shared information experiment was done to determine if more traditional approach of using one as input features in classification of other. This experiment was done only by using parts of speech as one-hot coded input features when doing lemma classification. Expecting to gain benefit from using parts of speech as input feature to lemmatization is reasonably well founded because knowing the part of speech for a given word limits the number of possible lemmas significantly. The big question in this hypothesis is whether the POS-tagging was done well enough to reveal the possible benefits. It's very well possible that errors done in POS-tagging cascade to lemmatization task so badly that lemmatization performance actually decreases from the baseline. To study the effects of POS-tagging performance to lemmatization performance, a second experiment was deviced: to train and test the network using gold-standard parts of speech available in the dataset. Figure \ref{figure:pos_tag_input_experiment} shows the setup for using parts of speech as input feature for lemmatization.

\begin{figure}[htbp]
\caption{Using parts of speech as input feature for lemmatization}
\label{figure:pos_tag_input_experiment}
\centering
\includegraphics[width=15cm]{pos_tag_input_experiment.png}
\end{figure}

Last experiment was the training and testing of the joint learning neural network architecture for lemmatization and POS-tagging. Figure \ref{figure:joint_model_experiment} illustrates the test setup for joint model.

\begin{figure}[htbp]
\caption{Joint model for lemmatization and POS-tagging}
\label{figure:joint_model_experiment}
\centering
\includegraphics[width=15cm]{joint_model_experiment.png}
\end{figure}

Comparing results from the baseline, POS-tags as input features for lemmatization and joint model should be enough to test the hypothesis. Experiments don't include other languages than Finnish or other datasets than Finnish Universal Dependencies. Experiments in this thesis with POS-tagging all use the universal POS-tags, effect of using language specific POS-tags with or without universal POS-tags was not tested. Similarly effect of tokenization performance was not tested but all experiments used gold standard tokenization available in the Finnish universal dependencies dataset.

The effect of morphological information such as word inflections and forms were not tested for this thesis even though there is a well founded arguments for improving results by using the morphology also. Since morphological features describe how the word was inflected they should assist the lemmatization because lemmatization can though as a reverse process of inflecting the word. However scope of the thesis was kept as narrow as possible, but wide enough for testing the hypothesis, and therefore morphology was left out. For that same reason also use of language specific POS-tags available in Universal Dependecies datasets was left out of scope of this thesis. 

Attentive reader might have noticed that experiments included in this thesis have parts of speech as input features for lemmatization but not lemmas as input features for POS-tagging. Reason for excluding experiment of lemmas as input features for POS-tagging is that the described task is not very interesting or even meaningful: if a lemma of a word is known, part of speech for that word can be looked up from a dictionary with the exceptions of words with multiple meanings.


\section{Test Methods}
\label{se:test_methods}
The dataset use for experiments in this thesis is Finnish Universal Dependencies 1.4 as mentioned in earlier sections. The Finnish Universal Dependencies dataset is divided into three sections: training dataset, development dataset (also called validation dataset in this thesis) and testing dataset. Each experiment outlined in section \ref{se:experiments} was done using all three datasets. Neural network weight and bias optimization was done using only training dataset, model performance was observed during training by validating the network with development dataset and all final results are obtained by doing predictions with test dataset and comparing with correct labels. Training and testing procedure for each experiment is illustrated in figure \ref{figure:test_setup}.

\begin{figure}[htbp]
\caption{Test setup for doing the experiments}
\label{figure:test_setup}
\centering
\includegraphics[width=15cm]{test_setup.png}
\end{figure}

Other corpuses exist for Finnish but format and annotation schema varies in those corpuses and are only available for Finnish. Univesal Dependencies is very suitable this work because it has over 40 different languages available all with same form and annotation scheme making possible future expansions of this study easier and results comparable. Another reason for selecting Universal Dependencies is that pre-existing results are avaialble for POS-tagging and lemmatization, most notably from UDPipe (\cite{Straka2017}).

Several options exist for the metric with which experiment results can be compared when testing the hypothesis. This work settled for simple accuracy number ie. percentage of correct prediction over all examples in the testing dataset. F1 score is another widly used metric which takes precision and recall into account and therefore reveals the performance of the model better. However F1 score may not be intuitive and easy to understand for a reader without prior experience or education in statistical analysis. Accuracy should also provide enough information to test the hypothesis.

Another drawback of accuracy is that it tells nothing about the confidence of the network; even if model's accuracy is high, the model might give very high probabilities for the few incorrect predictions it produces. If output probabilities are required for the task, for example for using a probability threshold for determining if an answer should be given at all, this kind of model is not very suitable for the task. In this work however the training is done based on the softmax cross-entropy loss which optimizes the form of prediction probability distribution. Also the early stopping used in the model training is based on the cross-entropy loss so training is stopped before network starts to be overconfident about it's incorrect predictions. Softmax cross-entropy is a good metric for optimzing the network, but using cross-entropy loss for evaluating performance of different results suffers from the same problem as F1 score and provides even less insight for the reader about how well the model performs.

Given all of these considerations for different benefits and drawbacks of the discussed metrics, it is still most probable that any of them would suffice to reveal the classification performance differences between the experiments. Some of the metrics might emphasis the difference more than the others, but the bottom line is to test the hypothesis and provide results for the reader that are understood by the reader.

\section{Results}
\label{se:results}
Results as accuracies for experiments described in section \ref{se:experiments} are shown in table \ref{table:results}. UDPipe is used as a baseline for comparing absolute results achieved in this work. UDPipe achieves accuracies of 94,9\% for POS-tagging and 86,8\% for lemmatization on Finnish Universal Dependencies 2.0 dataset \cite{udpipe-manual}. POS-tagging result comparison between UDPipe and this work are not perfectly fair since UDPipe results reported on UDPipe User's Manual page are obtained with Universal Dependecies 2.0 Finnish dataset whereas this work uses 1.4. Lemmatization results are not comparable at all because UDPipe lemmatizer is a generative model which forms the lemma from the stem of the word \cite{Straka2017}. As a generative model the UDPipe lemmatizer does not suffer from vocabulary related issues, but also can produce typographical errors and other mistakes common for generative models.

\begin{table}[htbp]
\caption{Results as accuracies for the experiments.}
\label{table:results}
\centering
\begin{tabular}{|l|c|c|l|}
  \hline
  Experiment & POS-taging & Lemmatization \\
  \hline
  \hline
  Separate POS-tagger & \textbf{94,30\%} & - \\
  Separate lemmatizer & - & 94,25\% \\
  Lemmatization with classified POS-tags & - & 94,40\% \\
  Lemmatization with gold POS-tags & - & \textbf{96,22\%} \\
  Joint learning model & 94,14\% & \textbf{95,24\%} \\
  \hline
\end{tabular}
\end{table}

From the results we can see that the best POS-tagging accuracy is obtained by the baseline, doing POS-tagging without lemmatization, with 94,30\%. Baseline lemmatizer achieves very similar results of 94,25\%. Baseline POS-tagging accuracy is remarkably close to the UDPipe accuracy even though the learning approaches are different. The small difference of only 0,6\%-points between POS-tagging accuracies in UDPipe and the baseline of this work could possibly be explained by the limitations of dataset; perhaps larger (or smaller) dataset could turn the POS-tagging accuracy into a favor of one or the other.

It's worth noting that the similarity of results betwee baseline POS-tagging and lemmatization don't mean that both tasks are similarly difficult; lemma vocabulary coverage has significant effect on lemmatization performance. Higher lemma vocabulary coverage will lead to having a lot more lemmas, almost doubling the number of unique lemmas in the vocabulary. These new lemmas also have significantly less examples in the dataset making them much harder to classify correctly. Similarity of results between the baseline tasks is more of a coincidence than feature of the tasks.

The more traditional approach to shared information learning, using parts of speech as input features for lemmatization, yields a small improvement over the baseline lemmatization. Difference between the results is mere 0,15\%-points. There are two possible explanations for this difference: using parts of speech for lemmatization does not in fact improve results and the difference is within of margin of error or errors in part of speech classification used to produce input features for lemmatization are so significant that they undermine any possible benefits.

Result accuracy for lemmatization using gold standard POS-tags, ie. POS-tags available in the Finnish Universal Dependencies corpus, provides clear insight that the latter of the proposed explanations is the correct one. Lemmatization using correct parts of speech can achieve accuracy of 96,22\% which is significantly higher than the baseline and lemmatization using classified parts of speech. Improvement gained using gold standard parts of speech is 1,97\%-points, which is 34,26\% decrease in accuracy error. This clearly shows that there is benefit from using parts of speech as input features for lemmatization and that the shared information between the two features is useful for the tasks at hand.

Joint learning model, which is the main focus of this work, can achieve 94,14\% for POS-tagging and 95,25\% for lemmatization. Lemmatization performance gain from the joint model is 0,99\%-points (17,22\% decrease in accuracy error), which is high enough that results cannot be explained with numerical instability. Joint model lemmatization result is also the best practical result among the experiments. Lemmatization with gold standard parts of speech did obtain better accuracy than lemmatization of joint model, but in real life predictions cannot be done with gold standard parts of speech since they are created by a team of liguists and as such are not available at that time.

Maybe the more interesting result of the joint model accuracies is the POS-tagging accuracy. POS-tagging accuracy in the joint model went down 0.16\%-points when compared to the baseline separate POS-tagger. What could explain this accuracy decrease is not clear at this time. Perhaps lemmatization as more information rich task dominates the POS-tagging in the joint model not allowing the gradient descent to have enough power to adjust network weights and biases favorably for the POS-tagging task. On the other hand the difference is small enough to be at least partly explained by the numerical instability. It is also possible that doing several additional experiments with different weights for the joint loss function would have resulted into better accuracy for the POS-tagging task without much deterioration of lemmatization accuracy.



\chapter{Discussion}
\label{ch:discussion}
Results obtained in this work show that shared information in POS-tagging and lemmatization can be used to improve model performance when jointly learned. Strictly speaking the results are valid only for the specific configurations and test setup used in the experimens. Whether these results generalize for other languages, and what languages is still up for debate. Finnish is very different from other languages spoken in Europe and therefore it could be that POS-tagging doesn't benefit lemmatization in joint model when testing on for example Swedish, German, English, French or Spanish. However part of speech and lemma are very similar concepts in basically all languages of the world with strong shared information between the features, therefore it is not far fetched to hypothesis that one can obtain similar results with other languages too.

Speculating how the results generalize to other tasks in natural language processing is another question entirely. If results obtained in this work are indeed mainly due to the strong shared information between the part of speech and the lemma of the word, then other similar task pairs that share mutual information are good candidates for joint learning models. \cite{Liu2016a} showed in their study "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling" that there exists at least one other task pair which benefits from joint learning with neural networks. With the results of \cite{Liu2016a} and of this work it is safe to say that neural networks are indeed suitable for jointly learning natural language processing tasks with shared information.

Even further speculation about how well these results generalize can be done for tasks in other machine learning domains than natural language processing and to task pairs that don't share any direct mutual information but are still strongly correlated. It is possible that the two tasks don't actually need to share mutual information for gaining benefit from joint learning as long as correlation in the data between the two tasks exists. If this is the case then probably even the results obtained in this thesis could be explained by that correlation. Finding such a discovery of very general nature of joint task benefits would be extremely helpful in designing future machine learning systems and could be applied across several problem domains. For example a neural network could be developed for predicting difficult to measure phenomena in industrial setting (eg. a factory or bioreactor) which uses multitude of correlated measurements as input, lowering the cost structure for measurements.

All in all there seems to be enough evidence to justify further research into joint learning models with neural networks. Especially interesting evidence are the results of \cite{Liu2016a} because the two jointly learned tasks are classified at different hierarchical levels. Slots can be considered keywords of the sentence relative to spoken language understanding and intent is a single label of semantic meaning for the sentence. This means that there exists now evidence for benefits from two word level tasks as well as word level and sentence level tasks. It should be fairly safe to assume that the task pairs can be selected from other hierarchical levels also, for example two document level tasks or document level task and a sentence level task or a word level task such as POS-tagging and a character level task such as lemma generation. The new problem which must be addressed when using two tasks of different hierarchical level is the choice of validation metrics: should for example the generative lemmatization be evaluated at word level or at character level.

\section{Assumptions and Simplifications}
Assumptions made in this thesis are mainly related to the dataset used, test setup and neural network tools used. The Finnish Universal Dependencies dataset is a machine annotated corpus with manual corrections \cite{Haverinen2014}. As such the dataset was assumed to be sufficiently correctly annotated. Errors in the input dataset are fatal for machine learning methods since these methods don't have understanding about the real world, they just approximate the hidden functions of the input data. Even though the machine annotations have been manually corrected by linguists, there still exists a possibility for human errors. In fact it's not possible to be entirely sure about having 100\% absolute true annotations in any corpus because the ground truth is ultimately determined by the humans who create, correct or check the corpus. Errors can only be reduced by having multiple annotators to do the same annotations and doing a cross reference, but since none of the annotators can be 100\% sure, the probability for final annotations' correctness cannot be 100\%.

Next assumption about the dataset used is that dataset represents Finnish language well. This assumption may not hold true as well because of the way texts in the Finnish Universal Dependencies were selected. Finnish UD dataset consists mainly of news articles, Wikipedia articles, blog posts, fiction and some other formal sources such as Europarl speeches \cite{Haverinen2014}. None of these sources include texts from conversations such as online discussion forums or even personal communication chat where form of the language may differ significantly from the more curated and formal texts. Also different dialects of Finnish language is not well represented in the corpus. All these limitations of the dataset distribution limit the model's ability to generalize to said situations and writing styles. It is also not known how big of an impact for example colloquial speech would have to benefits of POS-tagging and lemmatization joint task. At least the increased typographical variation introduced with colluquial speech is sure to raise problems related with data sparsity unless massive dataset is used. How big these issues will be is left to be tested.

When training, validating and testing the neural net model an assumption has to be made that data distribution in each of these sets are similar. By looking at the datasets one can easily deduce that at least the sentences divided into training, development and test datasets were not randomized before doing the division; there are several subsequent sentences that are obviously from the same text source in an order which looks like it could be the original one. If, and when, the sentences are not suffled before diving into datasets, there is no guarantee that datasets share the same distribution. It could be even that for example all of the blog entries have ended up into development and training datasets, leaving no training examples about blogs to training dataset. This would surely hinder the model's cabability to POS-tag and lemmatizer these texts which are most likely written in more informal language that eg. news articles.

In addition to assumptions about dataset, assumptions were made about the test setup. Perhaps the most notable assumption about test setup is that limiting the lemma vocabulary and using unknown tokens for out of vocabulary lemmas would represent more realistic situation where lemma vocabulary is not artificially limited. Lemma vocabulary was limited by selecting certain coverage of uses in the training set, this entails an assumption that same coverage would be found in the development and testing datasets, or at least neural net model could generalize to learn to classify unseen lemmas as unknown tokens.

A quick lemma vocabulary coverage about the Finnish UD datasets reveals that a lemma vocabulary which covers 100\% of uses in the training set only covers about 75\% of uses in validation and testing datasets. This is also an indication about different data distributions in the three datasets. Lemma coverage and data distribution issues seemed not to be a big problem since the lemmatization accuracy for the test set was over 95\%. Whether the accuracy would have been even higher if the distributions had matched better was not tested in this thesis. It's also worth noting that neural network model learned remarkably well to classify unseen words as unknown tokens, if it hadn't the lemmatization accuracy for testing dataset would have been significantly lower. The final selected lemma vocabulary coverage was 90\% meaning that unknown tokens cover 10\% of the lemma usages in training dataset. The second most frequent term was less than 5\%, so there was significant priori bias to select unknown token. It was not tested among the experiments how the lemmatization accuracy of unknown tokens would change if the unknown tokens didn't have such a big priori bias.

Another big assumption about the test setup is the complete omittance of all possible problems associated with treating all words with same written forms as a single word. In reality these words have very different semantic meanings and will disturb learning of word embedding vectors by word embedding component. As was discussed in previous sections, this might not have a big impact on the performance of the model for these tasks. However problem might be really bad in downstream processing tasks which use lemmas as input features; if multiple words with different meanings but same written form need to be distinguished from each other, lemmatization done with this model is of no help.

Last significant assumption about the test setup was that running all experiments with same hyperparameters would not prevent from producing significant difference in each experiment. In optimal case each experiment would have had it's own hyperparameters optimized for the said task. Unfortunately multiple hyperparameter searches was not possible with the resources available. It is not known at this time if differences would have been greater or smaller if hyperparameters had been optimized for each taks. Hyperparameters were optimized for the joint task which showed a notable improvement for lemmatization over the baseline model, perhaps optimizing separate set of hyperparameters for the baseline model would have lead to smaller difference. However it is unlikely that the whole performance gain is explained by the better hyperparameters.

More minor simplification of the test setup is the fact that experiments were done with gold standard tokenization. Incorrect tokenization can completely ruin word level classification tasks for the incorrectly tokenized words. However tokenization is a lot simpler task which can be inferred from the UDPipe tokenization results \cite{Straka2017}. UDPipe tokenizer achieves F1 score of 99,69 for Finnish Universal Dependencies 2.0. This means that there is approximately one incorrect tokenization in 77 sentences, when average sentence length is 13. If worst case scenario is assumed ie. one incorrect tokenization ruins lemmatization and POS-tagging for the entire sentence, causes tokenization errors $1/77 = 1,3\%$ error for both tasks. However it's not very reasonable to assume such an effect for all the words in the sentence. Tokenization error can certainly affect the POS-tagging and lemmatization results, but the effect should be relatively small. This same simplification also applies to sentence segmentation. Whether the sentence segmentation is necesasry step of the pipeline depends on the task. If the purpose is just to provide a tool for linguists to help with lemmatization and POS-tagging of separate sentences, the sentence segmentation is not required.

Hyperparameter optimization was also exposed to some simplifications in addition to not doing it for all of the experiments separately. Firstly the hyperparameter optimization had fixed maximum time budgets for the model trainings in order to counter the possibly very long training times with certain hyperparameter configurations. Limiting the maximum time budget also puts severe constraints on the models which are affected by the time limit, these models are not allowed to fully converge but are stopped prematurely. This means that the hyperparameters are not globally optimal, but only optimal within the given constraints. Hyperparameter optimization was also constrained by the maximum number of training runs used for the search, this entails a situation where the hyperparameters are not fully converged to their optimal values but rather are the best ones that were obtained with the limited number of runs. Hyperparameter search ranges were also limited for each hyperparameter so the hyperparameters found with the search can only be optimal within those ranges. If all simplifications of hyperparameter search were to removed, countless number of runs would have been needed and a single run could have taken time that approaches infinite as the layer sizes and counts rise, certainly infeasible situation.

Different kind of limitation to hyperparameter search was the exclusion of weight $\alpha$ in joint model loss function defined in equation \ref{equation:joint_loss}. $\alpha$ cannot be optimized along with other hyperparameters because adjusting the $\alpha$ will cause the optimization target to change. Changing optimization target between optimization runs renders the entire optimization process meaningless. Determining the value for $\alpha$ is really a decision about what is appreciated, if POS-tagging accuracy is valued higher than lemmatization accuracy then more weight has to be given to POS-tagging loss in the joint loss. One is not worse than the other, it's simply a question of preference.

Minor assumptions were made for neural network methods and tools used for implementing the model. The neural network weights and biases were initialized with small random values before starting the training. Using random values will produce different initializations for each training run which in turn will eventually lead to different results obtained from the two models. How big this difference depends largely on the size of the dataset, the larger the dataset the smaller is the impact of initialization. Finnish Universal Dependencies dataset contains 204399 tokens of which about 10\% is reserved for development set and about 10\% for test set, leaving over 160000 tokens for the training dataset. 160000 tokens is sufficient to rule out significant impact of the random initializations. However smaller differences such as the 0,15\%-point increase on lemmatization accuracy when using classified POS-tags as input features and 0,16\%-point decrease in POS-tagging accuracy for the joint model could be partly explained by the random initializations.

It was also assumed that typical numerical instability always present at limited precision computer systems would have near to zero impact on the results and variance between the results. Other sources of numerical errors are for system memory and GPU memory; the neural network was trained on regular consumer hardware which doesn't error correction for the memories used. It was also assumed that these error would not play notable part in the results because neural network training can be considered as nothing more than an error correction process.



\chapter{Conclusions and Proposals for Future Research}
\label{ch:conclusions}
Objective of this thesis was to build a neural network architecture for syntactic parsing, namely lemmatization and POS-tagging. Research hypothesis was if the architecture built for separate tasks could be modified in such a way that both tasks, lemmatization and POS-tagging, could be learned and predicted at the same time and if the joint learning model would be better at classifying lemmas and POS-tags. Objective was reached and research hypothesis was proved partly correct. Required architectural modifications were minor to produce the joint model, and it was observed that joint model is better at classifying lemmas but not parts of speech.

Chapter \ref{ch:natural_language_processing} of this thesis was the theory sectiono in which current academic status, and a bit of history, was presented on natural language processing with and without neural networks. Theory section discussed the most important problems in representing natural language with computer systems, what implications text as input data introduces to machine learning methods and what kind of approaches previous research has taken into solving them. Also problem associated with highly morphological languages such as Finnish were addressed in the theory section with different methods of input representations and their pros and cons in this context. Conclusion was that higly morphological languages are not suitable for word level representations because of vocabulary explosion problem. Character level models were introduced as a solution to inflections and morphology. Theory section also included syntactic parsing tasks most relevant for this thesis. Lemmatization and POS-taggin were the main focus of syntactic parsing tasks in theory section, as the were main focus of the research problem in experiments chapter.

Chapter \ref{ch:experiements_on_joint_model_for_pos_tagging_and_lemmatization} is the practical and research part of this thesis. First the hypothesis and research problem were introduced in experiments chapter. Introduction of experiments chapter also includes reasoning for selecting lemmatization and POS-tagging as the observed tasks and, selecting neural networks as implementational approach to forming the solution and selecting the joint model for the basis of the hypothesis. Lemmatization and POS-tagging were selected as the tasks because they share strong mutual information and are strongly correlated which make suitable for joint learning model tasks. Neural networks were selected as implementational approach because they have shown tremendous progress on several different problem domains and also in natural language processing. Other reason for selecting neural networks was that they are generally fairly simple to modify for joint learning tasks and since they are trained by optimizing the loss function multiple tasks can be learned simultaneously by selecting appropriate loss function.

First part after the introduction (\ref{se:architecture}) of the 3rd chapter describes the neural network architecture used for the experiments with it's various components developed for different sub-tasks. Architecture is composed of four components each on their own hierarchical level: first is the character level representation, second word level representation, and then the word representations are joined for contextual representation which is used as an input for the classification component which outputs the predictions. Neural network architecture section also presents the reasoning for the architectural decisiong for doing lemmatization as word level classification task only, this was mainly to keep the architecture simpler and keep all predictions on the same hierarchical level, word level, to avoid problems associated with selecting a metric that can handle multiple hierarchical levls. The last part of the 3rd architecture section explained the neural network training and hyperparameter optimization procedures.

Experiments used to test the research hypothesis were presented in the experiments section \ref{se:experiments} of the 3rd chapter. Experiment section also has reasoning for selection of different expriments. Baseline was established by having the two selected tasks, POS-tagging and lemmatization, as separate models. Then an experiment with joint learning model was introduced which can be compared to the separate models to gain insights about how much improvement the joint model can provide over the baseline, if at all. Additional experiments were added to determine if more traditional approach of using POS-tags as input features for lemmatization would provide similar benefits: one with classified parts of speech and one with gold standard ones.

Section \ref{se:test_methods} outlined the test setup used to do the experiments. Firstly the section states that Finnish Universal Dependencies were used as a dataset for training, developing and testing the neural network models. Then several possible metrics for evaluating the results was discussed with the conclusion of using a simple accuracy because it should be enough to test the hypothesis and is easier to understand for a reader without education in statistical analysis.

Last section of the 3rd chapter presents results obtained by doing the experiments with the described test setup. Main discovery was that joint model does indeed improve lemmatization performance over the baseline with 0,99\%-point increase and 17,22\% decrease in accuracy error. Interestingly POS-tagging perfomance was not improved and as a matter of fact was very slightly decreased from the baseline. Experiments done with using POS-tags as additional input features reveled that no significant improvement can be gained by using classified POS-tags unless the POS-tagging accuracy is very high. This perfect POS-tags the lemmatization saw very significant improvement of 1,97\%-points in classification accuracy, which is also a 34,26\% decrease in accuracy error.

Chapter 4 has discussion for the generalizability of the results. It was stated in chapter 4 that results obtained in the experiments should be generalizable to other other languages since almost all languages have same concepts for lemma and part of speech. Additional note was made that results should generalize to other shared information taks pairs within natural language understanding, this was further reinforced by the results of \cite{Liu2016a}. It was mentioned that assuming that the results would generalize to other problem domains or even to general correlating tasks is not safe to do based on these results alone. 

The latter part of discussions chapter covered assumptions made about data, test setup and results. Dataset was assumed to be annotated correctly enough, even though there still is the possibility of human error. Another important assumption made about the dataset that it would represent Finnish language well, which might not be the case entirely as was noted. Major assumptions and simplifications about the test setup were mentioned to be the treatment of lemmatization as word level classification task with the unknown tokens and running all experiments with same hyperparameters.

Assumptions, limitations and simplifications discussed in the 4th chapter open up several interesting research opportunities for the future. Firstly and most obviously it must be tested what kind of performances the model can achieve for other languages, and especially the other Finnish UD dataset Finnish FTB. The beauty of Universal Dependencies lies with the fact that all languages have unifrom annotations and therefore plugging in another language from Universal Dependencies should be as easy as feeding in another data files. Taking language capabilities even further a research about mutli-lingual models should be conducted. A neural network model which takes multiple datasets as input, all of which are in different language would prove very useful and at least ease the multi-lingual processing by limiting number of models to one. Whether multi-lingual model can provide as good results or even better as separate models for each language remains to be seen. A good foundation for multi-lingual models is provided by pairs or sets of languages which are linguistically close to each other, one such pair could be Swedish and Norwegian.

Another interesting prospect for a future research is the usage of additional information available in the Universal Dependencies corpuses such as morphology and language specific POS-tags. There is a solid reasoning for using morphology for lemmatization or even adding morphological parsing to the joint learning model as third task. Morphology describes the form and inflection of a word and since lemmatization can be seen as reverse process of inflecting the word, should morphological information be of great benefit for lemmatization. Modifying the architecture to include third task is not a large work, but bigger question is whether the three tasks can be learned successfully together without one task starting to dominate the others.

Architecture developed for this thesis also has room for improvement by tuning the $\alpha$ weight of joint loss function \ref{equation:joint_loss}. If POS-tagging is the only task to be performed, then adding a joint model which learn lemmatization also is probably not worth the added computational cost. If both tasks need to be done, but priority is for the POS-tagging, then a joint model could prove suitable if joint loss weight is tuned to favor POS-tagging. POS-tagging is recommended to be done always even if only lemmatization is requried since it improves the lemmatization accuracy without adding too much computational cost. Best possible results could be obtained by training two different joint learning models, one with loss weight and other hyperparameters tuned for POS-tagging performance and one with with weight and hyperparameters tuned for lemmatization performance. Drawback of this approach is the doubling of memory usage, which can be scarce resource with GPUs, and doubling of computational cost. All these speculations need to be confirmed or belied with further research.

%
% The bibliography, i.e the list of references (3 options available)
%
\newpage


% Extra for Finnish theses

\renewcommand{\bibname}{Bibliography}     % Bilingual babel puts Finnish ``Kirjallisuttaa'' otherwise. Strange...
%\renewcommand{\bibname}{Lähteet}         % Set Finnish header, remove this if using English
%\addcontentsline{toc}{chapter}{Lähteet}  % Include this in TOC
\addcontentsline{toc}{chapter}{\bibname}  % Include this in TOC


\printbibliography                  % a) heading in English
%\printbibliography[title=Lähteet]   % b) heading in Finnish
%\addtocontents{toc}{%               % b) add Finnish heading to table of contents
%\protect\noindent Lähteet\protect\par
%} 


\end{document}

