\documentclass[12pt,a4paper,english
% ,twoside,openright
]{tutthesis}
%\documentclass[12pt,a4paper,finnish]{tutthesis}

% Note that you must choose either Finnish or English here and there in this
% file.
% Other options for document class
  % ,twoside,openright   % If printing on both sides (>80 pages)
  % ,twocolumn           % Can be used in lab reports, not in theses

% Ensure the correct Pdf size (not needed in all environments)
\special{papersize=210mm,297mm}


% LaTeX file for BSC/MSc theses and lab reports.
% Requires the class file (=template) tutthesis.cls and figure files,
% either tut-logo, exampleFig (as pdf or eps) and example_code.c
% Author: Sami Paavilainen (2006)
% Modified: Heikki Huttunen (heikki.huttunen@tut.fi) 31.7.2012.
%           Erno Salminen, @tut.fi, 2014-08-15
%             - added text snippets from the writing guide
%             - added lots of comments: both tips and alternative styles
%             - added an example table
%             - and so on...

%
% Define your basic information
%
\author{Jaakko Pasanen}
\title{Natural Language Understanding for Information Retrieval in Chat Based Customer Support} % primary title (for front page)
\titleB{Luonnollisen kielen ymmärrys tiedon hakua varten chat-pohjaisessa asiakaspalvelussa}     % translated title for abstract
\thesistype{Master of Science thesis} % or Bachelor of Science, Laboratory Report... 
\examiner{Ari Visa} % without title Prof., Dr., MSc or such

% Put your thesis' main language last
% http://mirrors.ctan.org/macros/latex/required/babel/base/babel.pdf
\usepackage[finnish, main=english]{babel}

% http://www.ctan.org/pkg/biblatex
\usepackage[
  style=authoryear,
  maxcitenames=2,
  backend=biber,
  firstinits=true
]{biblatex}
\bibliography{thesis_refs.bib}
%% Note that option style=numeric works as well
\usepackage{fontspec}
\usepackage{amsfonts}


% You can also add your own commands
\newcommand\todo[1]{{\color{red}!!!TODO: #1}} % Remark text in braces appears in red
\newcommand{\angs}{\textsl{\AA}}              % , e.g. slanted symbol for Ångstöm

% Preparatory content ends here


\pagenumbering{roman} % was: {Roman}
\pagestyle{headings}
\begin{document}

% Special trick so that internal macros (denoted with @ in their name)
% can be used outside the cls file (e.g. \@author)
\makeatletter

%
% Create the title page.
% First the logo. Check its language.
\thispagestyle{empty}
\vspace*{-.5cm}\noindent
\includegraphics[width=8cm]{tty_tut_logo}   % Bilingual logo

% Then lay out the author, title and type to the center of page.
\vspace{6.8cm}
\maketitle
\vspace{6.7cm} % -> 6.7cm if thesis title needs two lines

% Last some additional info to the bottom-right corner
\begin{flushright}  
  \begin{minipage}[c]{6.8cm}
    \begin{spacing}{1.0}
      %\textsf{Tarkastaja: Prof. \@examiner}\\
      %\textsf{Tarkastaja ja aihe hyväksytty}\\ 
      %\textsf{xxxxxxx tiedekuntaneuvoston}\\
      %\textsf{kokouksessa dd.mm.yyyy}\\
      \textsf{Examiner: Prof. \@examiner}\\
      \textsf{Examiner and topic approved by the}\\ 
      \textsf{Faculty Council of the Faculty of}\\
      \textsf{Engineering Sciences}\\
      \textsf{on 31st December 2016}\\
    \end{spacing}
  \end{minipage}
\end{flushright}

% Leave the backside of title page empty in twoside mode
\if@twoside
\clearpage
\fi

%
% Use Roman numbering I,II,III... for the first pages (abstract, TOC,
% termlist etc)
\pagenumbering{Roman} 
\setcounter{page}{0} % Start numbering from zero because command 'chapter*' does page break

% Some fields in abstract are automated, namely those with \@ (author,
% title in the main language, thesis type, examiner).
late\chapter*{Abstract}

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@title\\   % use \@titleB when thesis is in Finnish
         \textsf{Tampere University of Technology}\\
         \textsf{\@thesistype, xx pages, x Appendix pages} \\
         \textsf{December 2016}\\
         \textsf{Master's Degree Programme in Automation Technology}\\
         \textsf{Major: Learning and Intelligent Systems}\\
         \textsf{Examiner: Prof. \@examiner}\\ % 
         \textsf{Keywords: Hype}\\
\end{spacing}


The abstract is a concise 1-page description of the work: what was the
problem, what was done, and what are the results. Do not include
charts or tables in the abstract.

Put the abstract in the primary language of your thesis first and then
the translation (when that is needed).


% Foreign students do not need Fininsh abstract (tiivistelmä). Move
% this before English abstract if thesis is in Finnish. Move also the
% otherlanguage command to the English abstract (if needed).

\begin{otherlanguage}{finnish} %  Following text in in 2nd language
\chapter*{Tiivistelmä} % Asterisk * turns numbering off

\begin{spacing}{1.0}
         {\bf \textsf{\MakeUppercase{\@author}}}: \@titleB\\  % or use \@title when thesis is in Finnish
         \textsf{Tampereen teknillinen yliopisto}\\
         \textsf{Diplomityö, xx sivua, x liitesivua}\\ %
         \textsf{Joulukuu 2016}\\
         \textsf{Automaatiotekniikan koulutusohjelma}\\
         \textsf{Pääaine: Oppivat ja älykkäät järjestelmät}\\
         \textsf{Tarkastajat:  Prof. \@examiner}\\ % automated, if just 1 examiner
         \textsf{Avainsanat: Hype}\\
\end{spacing}

The abstract in Finnish. Foreign students do not need this page.

Suomenkieliseen diplomityöhön kirjoitetaan tiivistelmä sekä suomeksi
että englanniksi.

Kandidaatintyön tiivistelmä kirjoitetaan ainoastaan kerran, samalla
kielellä kuin työ. Kuitenkin myös suomenkielisillä kandidaatintöillä
pitää olla englanninkielinen otsikko arkistointia varten.

\end{otherlanguage} % End on 2nd language part


\chapter*{Preface}

This document template conforms to Guide to Writing a Thesis at
Tampere University of Technology (2014) and is based on the previous
template. The main purpose is to show how the theses are formatted
using LaTeX (or \LaTeX ~ to be extra fancy) .

The thesis text is written into file \texttt{d\_tyo.tex}, whereas
\texttt{tutthesis.cls} contains the formatting instructions. Both
files include lots of comments (start with \%) that should help in
using LaTeX. TUT specific formatting is done by additional settings on
top of the original \texttt{report.cls} class file. This example needs
few additional files: TUT logo, example figure, example code, as well
as example bibliography and its formatting (\texttt{.bst}) An example
makefile is provided for those preferring command line. You are
encouraged to comment your work and to keep the length of lines
moderate, e.g. <80 characters. In Emacs, you can use \texttt{Alt-Q} to
break long lines in a paragraph and \texttt{Tab} to indent commands
(e.g. inside figure and table environments). Moreover, tex files are
well suited for versioning systems, such as Subversion or Git.  
% \url{http://www.ctan.org/tex-archive/info/lshort/english/lshort.pdf}

Acknowledgements to those who contributed to the thesis are generally
presented in the preface. It is not appropriate to criticize anyone in
the preface, even though the preface will not affect your grade. The
preface must fit on one page. Add the date, after which you have not
made any revisions to the text, at the end of the preface.

~ 
% Tilde ~ makes an non-breakable spce in LaTeX. Here it is used to get
% two consecutive paragraph breaks

Tampere, 11.8.2014

~

On behalf of the working group, Erno Salminen


% Add the table of contents, optioanlly also the lists of figures,
% tables and codes.

%\renewcommand\contentsname{Sisällys} % Set Finnish name, remove this if using English
\setcounter{tocdepth}{3}              % How many header level are included
\tableofcontents                      % Create TOC


%
% Term and symbol exaplanations use a special list type
%
\chapter*{List of abbreviations and symbols}
\markboth{}{}                                % no headers
%\chapter*{Lyhenteet ja merkinnät}

% You don't have to align these with whitespaces, but it makes the
% .tex file more readable
\begin{termlist}
\item[ANN] Artificial Neural Network
\item[LAS] Labelled Attachment Score
\item[LDA] Latent Dirilecht Allocation
\item[LSA] Latent Semantic Analysis
\item[LSTM] Long short term memory; type of RNN with short term memory.
\item[NER] Named entity recognition
\item[NLP] Natural Language Processing
\item[PMI] Pointwise mutual information
\item[POS] Part-of-speech; also called lexical category
\item[RNN] Recurrent neural network
\item[S-LSTM] Stack long short term memory
\item[TUT] Tampere University of Technology
\item[UAS] Unlabelled Attachment Score
\end{termlist}


\chapter*{Notes}

\section{Reading}
These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003). \cite{Pennington2014}


\section{Terms for Computational Linguistics}
\begin{description}
\item[1-of-V Coding] Representing words as sparse binary vectors which have 1 at the word's vocabulary index and 0 all others. With vocabulary \{dog, cat, mouse\}, dog becomes [1, 0, 0], cat becomes [0, 1, 0] and mouse [0, 0, 1].

\item[Bag-of-words] Multiset of words appearing in a text with occurrence counts for each word. Used as a tool for feature generation. Does not preserve word order or grammar. Can implemented as a dictionary (or associative array) where words are the keys and counts are the values.

\item[Conditional Random Field]

\item[Constituent] In syntactic analysis, a constituent is a word or a group of words that function(s) as a single unit within a hierarchical structure. Many constituents are phrases. \textit{Yesterday I saw \textbf{an orange bird with a white neck}}

\item[Corpus] A collection of texts with linguistic annotations.

\item[Dimensionality] When discussing word embeddings and word vector spaces the dimensionality refers to definition in linear algebra. Dimensionality of arrays in computing means the number of indices required to specify an element in the array. Word vector in 50 dimensional vector space $\mathbb{R}^{50}$ would be represented in computing as one dimensional array of length 50 [d1, d2, d3, ..., d50]

\item[Distributional Hypothesis] Words that are used and occur in the same contexts tend to purport similar meanings. \cite{Harris1954}

\item[Feature] Numeric data representation that can be effectively exploited in machine learning tasks. E.g. Word occurrence frequencies.

\item[Feature Vector] Vector containing all the features. For an image a feature vector could be all the raw values of pixels as a single sequence. For a trigram model with 300 dimensional word embeddings a feature vector would be a 900 dimensional vector formed by concatenating all the separate word embedding vectors.

\item[Gazzetteer] In Named Entity Recognition a gazzetteer is a dictionary of known named entities.

\item[Language Model] Probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $P(w_{1},\ldots ,w_{m})$ to the whole sequence. Problems caused by growing vocabulary can be addressed with continuous language models such as neural net language models (NNML). Word2Vec by \cite{Mikolov2013} addresses this problem with Continuous Bag-of-words and Skip-gram models.

\item[Lemmatisation] Process of finding the base form of a word, e.g. flew -> fly

\item[Lexeme] A basic lexical unit of a language consisting of one word or several words, the elements of which do not separately convey the meaning of the whole.

\item[n-gram] Probabilistic language model where probability of current word is the joint probability of previous \textit{n} words. Bigram example: $P(I, saw, the, red, house) \approx P(I | ^\wedge)P(saw | I)P(red | the)P(house | red)P(\$ | house)$. The words unigram, bigram and trigram language model denote n-gram model language models with n = 1, n = 2 and n = 3, respectively.

\item[One-hot] Group of bits which the legal comibations of values are only those with a single high (1) and all the others low (0). See also 1-of-V Coding.

\item[Parsing] Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.

\item[PMI] Word co-occurence probability metric. High values for words that occur often together.

\item[POS-tagging] Process of marking up a word to particular part-of-speech (nouns, verbs, etc...) based on both its definition and its context.

\item[ReLU] Rectified Linear Unit. $h(x) = max\{0, x\}$. Used as non-linear activation function in neural nets particularly in convolutional net

\item[Skip-gram] Language model which predicts the context (previous and next \textit{n} words) of a current word from the current word instead of traditional way of predicting current word from the context.

\item[Structured Prediction] Predicting structured objects, rather than scalar discrete or real values. Translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.

\item[Token] A structure representing a lexeme that explicitly indicates its categorization for the purpose of parsing. In plain words tokens are instances of words in a text. Not to be confused with word type.

\item[Tree bank] Parsed text corpus that annotates syntactic or semantic sentence structure. Contains trees for sentences where phrases in a sentence are structured in a tree of syntactic or semantic relations. Very useful for training POS-taggers etc...

\item[Tri-Training] Parsing unlabeled data with two different parses and selecting only the sentences for which the two parsers produce the same trees \cite{Weiss2015}

\item[Word Lookup Table] Matrix $\textbf{P} \in \mathbb{R}^{d \times |V|}$ of d rows and |V| columns, where d is the word vector dimensionality and |V| is the size of vocalbulary. Word lookup tables are unable to generate representations for previoulsy unseen words, as is required for morphology. \cite{Ling2015}

\item[Word Type] Unique words in a text. \textit{Good wine is good} has 4 tokens but only 3 word types.

\item[Word vector] N-dimensional vector representation of a word with interesting properties such as: vector('Paris') - vector('France') + Vector('Italy') -> vector('Rome')
\end{description}

\section{Universal Dependecies}

\subsection{CoNNL-U format}
Universal dependencies use CoNNL-U format for treebanks, CoNNL-U is revised version of CoNNL-X. Annotations are encoded in text files with word lines, blank lines for sentence boundaries and comments starting with hash (\#).

Word lines consist of following columns:
\begin{termlist}
\item[ID] Word ID in sentence
\item[FORM] Word form or punctuation symbol
\item[LEMMA] Lemma or stem of word form
\item[UPOSTAG] Universal part-of-speech tag
\item[XPOSTAG] Language specific part-of-speech tag
\item[FEATS] List of morphological features
\item[HEAD] Head of the curren token, value of ID or zero (0)
\item[DEPREL] Universal dependecy relation to the HEAD
\item[DEPS] List of secondary dependencies
\item[MISC] Any other annotation
\end{termlist}

Example in Finnish: Jäällä kävely avaa aina hauskoja ja erikoisia näkökulmia kaupunkiin

\begin{tabular}{l l l l l} 
ID & FORM & LEMMA & UPOSTAG & XPOSTAG \\
\hline
1 & Jäällä & jää & NOUN & N \\
2 & kävely & kävely & NOUN & N \\
3 & avaa & avata & VERB & V \\
4 & aina & aina & ADV & Adv \\
5 & hauskoja & hauska & ADJ & A \\
6 & ja & ja & CONJ & C \\
7 & erikoisia & erikoinen & ADJ & A \\
8 & näkökulmia & näkö\#kulma & NOUN & N \\
9 & kaupunkiin & kaupunki & NOUN & N \\
10 & . & . & PUNCT & Punct
\end{tabular}

\begin{tabular}{l}
FEATS \\
\hline
Case=Ade|Number=Sing \\
Case=Nom|Number=Sing \\
Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
\_ \\
Case=Par|Degree=Pos|Number=Plur \\
Case=Par|Number=Plur \\
Case=Ill|Number=Sing \\
\_
\end{tabular}

\begin{tabular}{l l l l}
HEAD & DEPREL & DEPS & MISC \\
\hline
2 & nmod & \_ & \_ \\
3 & nsubj & \_ & \_ \\
0 & root & \_ & \_ \\
3 & advmod & \_ & \_ \\
8 & amod & \_ & \_ \\
5 & cc & \_ & \_ \\
5 & conj & 8:amod & \_ \\
3 & dobj & \_ & \_ \\
8 & nmod & \_ & SpaceAfter=No
\end{tabular}

\subsection{Universal POS tags}
\begin{termlist}
\item[ADJ] Adjective. Describing word qualifying noun or noun phrase. \textbf{deep}, \textbf{intelligent}
\item[ADP] Adposition. Word expressing spatial or temporal relations \textbf{under}, \textbf{around}, \textbf{before} or mark various semantic roles \textbf{of}, \textbf{for}
\item[ADV] Adverb. Modifies another word. Typically express manner, place, time, frequency etc. She sang \textbf{loudly}. You are \textbf{quite} right.
\item[AUX] Auxiliary verb. A verb used in forming the tenses, moods, and voices of other verbs. \textbf{Do} you want tea?. He \textbf{has} given his all.
\item[CONJ] Coordinating conjunction. Conjunction placed between words, phrases, clauses or sentences of equal rank. \textbf{and}, \textbf{but}, \textbf{or}.
\item[DET] Determiner. Expresses reference of a noun (group). \textbf{The} girl is \textbf{a} student. \textbf{Which} book is that?
\item[INTJ] Interjection. Shows emotion or feeling of the author, includes exclamations, curses, greetings and such. \textbf{Ouch!}, \textbf{hey}, \textbf{huh?}.
\item[NOUN] Noun. Denotes a person, animal, place thing or idea. The \textbf{cat} sat on a \textbf{mat}.
\item[NUM] Numeral. Number, written with digits or letters. \textbf{12}, \textbf{eleven}.
\item[PART] Particle. Cannot be inflected. Interjections and conjunctions. In finnish also \textbf{että}, \textbf{jotta}, \textbf{koska}, \textbf{kun} etc...
\item[PRON] Pronoun. Replaces (often previously introduced) noun. Joe saw Jill, and \textbf{he} waved at \textbf{her}.
\item[PUNCT] Punctuation. Full stop, comma, bracket etc.
\item[SCONJ] Subordinating conjunction. A conjunction that introduces a subordinating clause, e.g. \textbf{although}, \textbf{because}, \textbf{whenever}.
\item[SYM] Symbol.
\item[VERB] Verb. Conveys an action \textbf{bring}, \textbf{read}, an occurrence \textbf{happen}, \textbf{become}, or a state of being \textbf{be}, \textbf{exist}.
\item[X] Other
\end{termlist}

\newpage

% The actual text begins here and page numbering changes to 1,2...
% Leave the backside of title empty in twoside mode
\if@twoside
%\newpage
\cleardoublepage
\fi

\pagenumbering{arabic}
\setcounter{page}{1} % Start numbering from zero because command
                     % 'chapter*' does page break
\renewcommand{\chaptername}{} % This disables the prefix 'Chapter' or
                              % 'Luku' in page headers (in 'twoside'
                              % mode)


\chapter{Introduction}
\label{ch:intro}
Testing citation \cite{Andor2016}


\chapter{Natural Language Processing}
\label{ch:natural_language_processing}
\begin{itemize}
\item Natural Language Processing is vastly wide field, this thesis discusses only on the sections of NLP relevant to the experiments.
\item Most of the NLP work has been for english.
\item Cross-linguistic annotation and parsing has been a reality only after introduction of The Universal Dependencies project and SyntaxNet.
\item Similarly Finnish parsing has been unreachable until the first Finnish corpus Turku Dependency Treebank \cite{Haverinen2014} and cross-linguistic parsers.
\item Traditionally NLP systems are tailored to the single problem at hand with hand engineered features suited for the problem. Recently general approach has received interest where feature engineering and task specific architectures are not needed. \cite{Collobert2011}, \cite{Zhang2015}
\item This thesis focuses on task specific systems because general language understanding systems are still lacking a good support for production systems and depend on vast amounts of good data \cite{Zhang2015} unobtainable within our time frame and resources.
\end{itemize}

\section{Pre-processing}
\label{ch:pre-processing}

\section{Feature Engingeering in NLP}
\label{ch:feature_engineering_in_nlp}
\begin{itemize}
\item Machine learning algorithms require words to be represented quantifiable features such as IDs or real number vectors.
\item Traditional feature selection requires hand engineered features.
\item Engineered features hog 95\% of the computation time. \cite{Chen2014}
\item Traditionally words have been represented by indices. \cite{Mikolov2013}
\item Next step was to use 1-of-V coding.
\item Index representation is simple as computationally cheap, making use of huge datasets possible. Simple models with huge data outperform complex models with less data. \cite{Mikolov2013}
\item See LSA and LDA for previous systems. Neural networks significantly outperform LSA in preserving linearities. LDA doesn't scale for large datasets. \cite{Mikolov2013}
\end{itemize}

\subsection{Word Embeddings}
\begin{itemize}
\item \textbf{\textcolor{red}{see section 1.2 of \cite{Mikolov2013} for previous work and history of word embeddings}}
\item Word embeddings represent words as n-dimensional vectors. \cite{Mikolov2013}
\item LSA leverages statistical information of a corpus but performs poorly on word analogy task. \cite{Pennington2014}
\item Skip-gram is good for word analogies but doesn't utilize corpus statistics well since vectors are trained on local context. \cite{Pennington2014}
\item Word embeddings try to map words with semantic similarities close to each other. Words may have several types of similarities such as \textit{France} and \textit{Italy} are countries but \textit{dogs} and \textit{triangles} are both in plural form. \cite{Mikolov2013a}
\item \cite{Chen2014} use 50 dimensional word embeddings created with Word2vec.
\item \cite{Chen2014} also use embeddings for POS tags and dependecy arcs. Only embedding POS tags has clear benefit, \cite{Chen2014} suspect that embedding arc labels have no effect since POS tags already contain the relational information.
\end{itemize}

\subsection{Word2vec}
\begin{itemize}
\item \cite{Mikolov2013}
\item Can be used with datasets of billions of words
\item Has two models: Continuous bag-of-words and continuous skip-gram
\item Continuous bag-of-words predicts current word from the context (surrounding words)
\item Continuous skip-gram predicts context (surrounding words) from current word.
\item Continuous Bag-of-Words is better for small datasets, continuous skip-gram is better for large datasets.
\item CBOW is better for syntax, Skip-gram is better for semantics.
\item Can be used to find semantic relationships like vector('biggest') - vector('big') + vector('small') => vector('smallest') 
\item State of the art (as of 2013)
\end{itemize}

\subsection{GloVe}
\begin{itemize}
\item GloVe by \cite{Pennington2014} capture global corpus statistics with log-bilinear co-occurence count model.
\item Memory requirements for GloVe are substantial since global co-occurence matrix for entire vocabulary is required, even though GloVe eliminates the need for zero occurence elements. Problem becomes worse for inflectional languages such as Finnish were vocabulary requires word type for each infliction for each word.
\item GloVe outperforms other methods on almost all tested tasks. All tasks are English only. \cite{Pennington2014}
\end{itemize}

\subsection{Charater to Word}
\begin{itemize}
\item Word embeddings can be generated from character sequences with significantly better performance for morphological languages. \cite{Ling2015}
\end{itemize}

\section{Annotations}
\label{se:annotations}
\begin{itemize}
\item Stanford Dependencies by \cite{DeMarneffe2006}
\item Stanford Dependencies emerged as de facto annotation scheme for english, but has been adapted to several other languages including Finnish. \cite{Nivre2016}, \cite{Haverinen2014}.
\item Turku Dependency Treebank has been tranformed into universal dependencies. \cite{Pyysalo2015}
\item Unified annotation scheme reduces need for cross-language adaptations in downstream development. \cite{Petrov2012}
\item Universal Dependencies project started from the requirement for cross-linguistically consistent treebank annotations even for morphological languages. \cite{Nivre2016}.
\item Universal Dependencies project was born from merging several previous attempts to form a cross-linguistically sound dependency annotation schemes. \cite{Nivre2016}
\item UD data has been encoded in the CoNLL-U format, a revision of the popular CoNLL-X format. \cite{Nivre2016}
\item UD treebanks released in November 2015. \cite{Nivre2016}
\end{itemize}

\subsection{Turku Dependecy Treebank}
\begin{itemize}
\item \cite{Haverinen2014}
\item Treebanks are needed in computational linguistics.
\item First Finnish treebank.
\item Open licence, including for text annotated
\item 204339 tokens, 15126 sentences
\item Based on Stanford Dependency scheme with minor modifications to exclude phenomena not present in Finnish and to include new annotations not present in English.
\item Transposed to CoNNL-U scheme by universal dependencies project
\item Connexor Machinese Syntax is the only currenty available Finnish full dependency parser.
\item Texts from 10 different categories ranging from news and legal text to blog entries and fiction.
\item Dependency parsing is done manually with full double annotation process.
\item Uses Omorfi for morphological analysis. Ambiguous tokens are handled partly manually, partly rule based and partly with machine learning.
\item FTB uses 3 different taggers for morphology, check them out!
\item FTB is 97\% grammar examples, meant for rule based POS tagger development
\end{itemize}

\section{Syntactic Parsing}
\label{se:syntactic_parsing}
\begin{itemize}
\item Commonly divided to constituency parsing and dependency parsing.
\item Constituency parser creates a parse tree of constituencies.
\item Dependency parser creates a parse tree of word token dependencies.
\item Constituency parsers are slower but more informal than dependency parsers. \cite{Fernandez-Gonzalez2015}
\item \cite{Fernandez-Gonzalez2015} show that it is possible to build constituency parser with dependency parser by reducing constituents to dependency parsing.
\end{itemize}

\subsection{POS-tagging}
\begin{itemize}
\item Started from rule based taggers
\item Tagger by \cite{Brill1992} (known as Brill tagger) learns the rules and as such can be considered as a hybrid approach
\item Contemporary research is focused on statistical and ANN based taggers
\item Rest of this section focuses on statistical parsers
\item \cite{Ling2015} introduced S-LSTM based State-of-the-art tagger
\item \cite{Andor2016} Improved accuracy with transition based tagger
\item \cite{Chen2014} were first to represent POS-tag and arc labels as embeddings
\item \cite{Andor2016} and \cite{Weiss2015} built their solutions based on \cite{Chen2014}
\item \cite{Nivre2004} introduced system for transition based taggers known as arc-starndard system. \cite{Chen2014}
\end{itemize}

\subsection{Transition Based Parsers}
\begin{itemize}
\item Good balance between efficiency and accuracy \cite{Weiss2015}
\item Parsed left to right; at each position the parses chooses action from a set of possible actions.
\item Greedy models are fast but error prone and need hand engineered features \cite{Weiss2015}
\item Actions can be chosen by ANN to avoid hand engineering \cite{Chen2014}, \cite{Weiss2015}
\end{itemize}

\subsection{Syntaxnet}
\begin{itemize}
\item Transition based
\item Locally and globally normalized
\item Backpropagation through entire net
\item State-of-the-Art
\item \cite{Andor2016}
\end{itemize}

\section{Co-Reference Parsing}
\label{se:co-reference_parsing}


\section{Sentence Segmentation}
\label{se:sentence_segmentation}
This is not relevant in our system?

\section{Machine Translation}
\label{se:machine_translation}
See section \ref{ss:translating_colloquial_speech} for more info on our implementation.

\section{Lemmatisation}
\label{se:lemmatisation}

\section{Synonym recognition}
\label{se:synonym_recognition}
\begin{itemize}
\item See OMorFi
\end{itemize}


\chapter{Chat Based Customer Support}
\label{ch:chat_based_customer_support}
Customer support problem domain. History?
\section{Chat Revolution}
\section{Automation with Virtual Agents}
\section{Existing Systems}


\chapter{Our System}
\label{ch:our_system}
\begin{itemize}
\item This thesis only considers customer message understanding.
\item Greetings, Chatting, information retrieval and response generation are important part of a dialogue system but have been left out of scope of this thesis.
\item Large portion of the research on the conversational models focus on chatter systems where utterance responses are retrieved or generated from response corpus. These systems do not aim to solve customer problem by incorporating relevant information retrieved from a specific knowledge base.
\item Utterance systems are not suitable for customer support out side of simple frequently asked questions.
\item Furthermore large portion of responses generated by neural conversation models are safe responses e.g. \textit{I don't know} \cite{Li2015}.
\item Conversational models trained on customer support history incorporate data from long time period and as such data may contain outdated information which cannot be corrected unless sentences containing outdated information are replaced. Finding and replacing such information from customer support dialogues requires reading much of the dialogues by humans and rewriting conversation history; task both time consuming, error prone and expensive.
\item Also incorporating new information into such dataset requires manual conversation synthesis where humans write new simulated conversations into a dialogue history used for training the system.
\item Other ways for updating old and adding new information are required for real world customer support. Our system draws it's information from a knowledge base with traditional user interface for updating and adding new knowledge.
\end{itemize}

\section{Pipeline}
\label{se:pipeline}

\subsection{Translating Colloquial Speech}
\label{ss:translating_colloquial_speech}
Mä -> Minä

\section{Evaluation}
\label{se:evaluation}
\begin{itemize}
\item Recently end-to-end dialogue system have adopted metrics from amchine translation and text summarization. These don't work so well. \cite{Liu2016}
\item \cite{Liu2016} considers unsupervised utterance systems where response is generated or selected from a set of possible responses. This is irrelevant since we are building a problem solving dialogue system instead of chattering system.
\end{itemize}


\chapter{Customer Business Value and Satisfaction}
\label{ch:customer_business_value_and_satisfaction}


\chapter{Conclusions and Future Work}
\label{ch:conclusions_and_future_work}


%
% The bibliography, i.e the list of references (3 options available)
%
\newpage


% Extra for Finnish theses

\renewcommand{\bibname}{Bibliography}     % Bilingual babel puts Finnish ``Kirjallisuttaa'' otherwise. Strange...
%\renewcommand{\bibname}{Lähteet}         % Set Finnish header, remove this if using English
%\addcontentsline{toc}{chapter}{Lähteet}  % Include this in TOC
\addcontentsline{toc}{chapter}{\bibname}  % Include this in TOC


\printbibliography                  % a) heading in English
%\printbibliography[title=Lähteet]   % b) heading in Finnish
%\addtocontents{toc}{%               % b) add Finnish heading to table of contents
%\protect\noindent Lähteet\protect\par
%} 


%
% Appendices are optional. 
% This part is semi-ugly at the moment. Please give feedback if can
% improve it.
\appendix
\pagestyle{headings}
% \renewcommand{\appendixname}{Liite} % Extra. Set Finnish prefix for page header

%
% a) Not-so-handy way, but at least it works
% 
\def\appA{APPENDIX A. Something extra} % Define the name and numbering manually
\chapter*{\appA}                       % Create chapter heading
\markboth{\appA}{\appA}                % Set page header
\addcontentsline{toc}{chapter}{\appA}  % Include this in TOC
% Note that \label does not work with unnumbered chapter

Appendices are purely optional.  All appendices must be referred to in
the body text

\def\appB{APPENDIX B. Something completely different} % Define another new command
\chapter*{\appB}                       % As above, but use \appB instead of \appA
\label{app:B}
\markboth{\appB}{\appB}                     
\addcontentsline{toc}{chapter}{\appB}  


You can append to your thesis, for example, lengthy mathematical
derivations, an important algorithm in a programming language, input
and output listings, an extract of a standard relating to your thesis,
a user manual, empirical knowledge produced while preparing the
thesis, the results of a survey, lists, pictures, drawings, maps,
complex charts (conceptual schema, circuit diagrams, structure charts)
and so on.


%
% b) The other option is to use numbered chapter and our baseline
% template report.cls numbers them as A, B... The heading and TOC do
% not include prefix 'Appendix' although the page header does.
%\chapter{name of the appendix}
%\label{app:A}                          % For cross-references



\end{document}

