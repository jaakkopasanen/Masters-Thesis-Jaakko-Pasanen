Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mikolov2013a,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations{\#}0$\backslash$nhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
year = {2013}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Haverinen2014,
abstract = {In this paper, we present the final version of a publicly available$\backslash$ntreebank of Finnish, the Turku Dependency Treebank. The treebank$\backslash$ncontains 204,399 tokens (15,126 sentences) from 10 different text$\backslash$nsources and has been manually annotated in a Finnish-specific version of$\backslash$nthe well-known Stanford Dependency scheme. The morphological analyses of$\backslash$nthe treebank have been assigned using a novel machine learning method to$\backslash$ndisambiguate readings given by an existing tool. As the second main$\backslash$ncontribution, we present the first open source Finnish dependency$\backslash$nparser, trained on the newly introduced treebank. The parser achieves a$\backslash$nlabeled attachment score of 81 {\%}. The treebank data as well as the$\backslash$nparsing pipeline are available under an open license at$\backslash$nhttp://bionlp.utu.fi/.},
author = {Haverinen, Katri and Nyblom, Jenna and Viljanen, Timo and Laippala, Veronika and Kohonen, Samuel and Missil{\"{a}}, Anna and Ojala, Stina and Salakoski, Tapio and Ginter, Filip},
doi = {10.1007/s10579-013-9244-1},
file = {:home/jaakko/Desktop/Masters-Thesis-Jaakko-Pasanen/sources/turku{\_}dependecy{\_}treebank.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Finnish,Morphology,Parsing,Treebank},
number = {3},
pages = {493--531},
title = {{Building the essential resources for Finnish: the Turku Dependency Treebank}},
volume = {48},
year = {2014}
}
@article{Durrett2013,
abstract = {Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features com-puted from hand-crafted heuristics. In con-trast, we present a state-of-the-art coreference system that captures such phenomena implic-itly, with a small number of homogeneous feature templates examining shallow proper-ties of mentions. Surprisingly, our features are actually more effective than the corre-sponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win " easy victories " without crafted heuris-tics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow seman-tic features from the literature, suggesting that this approach to semantics is an " uphill bat-tle. " Nonetheless, our final system 1 outper-forms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5{\%} absolute on the CoNLL metric and outperforms the IMS system (Bj{\"{o}}rkelund and Farkas (2012), the best publicly available En-glish coreference system) by 1.9{\%} absolute.},
author = {Durrett, Greg and Klein, Dan},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Durrett, Klein - 2013 - Easy victories and uphill battles in coreference resolution.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the Conference on Empirical {\ldots}},
number = {October},
pages = {1971--1982},
title = {{Easy victories and uphill battles in coreference resolution}},
url = {http://nlp.cs.berkeley.edu/pubs/Durrett-Klein{\_}2013{\_}Coreference{\_}paper.pdf},
year = {2013}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Martins2009a,
abstract = {We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.},
author = {Martins, Andr{\'{e}} F.T. and Smith, Noah A. and Xing, Eric P.},
doi = {10.3115/1687878.1687928},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Smith, Xing - 2009 - Concise integer linear programming formulations for dependency parsing.pdf:pdf},
isbn = {9781932432459},
issn = {1532-0618},
journal = {Proceedings ACL},
keywords = {dependency,parsing},
number = {August},
pages = {342--350},
title = {{Concise integer linear programming formulations for dependency parsing}},
url = {http://dl.acm.org/citation.cfm?id=1687928},
year = {2009}
}
@article{Brill1992,
abstract = {Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a sim-ple rule-based part of speech tagger which au-tomatically acquires its rules and tags with ac-curacy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor-pus genre or language to another. Perhaps the biggest contribution of this work is in demon-strating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, search-ing for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9406010},
author = {Brill, Eric},
doi = {10.3115/1075527.1075553},
eprint = {9406010},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brill - 1992 - A Simple Rule-Based Part of Speech Tagger.pdf:pdf},
isbn = {1558602720},
issn = {00992399},
journal = {Applied natural language},
pages = {3},
pmid = {12043861},
primaryClass = {cmp-lg},
title = {{A Simple Rule-Based Part of Speech Tagger}},
year = {1992}
}
@article{Martins2011,
abstract = {Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decompositions are not readily available (e.g., due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.},
author = {Martins, Andr{\'{e}} F T and Smith, Noah a and Aguiar, Pedro M Q and Figueiredo, M{\'{a}}rio a T},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins et al. - 2011 - Dual decomposition with many overlapping components.pdf:pdf},
isbn = {978-1-937284-11-4},
journal = {Emnlp},
keywords = {Dual Decomposition},
number = {1},
pages = {238--249},
title = {{Dual decomposition with many overlapping components}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145460},
year = {2011}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v2},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
doi = {10.18653/v1/P16-1231},
eprint = {arXiv:1603.06042v2},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks.pdf:pdf},
journal = {Acl 2016},
pages = {2442--2452},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Martins2014,
author = {Martins, Andr{\'{e}} F T and Almeida, Mariana S C and Labs, Priberam and Henriques, Alameda D Afonso},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins et al. - 2014 - Priberam A Turbo Semantic Parser with Second Order Features.pdf:pdf},
journal = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
number = {SemEval},
pages = {471--476},
title = {{Priberam: A Turbo Semantic Parser with Second Order Features}},
url = {http://www.aclweb.org/anthology/S14-2082},
year = {2014}
}
@article{Ratinov2009,
abstract = {We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.},
author = {Ratinov, Lev and Roth, Dan},
doi = {10.3115/1596374.1596399},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ratinov, Roth - 2009 - Design challenges and misconceptions in named entity recognition.pdf:pdf},
isbn = {9781932432299},
issn = {1932432299},
journal = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning CoNLL 09},
number = {June},
pages = {147},
title = {{Design challenges and misconceptions in named entity recognition}},
url = {http://portal.acm.org/citation.cfm?doid=1596374.1596399},
year = {2009}
}
@article{Petrov2009,
archivePrefix = {arXiv},
arxivId = {1104.2086},
author = {Petrov, Slav and Das, Dipanjan and Mcdonald, Ryan},
eprint = {1104.2086},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrov, Das, Mcdonald - 2009 - A Universal Part-of-Speech Tagset.pdf:pdf},
keywords = {annotation guidelines,multilinguality,part-of-speech tagging},
title = {{A Universal Part-of-Speech Tagset}},
year = {2009}
}
@article{Martins2010,
abstract = {We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1},
author = {Martins, Andr{\'{e}} F T and Smith, Noah A and Xing, Eric P and Aguiar, Pedro M Q and Figueiredo, M{\'{a}}rio A T},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins et al. - 2010 - Turbo parsers dependency parsing by approximate variational inference.pdf:pdf},
isbn = {1932432868},
journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
number = {October},
pages = {34--44},
title = {{Turbo parsers: dependency parsing by approximate variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1870658.1870662},
year = {2010}
}
@article{Khani2016,
abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100{\%} precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
archivePrefix = {arXiv},
arxivId = {1606.06368},
author = {Khani, Fereshte and Rinard, Martin and Liang, Percy},
eprint = {1606.06368},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khani, Rinard, Liang - 2016 - Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings.pdf:pdf},
journal = {Acl},
pages = {952--962},
title = {{Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings}},
year = {2016}
}
@article{Ling2015,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096},
author = {Ling, Wang and Luis, Tiago and Marujo, Lu{\'{i}}s Luis and Astudillo, Ramon Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel and Fermandez, Ramon and Amir, Silvio and Marujo, Lu{\'{i}}s Luis and Lu{\'{i}}s, Tiago},
doi = {10.18653/v1/D15-1176},
eprint = {1508.02096},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {1520--1530},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://dx.doi.org/10.18653/v1/d15-1176$\backslash$nfile:///Files/68/6810072d-e133-426e-807f-445df2840420.pdf$\backslash$npapers3://publication/doi/10.18653/v1/d15-1176$\backslash$nhttp://arxiv.org/abs/1508.02096},
year = {2015}
}
@article{Weiss2015,
abstract = {We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat-ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc-tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26{\%} un-labeled and 92.41{\%} labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.},
archivePrefix = {arXiv},
arxivId = {1506.06158},
author = {Weiss, David and Alberti, Chris and Collins, Michael and Petrov, Slav},
doi = {10.3115/v1/P15-1032},
eprint = {1506.06158},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiss et al. - 2015 - Structured Training for Neural Network Transition-Based Parsing.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
number = {2012},
pages = {323--333},
title = {{Structured Training for Neural Network Transition-Based Parsing}},
url = {http://www.aclweb.org/anthology/P15-1032},
year = {2015}
}
@article{Zhang2012,
author = {Zhang, Hao and McDonald, Ryan},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, McDonald - 2012 - Generalized Higher-Order Dependency Parsing with Cube Pruning.pdf:pdf},
isbn = {9781937284435},
journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {320--331},
title = {{Generalized Higher-Order Dependency Parsing with Cube Pruning}},
url = {http://www.aclweb.org/anthology/D12-1030},
year = {2012}
}
@article{Martins2013,
abstract = {We present fast, accurate, direct non-projective dependency parsers with third-order features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-of- the-art accuracies for the largest datasets (English, Czech, and German).},
author = {Martins, Andre and Almeida, Miguel and Smith, Noah A},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers.pdf:pdf},
isbn = {9781937284510},
journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
pages = {617--622},
title = {{Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers}},
url = {http://www.aclweb.org/anthology/P13-2109$\backslash$nhttp://www.cs.cmu.edu/{~}nasmith/papers/martins+almeida+smith.acl13.pdf},
year = {2013}
}
@article{Vigo2008,
author = {Vigo, Universidade De and Lagoas, Campus As and Labs, Priberam and Henriques, Alameda D Afonso},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vigo et al. - 2008 - Parsing as Reduction †.pdf:pdf},
number = {1983},
title = {{Parsing as Reduction †}},
year = {2008}
}
@article{Chen2014,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
author = {Chen, Danqi and Manning, Christopher D},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
isbn = {9781937284961},
issn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {i},
pages = {740--750},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {https://cs.stanford.edu/{~}danqi/papers/emnlp2014.pdf},
year = {2014}
}
@article{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus Robert},
doi = {10.1007/978-3-642-35289-8-3},
eprint = {9809069v1},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 2012 - Efficient backprop.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {9--48},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Efficient backprop}},
volume = {7700 LECTU},
year = {2012}
}
@article{Nivre2004,
abstract = {Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable},
author = {Nivre, Joakim},
doi = {10.3115/1613148.1613156},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nivre - 2004 - Incrementality in deterministic dependency parsing.pdf:pdf},
journal = {Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together},
pages = {50--57},
title = {{Incrementality in deterministic dependency parsing}},
url = {http://dl.acm.org/citation.cfm?id=1613156},
year = {2004}
}
@article{Martins2009,
abstract = {Recent approaches to learning structured predictors often require approximate infer- ence for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model class. This paper sheds some light on the first issue by establishing risk bounds for max-margin learning with LP relaxed inference and ad- dresses the second issue by proposing a new paradigm that attempts to penalize “time- consuming” hypotheses. Our analysis relies on a geometric characterization of the outer polyhedra associated with the LP relaxation. We then apply these techniques to the prob- lem of dependency parsing, for which a con- cise LP formulation is provided that handles non-local output features. A significant im- provement is shown over arc-factored models.},
author = {Martins, Andr{\'{e}} F. T. and Smith, Noah a. and Xing, Eric P.},
doi = {10.1145/1553374.1553466},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Smith, Xing - 2009 - Polyhedral outer approximations with application to natural language parsing.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
title = {{Polyhedral outer approximations with application to natural language parsing}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553466},
year = {2009}
}
