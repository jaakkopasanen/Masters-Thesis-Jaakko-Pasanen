Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Nivre2004,
abstract = {Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable},
author = {Nivre, Joakim},
doi = {10.3115/1613148.1613156},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nivre - 2004 - Incrementality in deterministic dependency parsing.pdf:pdf},
journal = {Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together},
pages = {50--57},
title = {{Incrementality in deterministic dependency parsing}},
url = {http://dl.acm.org/citation.cfm?id=1613156},
year = {2004}
}
@article{Liang2016,
abstract = {Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.},
archivePrefix = {arXiv},
arxivId = {1611.00020},
author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
eprint = {1611.00020},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Neural Symbolic Machines Learning Semantic Parsers on Freebase with Weak Supervision.pdf:pdf},
number = {Figure 1},
pages = {1--10},
title = {{Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision}},
url = {http://arxiv.org/abs/1611.00020},
year = {2016}
}
@article{Kanerva2014,
abstract = {{\textcopyright} 2014 The Authors and IOS Press. In this paper, we report on the development of a large-scale Finnish Internet parsebank, currently consisting of 1.5 billion tokens in 116 million sentences. The data is fully morphologically and syntactically analyzed and it has been used to extract flat and syntactic n-gram collections, as well as verb-argument and noun-argument n-grams. Additionally, distributional vector space representations of the words are induced using the word2vec method. All n-gram collections as well as the vector space models are made available under an open license.},
author = {Kanerva, Jenna and Luotolahti, Juhani and Laippala, Veronika and Ginter, Filip},
doi = {10.3233/978-1-61499-442-8-184},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva et al. - 2014 - Syntactic N-gram Collection from a Large-Scale Corpus of Internet Finnish.pdf:pdf},
isbn = {9781614994411},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Finnish,large-scale,n-grams,syntactic n-grams,syntactic parsing},
pages = {184--191},
title = {{Syntactic N-gram Collection from a Large-Scale Corpus of Internet Finnish}},
volume = {268},
year = {2014}
}
@article{Ginter2014,
abstract = {In this paper, we study methods to train the popular word2vec vector space representation of the lexicon using only n-gram collections. By using the n-grams rather than the full text corpus we gain a substantial speed-up in training, as well as get the opportunity to train from corpora which would not be available otherwise.},
author = {Ginter, Filip and Kanerva, Jenna},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ginter, Kanerva - 2014 - Fast Training of word2vec Representations Using N-gram Corpora.pdf:pdf},
title = {{Fast Training of word2vec Representations Using N-gram Corpora}},
year = {2014}
}
@article{DeMarneffe2006,
abstract = {This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.},
author = {{De Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
doi = {10.1.1.74.3875},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Marneffe, MacCartney, Manning - 2006 - Generating typed dependency parses from phrase structure parses.pdf:pdf},
journal = {Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006)},
pages = {449--454},
title = {{Generating typed dependency parses from phrase structure parses}},
url = {http://nlp.stanford.edu/pubs/LREC06{\_}dependencies.pdf},
year = {2006}
}
@article{Chen2014,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
author = {Chen, Danqi and Manning, Christopher D},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
isbn = {9781937284961},
issn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {i},
pages = {740--750},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {https://cs.stanford.edu/{~}danqi/papers/emnlp2014.pdf},
year = {2014}
}
@article{Martins2010,
abstract = {We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1},
author = {Martins, Andr{\'{e}} F T and Smith, Noah A and Xing, Eric P and Aguiar, Pedro M Q and Figueiredo, M{\'{a}}rio A T},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins et al. - 2010 - Turbo parsers dependency parsing by approximate variational inference.pdf:pdf},
isbn = {1932432868},
journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
number = {October},
pages = {34--44},
title = {{Turbo parsers: dependency parsing by approximate variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1870658.1870662},
year = {2010}
}
@article{Zhang2015,
abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and LeCun, Yann},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, LeCun - 2015 - Text Understanding from Scratch.pdf:pdf},
isbn = {0123456789},
issn = {2166-532X},
journal = {APL Materials},
number = {1},
pages = {011102},
pmid = {25246403},
title = {{Text Understanding from Scratch}},
url = {http://arxiv.org/abs/1502.01710},
volume = {3},
year = {2015}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Brill1992,
abstract = {Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a sim-ple rule-based part of speech tagger which au-tomatically acquires its rules and tags with ac-curacy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor-pus genre or language to another. Perhaps the biggest contribution of this work is in demon-strating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, search-ing for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9406010},
author = {Brill, Eric},
doi = {10.3115/1075527.1075553},
eprint = {9406010},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brill - 1992 - A Simple Rule-Based Part of Speech Tagger.pdf:pdf},
isbn = {1558602720},
issn = {00992399},
journal = {Applied natural language},
pages = {3},
pmid = {12043861},
primaryClass = {cmp-lg},
title = {{A Simple Rule-Based Part of Speech Tagger}},
year = {1992}
}
@article{Bhargava2013,
author = {Bhargava, A. and Celikyilmaz, A. and Hakkani-Tur, D. and Sarikaya, R.},
doi = {10.1109/ICASSP.2013.6639291},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhargava et al. - 2013 - Easy contextual intent prediction and slot detection.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {contextual models,intent prediction,slot detection,spoken language understanding},
pages = {8337--8341},
title = {{Easy contextual intent prediction and slot detection}},
year = {2013}
}
@article{Zhang2012,
author = {Zhang, Hao and McDonald, Ryan},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, McDonald - 2012 - Generalized Higher-Order Dependency Parsing with Cube Pruning(2).pdf:pdf},
isbn = {9781937284435},
journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {320--331},
title = {{Generalized Higher-Order Dependency Parsing with Cube Pruning}},
url = {http://www.aclweb.org/anthology/D12-1030},
year = {2012}
}
@article{Li2015,
abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., $\backslash$textit{\{}I don't know{\}}) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in $\backslash$bleu scores on two conversational datasets.},
archivePrefix = {arXiv},
arxivId = {1510.03055},
author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, William B.},
eprint = {1510.03055},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - A Diversity-Promoting Objective Function for Neural Conversation Models.pdf:pdf},
journal = {Arxiv},
number = {Mmi},
pages = {110--119},
title = {{A Diversity-Promoting Objective Function for Neural Conversation Models}},
url = {http://arxiv.org/abs/1510.03055},
year = {2015}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Ling2015,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096},
author = {Ling, Wang and Luis, Tiago and Marujo, Lu{\'{i}}s Luis and Astudillo, Ramon Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel and Fermandez, Ramon and Amir, Silvio and Marujo, Lu{\'{i}}s Luis and Lu{\'{i}}s, Tiago},
doi = {10.18653/v1/D15-1176},
eprint = {1508.02096},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {1520--1530},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://dx.doi.org/10.18653/v1/d15-1176{\$}{\%}5C{\$}nfile:///Files/68/6810072d-e133-426e-807f-445df2840420.pdf{\$}{\%}5C{\$}npapers3://publication/doi/10.18653/v1/d15-1176{\$}{\%}5C{\$}nhttp://arxiv.org/abs/1508.02096},
year = {2015}
}
@article{Liu2016a,
abstract = {Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56{\%} absolute (23.8{\%} relative) error reduction on intent detection and 0.23{\%} absolute gain on slot filling over the independent task models.},
archivePrefix = {arXiv},
arxivId = {1609.01454},
author = {Liu, Bing and Lane, Ian},
doi = {10.21437/Interspeech.2016-1352},
eprint = {1609.01454},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lane - 2016 - Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling.pdf:pdf},
number = {1},
pages = {2--6},
title = {{Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling}},
url = {http://arxiv.org/abs/1609.01454},
year = {2016}
}
@article{Xu2013,
abstract = {Multi-intent natural language sentence classification aims at identifying multiple user goals in a single natural language sentence (e.g., "find Beyonce's movie and music" ! find movie, find music). The main motivation of this work is to exploit the shared intents across different intent combinations rather than treating the combination as an atomic label. We propose to achieve this by (1) adding class features, and (2) adding hidden variables to identify segments belonging to each intent. Experimental results demonstrate significant gains in classification accuracy over the baseline methods across a number of training conditions (3{\%}-8{\%} absolute on multi-intent sentences, 2{\%}-3{\%} absolute on single intent sentences). Copyright {\textcopyright} 2013 ISCA.},
author = {Xu, Puyang and Sarikaya, Ruhi},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Sarikaya - 2013 - Exploiting shared information for multi-intent natural language sentence classification.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Hidden variable models,Semantic classification,Spoken language understanding},
number = {1},
pages = {3785--3789},
title = {{Exploiting shared information for multi-intent natural language sentence classification}},
year = {2013}
}
@article{Khani2016,
abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100{\%} precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
archivePrefix = {arXiv},
arxivId = {1606.06368},
author = {Khani, Fereshte and Rinard, Martin and Liang, Percy},
eprint = {1606.06368},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khani, Rinard, Liang - 2016 - Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings.pdf:pdf},
journal = {Acl},
pages = {952--962},
title = {{Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings}},
year = {2016}
}
@article{Li2016,
abstract = {We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gain in speaker consistency as measured by human judges.},
archivePrefix = {arXiv},
arxivId = {1603.06155},
author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
eprint = {1603.06155},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - A Persona-Based Neural Conversation Model.pdf:pdf},
journal = {Acl},
pages = {10},
title = {{A Persona-Based Neural Conversation Model}},
url = {http://arxiv.org/abs/1603.06155},
year = {2016}
}
@article{Weiss2015,
abstract = {We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat-ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc-tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26{\%} un-labeled and 92.41{\%} labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.},
archivePrefix = {arXiv},
arxivId = {1506.06158},
author = {Weiss, David and Alberti, Chris and Collins, Michael and Petrov, Slav},
doi = {10.3115/v1/P15-1032},
eprint = {1506.06158},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiss et al. - 2015 - Structured Training for Neural Network Transition-Based Parsing(2).pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
number = {2012},
pages = {323--333},
title = {{Structured Training for Neural Network Transition-Based Parsing}},
url = {http://www.aclweb.org/anthology/P15-1032},
year = {2015}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v2},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
doi = {10.18653/v1/P16-1231},
eprint = {arXiv:1603.06042v2},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks(2).pdf:pdf},
journal = {Acl 2016},
pages = {2442--2452},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Petrov2012,
archivePrefix = {arXiv},
arxivId = {1104.2086},
author = {Petrov, Slav and Das, Dipanjan and Mcdonald, Ryan},
eprint = {1104.2086},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrov, Das, Mcdonald - 2012 - A Universal Part-of-Speech Tagset.pdf:pdf},
keywords = {annotation guidelines,multilinguality,part-of-speech tagging},
title = {{A Universal Part-of-Speech Tagset}},
year = {2012}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M.},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1502.05698},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks.pdf:pdf},
isbn = {1502.05698},
issn = {1502.05698},
journal = {arXiv preprint},
keywords = {I,boring formatting information,machine learning},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
year = {2015}
}
@article{Levy2014,
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Levy, Omer and Goldberg, Yoav},
eprint = {1405.4053},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - 2014 - Neural Word Embedding as Implicit Matrix Factorization.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {2177--2185},
title = {{Neural Word Embedding as Implicit Matrix Factorization}},
url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
year = {2014}
}
@article{Pyysalo2015,
abstract = {There has been substantial recent interest in annotation schemes that can be applied consistently to many languages. Building on several recent efforts to unify morpho-logical and syntactic annotation, the Uni-versal Dependencies (UD) project seeks to introduce a cross-linguistically appli-cable part-of-speech tagset, feature inven-tory, and set of dependency relations as well as a large number of uniformly an-notated treebanks. We present Univer-sal Dependencies for Finnish, one of the ten languages in the recent first release of UD project treebank data. We detail the mapping of previously introduced annota-tion to the UD standard, describing spe-cific challenges and their resolution. We additionally present parsing experiments comparing the performance of a state-of-the-art parser trained on a language-specific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing tar-get. The introduced tools and resources are available under open licenses from},
author = {Pyysalo, Sampo and Kanerva, Jenna and Missil{\"{a}}, Anna and Laippala, Veronika and Ginter, Filip},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pyysalo et al. - 2015 - Universal Dependencies for Finnish.pdf:pdf},
journal = {Nordic Conference of Computational Linguistics NODALIDA 2015},
number = {Nodalida},
pages = {163},
title = {{Universal Dependencies for Finnish}},
year = {2015}
}
@article{Lankinen2016,
abstract = {Inspired by recent research, we explore ways to model the highly morphological Finnish language at the level of characters while maintaining the performance of word-level models. We propose a new Character-to-Word-to-Character (C2W2C) compositional language model that uses characters as input and output while still internally processing word level embeddings. Our preliminary experiments, using the Finnish Europarl V7 corpus, indicate that C2W2C can respond well to the challenges of morphologically rich languages such as high out of vocabulary rates, the prediction of novel words, and growing vocabulary size. Notably, the model is able to correctly score inflectional forms that are not present in the training data and sample grammatically and semantically correct Finnish sentences character by character.},
archivePrefix = {arXiv},
arxivId = {1612.03266},
author = {Lankinen, Matti and Heikinheimo, Hannes and Takala, Pyry and Raiko, Tapani and Karhunen, Juha},
eprint = {1612.03266},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lankinen et al. - 2016 - A Character-Word Compositional Neural Language Model for Finnish.pdf:pdf},
pages = {1--11},
title = {{A Character-Word Compositional Neural Language Model for Finnish}},
url = {http://arxiv.org/abs/1612.03266},
year = {2016}
}
@article{Liang2016a,
abstract = {Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.},
archivePrefix = {arXiv},
arxivId = {1611.00020},
author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
eprint = {1611.00020},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Neural Symbolic Machines Learning Semantic Parsers on Freebase with Weak Supervision(2).pdf:pdf},
number = {October},
title = {{Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision}},
url = {http://arxiv.org/abs/1611.00020},
year = {2016}
}
@article{Haverinen2014,
abstract = {In this paper, we present the final version of a publicly available$\backslash$ntreebank of Finnish, the Turku Dependency Treebank. The treebank$\backslash$ncontains 204,399 tokens (15,126 sentences) from 10 different text$\backslash$nsources and has been manually annotated in a Finnish-specific version of$\backslash$nthe well-known Stanford Dependency scheme. The morphological analyses of$\backslash$nthe treebank have been assigned using a novel machine learning method to$\backslash$ndisambiguate readings given by an existing tool. As the second main$\backslash$ncontribution, we present the first open source Finnish dependency$\backslash$nparser, trained on the newly introduced treebank. The parser achieves a$\backslash$nlabeled attachment score of 81 {\%}. The treebank data as well as the$\backslash$nparsing pipeline are available under an open license at$\backslash$nhttp://bionlp.utu.fi/.},
author = {Haverinen, Katri and Nyblom, Jenna and Viljanen, Timo and Laippala, Veronika and Kohonen, Samuel and Missil{\"{a}}, Anna and Ojala, Stina and Salakoski, Tapio and Ginter, Filip},
doi = {10.1007/s10579-013-9244-1},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haverinen et al. - 2014 - Building the essential resources for Finnish the Turku Dependency Treebank.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Finnish,Morphology,Parsing,Treebank},
number = {3},
pages = {493--531},
title = {{Building the essential resources for Finnish: the Turku Dependency Treebank}},
volume = {48},
year = {2014}
}
@article{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
eprint = {1508.06615},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2016 - Character-Aware Neural Language Models.pdf:pdf},
journal = {Aaai},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Schuster2016,
abstract = {Many shallow natural language understanding tasks use dependency trees to extract relations between content words. However, strict surface-structure dependency trees tend to follow the linguistic structure of sentences too closely and frequently fail to provide direct relations between content words. To mitigate this problem, the original Stanford Dependencies representation also defines two dependency graph representations which contain additional and augmented relations that explicitly capture otherwise implicit relations between content words. In this paper, we revisit and extend these dependency graph representations in light of the recent Universal Dependencies (UD) initiative and provide a detailed account of an enhanced and an enhanced++ English UD representation. We further present a converter from constituency to basic, i.e., strict surface structure, UD trees, and a converter from basic UD trees to enhanced and enhanced++ English UD graphs. We release both converters as part of Stanford CoreNLP and the Stanford Parser.},
author = {Schuster, Sebastian and Manning, Christopher D.},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuster, Manning - 2016 - Enhanced English Universal Dependencies An Improved Representation for Natural Language Understanding Tasks.pdf:pdf},
journal = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
keywords = {semantic representation,treebank conversion,universal dependencies},
pages = {2371--2378},
title = {{Enhanced English Universal Dependencies: An Improved Representation for Natural Language Understanding Tasks}},
url = {http://nlp.stanford.edu/{~}sebschu/pubs/schuster-manning-lrec2016.pdf},
year = {2016}
}
@article{Korenius2004,
abstract = {Stemming and lemmatization were compared in the clustering of Finnish text documents. Since Finnish is a highly inflectional and agglutinative language, we hypothesized that lemmatization, involving splitting of the compound words, would be more appropriate normalization approach than the straightforward stemming. The relevance of the documents were evaluated with a four-point relevance assessment scale, which was collapsed into binary one by considering all the relevant and only the highly relevant documents relevant, respectively. Experiments with four hierarchical clustering methods supported the hypothesis. The stringent relevance scale showed that lemmatization allowed the single and complete linkage methods to recover especially the highly relevant documents better than stemming. In comparison with stemming, lemmatization together with the average linkage and Ward's methods produced higher precision. We conclude that lemmatization is a better word normalization method than stemming, when Finnish text documents are clustered for information retrieval.},
author = {Korenius, Tuomo and Laurikkala, Jorma and J{\"{a}}rvelin, Kalervo and Juhola, Martti},
doi = {10.1145/1031171.1031285},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korenius et al. - 2004 - Stemming and lemmatization in the clustering of finnish text documents.pdf:pdf},
isbn = {1581138741},
journal = {Proceedings of the thirteenth ACM conference on information and knowledge management},
keywords = {clustering,lemmatization,normalization,stemming},
pages = {625--633},
title = {{Stemming and lemmatization in the clustering of finnish text documents}},
url = {http://portal.acm.org/citation.cfm?id=1031171.1031285{\&}coll=Portal{\&}dl=ACM{\&}CFID=88534260{\&}CFTOKEN=49348956},
year = {2004}
}
@article{Yao2015,
abstract = {In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.},
archivePrefix = {arXiv},
arxivId = {1510.08565},
author = {Yao, Kaisheng and Zweig, Geoffrey and Peng, Baolin},
eprint = {1510.08565},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao, Zweig, Peng - 2015 - Attention with Intention for a Neural Network Conversation Model.pdf:pdf},
journal = {NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction},
pages = {1--7},
title = {{Attention with Intention for a Neural Network Conversation Model}},
url = {http://arxiv.org/abs/1510.08565},
year = {2015}
}
@article{Chung2016,
abstract = {The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.},
archivePrefix = {arXiv},
arxivId = {1603.06147},
author = {Chung, Junyoung and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1603.06147},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung, Cho, Bengio - 2016 - A Character-level Decoder without Explicit Segmentation for Neural Machine Translation.pdf:pdf},
journal = {Acl-2016},
pages = {1693--1703},
title = {{A Character-level Decoder without Explicit Segmentation for Neural Machine Translation}},
year = {2016}
}
@article{Serban2016a,
abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
archivePrefix = {arXiv},
arxivId = {1507.04808},
author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
eprint = {1507.04808},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Serban et al. - 2016 - Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.pdf:pdf},
journal = {Aaai},
pages = {8},
title = {{Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models}},
url = {http://arxiv.org/abs/1507.04808},
year = {2016}
}
@article{Mikolov2013a,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations{\#}0{\%}5Cnhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
year = {2013}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Weston2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.06045v2},
author = {Weston, Jason},
eprint = {arXiv:1604.06045v2},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston - 2016 - Dialog-based Language Learning.pdf:pdf},
number = {Nips},
title = {{Dialog-based Language Learning}},
year = {2016}
}
@article{Harris1954,
author = {Harris, Zellig S},
doi = {10.1080/00437956.1954.11659520},
journal = {WORD},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://dx.doi.org/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
doi = {10.3115/v1/P14-1023},
eprint = {arXiv:1011.1669v3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni, Dinu, Kruszewski - 2014 - Don't count , predict ! A systematic comparison of context-counting vs . context-predicting semantic v.pdf:pdf},
isbn = {9781937284725},
issn = {1529-1898},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.},
pages = {238--247},
pmid = {25246403},
title = {{Don't count , predict ! A systematic comparison of context-counting vs . context-predicting semantic vectors}},
year = {2014}
}
@article{Fern2015,
abstract = {We reduce phrase-representation parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, "head-ordered dependency trees", shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-the-shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as the Berkeley parser for English and the best single system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin.},
archivePrefix = {arXiv},
arxivId = {1503.00030},
author = {Fern, Daniel},
eprint = {1503.00030},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern - 2015 - Parsing as Reduction.pdf:pdf},
isbn = {9781941643723},
journal = {In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
number = {1983},
pages = {1523--1533},
title = {{Parsing as Reduction}},
year = {2015}
}
@article{Nivre2016,
abstract = {Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.},
author = {Nivre, Joakim and Marneffe, Marie-catherine De and Ginter, Filip and Goldberg, Yoav and Manning, Christopher D and Mcdonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nivre et al. - 2016 - Universal Dependencies v1 A Multilingual Treebank Collection.pdf:pdf},
isbn = {978-2-9517408-9-1},
journal = {Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)},
keywords = {annotation,cross-linguistic,dependency,multilingual,treebanks,universal},
pages = {1659--1666},
title = {{Universal Dependencies v1: A Multilingual Treebank Collection}},
year = {2016}
}
@article{Ling2015,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096},
author = {Ling, Wang and Luis, Tiago and Marujo, Lu{\'{i}}s Luis and Astudillo, Ramon Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel and Fermandez, Ramon and Amir, Silvio and Marujo, Lu{\'{i}}s Luis and Lu{\'{i}}s, Tiago},
doi = {10.18653/v1/D15-1176},
eprint = {1508.02096},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation(2).pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {1520--1530},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://dx.doi.org/10.18653/v1/d15-1176{\%}5Cnfile:///Files/68/6810072d-e133-426e-807f-445df2840420.pdf{\%}5Cnpapers3://publication/doi/10.18653/v1/d15-1176{\%}5Cnhttp://arxiv.org/abs/1508.02096},
year = {2015}
}
@article{Zhang2016,
author = {Zhang, Xiaodong and Wang, Houfeng},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Wang - 2016 - A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding.pdf:pdf},
journal = {Ijcai},
keywords = {Natural Language Processing},
pages = {2993--2999},
title = {{A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding}},
year = {2016}
}
@article{Liu2016,
abstract = {We investigate evaluation metrics for end-to-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model's generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
archivePrefix = {arXiv},
arxivId = {1603.08023},
author = {Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V. and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
eprint = {1603.08023},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response.pdf:pdf},
journal = {Annual Meeting of the Association for Computational Linguistics (ACL)},
pages = {13},
title = {{How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation}},
url = {http://arxiv.org/abs/1603.08023},
year = {2016}
}
@article{Fernandez-Gonzalez2015,
author = {Fern{\'{a}}ndez-Gonz{\'{a}}lez, Daniel and Martins, Andr{\'{e}} F T},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern - 2015 - Parsing as Reduction.pdf:pdf},
journal = {CoRR},
title = {{Parsing as Reduction}},
url = {http://arxiv.org/abs/1503.00030},
volume = {abs/1503.0},
year = {2015}
}
@article{Silfverberg2015,
author = {Silfverberg, Miikka and Ruokolainen, Teemu and Lind??n, Krister and Kurimo, Mikko},
doi = {10.1007/s10579-015-9326-3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silfverberg et al. - 2015 - FinnPos an open-source morphological tagging and lemmatization toolkit for Finnish.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Averaged perceptron,Data-driven lemmatization,Finnish,Morphological tagging,Open-source},
number = {4},
pages = {863--878},
title = {{FinnPos: an open-source morphological tagging and lemmatization toolkit for Finnish}},
volume = {50},
year = {2016}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v2},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
doi = {10.18653/v1/P16-1231},
eprint = {arXiv:1603.06042v2},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks(2).pdf:pdf},
journal = {Acl 2016},
pages = {2442--2452},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Kestemont2016,
author = {Kestemont, Mike and de Pauw, Guy and van Nie, Renske and Daelemans, Walter},
doi = {10.1093/llc/fqw034},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kestemont et al. - 2016 - Lemmatization for variation-rich languages using deep learning.pdf:pdf},
issn = {2055-7671},
journal = {Digital Scholarship in the Humanities},
pages = {fqw034},
title = {{Lemmatization for variation-rich languages using deep learning}},
url = {http://dsh.oxfordjournals.org/lookup/doi/10.1093/llc/fqw034},
year = {2016}
}
@article{Takala2016,
abstract = {Abstract. Word-embedding models commonly treat words as unique symbols, for which a lower-dimensional embedding can be looked up. These representations generalize poorly with morphologically rich lan- guages, as vectors for all possible inflections cannot be stored, and words with the same stem do not share a similar representation. We study al- ternative representations for words, including one subword-model and two character-based models. Our methods outperform classical word embed- dings for a morphologically rich language, Finnish, on tasks requiring so- phisticated understanding of grammar and context. Our embeddings are easier to implement than previously proposed methods, and can be used to form word-representations for any common language processing tasks.},
author = {Takala, Pyry},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Takala - 2016 - Word Embeddings for Morphologically Rich Languages.pdf:pdf},
isbn = {9782875870278},
number = {April},
pages = {27--29},
title = {{Word Embeddings for Morphologically Rich Languages}},
year = {2016}
}
