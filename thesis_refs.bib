Automatically generated by Mendeley Desktop 1.17.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Korenius2004,
abstract = {Stemming and lemmatization were compared in the clustering of Finnish text documents. Since Finnish is a highly inflectional and agglutinative language, we hypothesized that lemmatization, involving splitting of the compound words, would be more appropriate normalization approach than the straightforward stemming. The relevance of the documents were evaluated with a four-point relevance assessment scale, which was collapsed into binary one by considering all the relevant and only the highly relevant documents relevant, respectively. Experiments with four hierarchical clustering methods supported the hypothesis. The stringent relevance scale showed that lemmatization allowed the single and complete linkage methods to recover especially the highly relevant documents better than stemming. In comparison with stemming, lemmatization together with the average linkage and Ward's methods produced higher precision. We conclude that lemmatization is a better word normalization method than stemming, when Finnish text documents are clustered for information retrieval.},
author = {Korenius, Tuomo and Laurikkala, Jorma and J{\"{a}}rvelin, Kalervo and Juhola, Martti},
doi = {10.1145/1031171.1031285},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korenius et al. - 2004 - Stemming and lemmatization in the clustering of finnish text documents.pdf:pdf},
isbn = {1581138741},
journal = {Proceedings of the thirteenth ACM conference on information and knowledge management},
keywords = {clustering,lemmatization,normalization,stemming},
pages = {625--633},
title = {{Stemming and lemmatization in the clustering of finnish text documents}},
url = {http://portal.acm.org/citation.cfm?id=1031171.1031285{\&}coll=Portal{\&}dl=ACM{\&}CFID=88534260{\&}CFTOKEN=49348956},
year = {2004}
}
@article{Haverinen2014,
abstract = {In this paper, we present the final version of a publicly available$\backslash$ntreebank of Finnish, the Turku Dependency Treebank. The treebank$\backslash$ncontains 204,399 tokens (15,126 sentences) from 10 different text$\backslash$nsources and has been manually annotated in a Finnish-specific version of$\backslash$nthe well-known Stanford Dependency scheme. The morphological analyses of$\backslash$nthe treebank have been assigned using a novel machine learning method to$\backslash$ndisambiguate readings given by an existing tool. As the second main$\backslash$ncontribution, we present the first open source Finnish dependency$\backslash$nparser, trained on the newly introduced treebank. The parser achieves a$\backslash$nlabeled attachment score of 81 {\%}. The treebank data as well as the$\backslash$nparsing pipeline are available under an open license at$\backslash$nhttp://bionlp.utu.fi/.},
author = {Haverinen, Katri and Nyblom, Jenna and Viljanen, Timo and Laippala, Veronika and Kohonen, Samuel and Missil{\"{a}}, Anna and Ojala, Stina and Salakoski, Tapio and Ginter, Filip},
doi = {10.1007/s10579-013-9244-1},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haverinen et al. - 2014 - Building the essential resources for Finnish the Turku Dependency Treebank.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Finnish,Morphology,Parsing,Treebank},
number = {3},
pages = {493--531},
title = {{Building the essential resources for Finnish: the Turku Dependency Treebank}},
volume = {48},
year = {2014}
}
@article{Khani2016,
abstract = {Can we train a system that, on any new input, either says "don't know" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100{\%} precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.},
archivePrefix = {arXiv},
arxivId = {1606.06368},
author = {Khani, Fereshte and Rinard, Martin and Liang, Percy},
eprint = {1606.06368},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khani, Rinard, Liang - 2016 - Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings.pdf:pdf},
journal = {Acl},
pages = {952--962},
title = {{Unanimous Prediction for 100 {\%} Precision with Application to Learning Semantic Mappings}},
year = {2016}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v2},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
doi = {10.18653/v1/P16-1231},
eprint = {arXiv:1603.06042v2},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andor et al. - 2016 - Globally Normalized Transition-Based Neural Networks.pdf:pdf},
journal = {Acl 2016},
pages = {2442--2452},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Harris1954,
author = {Harris, Zellig S},
doi = {10.1080/00437956.1954.11659520},
journal = {WORD},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://dx.doi.org/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Ratinov2009,
abstract = {We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.},
author = {Ratinov, Lev and Roth, Dan},
doi = {10.3115/1596374.1596399},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ratinov, Roth - 2009 - Design challenges and misconceptions in named entity recognition.pdf:pdf},
isbn = {9781932432299},
issn = {1932432299},
journal = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning CoNLL 09},
number = {June},
pages = {147},
title = {{Design challenges and misconceptions in named entity recognition}},
url = {http://portal.acm.org/citation.cfm?doid=1596374.1596399},
year = {2009}
}
@article{Zhang2015,
abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and LeCun, Yann},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, LeCun - 2015 - Text Understanding from Scratch.pdf:pdf},
isbn = {0123456789},
issn = {2166-532X},
journal = {APL Materials},
number = {1},
pages = {011102},
pmid = {25246403},
title = {{Text Understanding from Scratch}},
url = {http://arxiv.org/abs/1502.01710},
volume = {3},
year = {2015}
}
@article{DeMarneffe2006,
abstract = {This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.},
author = {{De Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
doi = {10.1.1.74.3875},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Marneffe, MacCartney, Manning - 2006 - Generating typed dependency parses from phrase structure parses.pdf:pdf},
journal = {Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006)},
pages = {449--454},
title = {{Generating typed dependency parses from phrase structure parses}},
url = {http://nlp.stanford.edu/pubs/LREC06{\_}dependencies.pdf},
year = {2006}
}
@article{Pyysalo2015,
abstract = {There has been substantial recent interest in annotation schemes that can be applied consistently to many languages. Building on several recent efforts to unify morpho-logical and syntactic annotation, the Uni-versal Dependencies (UD) project seeks to introduce a cross-linguistically appli-cable part-of-speech tagset, feature inven-tory, and set of dependency relations as well as a large number of uniformly an-notated treebanks. We present Univer-sal Dependencies for Finnish, one of the ten languages in the recent first release of UD project treebank data. We detail the mapping of previously introduced annota-tion to the UD standard, describing spe-cific challenges and their resolution. We additionally present parsing experiments comparing the performance of a state-of-the-art parser trained on a language-specific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing tar-get. The introduced tools and resources are available under open licenses from},
author = {Pyysalo, Sampo and Kanerva, Jenna and Missil{\"{a}}, Anna and Laippala, Veronika and Ginter, Filip},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pyysalo et al. - 2015 - Universal Dependencies for Finnish.pdf:pdf},
journal = {Nordic Conference of Computational Linguistics NODALIDA 2015},
number = {Nodalida},
pages = {163},
title = {{Universal Dependencies for Finnish}},
year = {2015}
}
@article{Levy2014,
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Levy, Omer and Goldberg, Yoav},
eprint = {1405.4053},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - 2014 - Neural Word Embedding as Implicit Matrix Factorization.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {2177--2185},
title = {{Neural Word Embedding as Implicit Matrix Factorization}},
url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
year = {2014}
}
@article{Chen2014,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
author = {Chen, Danqi and Manning, Christopher D},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Manning - 2014 - A Fast and Accurate Dependency Parser using Neural Networks.pdf:pdf},
isbn = {9781937284961},
issn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {i},
pages = {740--750},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {https://cs.stanford.edu/{~}danqi/papers/emnlp2014.pdf},
year = {2014}
}
@article{Liang2016a,
abstract = {Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.},
archivePrefix = {arXiv},
arxivId = {1611.00020},
author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
eprint = {1611.00020},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Neural Symbolic Machines Learning Semantic Parsers on Freebase with Weak Supervision(2).pdf:pdf},
number = {October},
title = {{Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision}},
url = {http://arxiv.org/abs/1611.00020},
year = {2016}
}
@article{Li2015,
abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., $\backslash$textit{\{}I don't know{\}}) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in $\backslash$bleu scores on two conversational datasets.},
archivePrefix = {arXiv},
arxivId = {1510.03055},
author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, William B.},
eprint = {1510.03055},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - A Diversity-Promoting Objective Function for Neural Conversation Models.pdf:pdf},
journal = {Arxiv},
number = {Mmi},
pages = {110--119},
title = {{A Diversity-Promoting Objective Function for Neural Conversation Models}},
url = {http://arxiv.org/abs/1510.03055},
year = {2015}
}
@article{Brill1992,
abstract = {Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a sim-ple rule-based part of speech tagger which au-tomatically acquires its rules and tags with ac-curacy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor-pus genre or language to another. Perhaps the biggest contribution of this work is in demon-strating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, search-ing for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9406010},
author = {Brill, Eric},
doi = {10.3115/1075527.1075553},
eprint = {9406010},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brill - 1992 - A Simple Rule-Based Part of Speech Tagger.pdf:pdf},
isbn = {1558602720},
issn = {00992399},
journal = {Applied natural language},
pages = {3},
pmid = {12043861},
primaryClass = {cmp-lg},
title = {{A Simple Rule-Based Part of Speech Tagger}},
year = {1992}
}
@article{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
doi = {10.3115/v1/P14-1023},
eprint = {arXiv:1011.1669v3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni, Dinu, Kruszewski - 2014 - Don't count , predict ! A systematic comparison of context-counting vs . context-predicting semantic v.pdf:pdf},
isbn = {9781937284725},
issn = {1529-1898},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.},
pages = {238--247},
pmid = {25246403},
title = {{Don't count , predict ! A systematic comparison of context-counting vs . context-predicting semantic vectors}},
year = {2014}
}
@article{Nivre2004,
abstract = {Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable},
author = {Nivre, Joakim},
doi = {10.3115/1613148.1613156},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nivre - 2004 - Incrementality in deterministic dependency parsing.pdf:pdf},
journal = {Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together},
pages = {50--57},
title = {{Incrementality in deterministic dependency parsing}},
url = {http://dl.acm.org/citation.cfm?id=1613156},
year = {2004}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Kanerva2014,
abstract = {{\textcopyright} 2014 The Authors and IOS Press. In this paper, we report on the development of a large-scale Finnish Internet parsebank, currently consisting of 1.5 billion tokens in 116 million sentences. The data is fully morphologically and syntactically analyzed and it has been used to extract flat and syntactic n-gram collections, as well as verb-argument and noun-argument n-grams. Additionally, distributional vector space representations of the words are induced using the word2vec method. All n-gram collections as well as the vector space models are made available under an open license.},
author = {Kanerva, Jenna and Luotolahti, Juhani and Laippala, Veronika and Ginter, Filip},
doi = {10.3233/978-1-61499-442-8-184},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva et al. - 2014 - Syntactic N-gram Collection from a Large-Scale Corpus of Internet Finnish.pdf:pdf},
isbn = {9781614994411},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Finnish,large-scale,n-grams,syntactic n-grams,syntactic parsing},
pages = {184--191},
title = {{Syntactic N-gram Collection from a Large-Scale Corpus of Internet Finnish}},
volume = {268},
year = {2014}
}
@article{Nivre2016,
abstract = {Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.},
author = {Nivre, Joakim and Marneffe, Marie-catherine De and Ginter, Filip and Goldberg, Yoav and Manning, Christopher D and Mcdonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nivre et al. - 2016 - Universal Dependencies v1 A Multilingual Treebank Collection.pdf:pdf},
isbn = {978-2-9517408-9-1},
journal = {Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)},
keywords = {annotation,cross-linguistic,dependency,multilingual,treebanks,universal},
pages = {1659--1666},
title = {{Universal Dependencies v1: A Multilingual Treebank Collection}},
year = {2016}
}
@article{Fernandez-Gonzalez2015,
author = {Fern{\'{a}}ndez-Gonz{\'{a}}lez, Daniel and Martins, Andr{\'{e}} F T},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern{\'{a}}ndez-Gonz{\'{a}}lez, Martins - 2015 - Parsing as Reduction.pdf:pdf},
journal = {CoRR},
title = {{Parsing as Reduction}},
url = {http://arxiv.org/abs/1503.00030},
volume = {abs/1503.0},
year = {2015}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Liu2016,
abstract = {We investigate evaluation metrics for end-to-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model's generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.},
archivePrefix = {arXiv},
arxivId = {1603.08023},
author = {Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V. and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
eprint = {1603.08023},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response.pdf:pdf},
journal = {Annual Meeting of the Association for Computational Linguistics (ACL)},
pages = {13},
title = {{How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation}},
url = {http://arxiv.org/abs/1603.08023},
year = {2016}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Yao2015,
abstract = {In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.},
archivePrefix = {arXiv},
arxivId = {1510.08565},
author = {Yao, Kaisheng and Zweig, Geoffrey and Peng, Baolin},
eprint = {1510.08565},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao, Zweig, Peng - 2015 - Attention with Intention for a Neural Network Conversation Model.pdf:pdf},
journal = {NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction},
pages = {1--7},
title = {{Attention with Intention for a Neural Network Conversation Model}},
url = {http://arxiv.org/abs/1510.08565},
year = {2015}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Weiss2015,
abstract = {We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat-ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc-tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26{\%} un-labeled and 92.41{\%} labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.},
archivePrefix = {arXiv},
arxivId = {1506.06158},
author = {Weiss, David and Alberti, Chris and Collins, Michael and Petrov, Slav},
doi = {10.3115/v1/P15-1032},
eprint = {1506.06158},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiss et al. - 2015 - Structured Training for Neural Network Transition-Based Parsing.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
number = {2012},
pages = {323--333},
title = {{Structured Training for Neural Network Transition-Based Parsing}},
url = {http://www.aclweb.org/anthology/P15-1032},
year = {2015}
}
@article{Zhang2012,
author = {Zhang, Hao and McDonald, Ryan},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, McDonald - 2012 - Generalized Higher-Order Dependency Parsing with Cube Pruning.pdf:pdf},
isbn = {9781937284435},
journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {320--331},
title = {{Generalized Higher-Order Dependency Parsing with Cube Pruning}},
url = {http://www.aclweb.org/anthology/D12-1030},
year = {2012}
}
@article{Serban2016a,
abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
archivePrefix = {arXiv},
arxivId = {1507.04808},
author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
eprint = {1507.04808},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Serban et al. - 2016 - Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.pdf:pdf},
journal = {Aaai},
pages = {8},
title = {{Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models}},
url = {http://arxiv.org/abs/1507.04808},
year = {2016}
}
@article{Mikolov2013a,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations{\#}0{\%}5Cnhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
year = {2013}
}
@article{Schuster2016,
abstract = {Many shallow natural language understanding tasks use dependency trees to extract relations between content words. However, strict surface-structure dependency trees tend to follow the linguistic structure of sentences too closely and frequently fail to provide direct relations between content words. To mitigate this problem, the original Stanford Dependencies representation also defines two dependency graph representations which contain additional and augmented relations that explicitly capture otherwise implicit relations between content words. In this paper, we revisit and extend these dependency graph representations in light of the recent Universal Dependencies (UD) initiative and provide a detailed account of an enhanced and an enhanced++ English UD representation. We further present a converter from constituency to basic, i.e., strict surface structure, UD trees, and a converter from basic UD trees to enhanced and enhanced++ English UD graphs. We release both converters as part of Stanford CoreNLP and the Stanford Parser.},
author = {Schuster, Sebastian and Manning, Christopher D.},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuster, Manning - 2016 - Enhanced English Universal Dependencies An Improved Representation for Natural Language Understanding Tasks.pdf:pdf},
journal = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
keywords = {semantic representation,treebank conversion,universal dependencies},
pages = {2371--2378},
title = {{Enhanced English Universal Dependencies: An Improved Representation for Natural Language Understanding Tasks}},
url = {http://nlp.stanford.edu/{~}sebschu/pubs/schuster-manning-lrec2016.pdf},
year = {2016}
}
@article{Ling2015,
abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
archivePrefix = {arXiv},
arxivId = {1508.02096},
author = {Ling, Wang and Luis, Tiago and Marujo, Lu{\'{i}}s Luis and Astudillo, Ramon Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W and Trancoso, Isabel and Fermandez, Ramon and Amir, Silvio and Marujo, Lu{\'{i}}s Luis and Lu{\'{i}}s, Tiago},
doi = {10.18653/v1/D15-1176},
eprint = {1508.02096},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2015 - Finding Function in Form Compositional Character Models for Open Vocabulary Word Representation.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {1520--1530},
title = {{Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation}},
url = {http://dx.doi.org/10.18653/v1/d15-1176{\%}5Cnfile:///Files/68/6810072d-e133-426e-807f-445df2840420.pdf{\%}5Cnpapers3://publication/doi/10.18653/v1/d15-1176{\%}5Cnhttp://arxiv.org/abs/1508.02096},
year = {2015}
}
@article{Petrov2012,
archivePrefix = {arXiv},
arxivId = {1104.2086},
author = {Petrov, Slav and Das, Dipanjan and Mcdonald, Ryan},
eprint = {1104.2086},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrov, Das, Mcdonald - 2012 - A Universal Part-of-Speech Tagset.pdf:pdf},
keywords = {annotation guidelines,multilinguality,part-of-speech tagging},
title = {{A Universal Part-of-Speech Tagset}},
year = {2012}
}
@article{Durrett2013,
abstract = {Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features com-puted from hand-crafted heuristics. In con-trast, we present a state-of-the-art coreference system that captures such phenomena implic-itly, with a small number of homogeneous feature templates examining shallow proper-ties of mentions. Surprisingly, our features are actually more effective than the corre-sponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win " easy victories " without crafted heuris-tics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow seman-tic features from the literature, suggesting that this approach to semantics is an " uphill bat-tle. " Nonetheless, our final system 1 outper-forms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5{\%} absolute on the CoNLL metric and outperforms the IMS system (Bj{\"{o}}rkelund and Farkas (2012), the best publicly available En-glish coreference system) by 1.9{\%} absolute.},
author = {Durrett, Greg and Klein, Dan},
file = {:home/jaakko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Durrett, Klein - 2013 - Easy victories and uphill battles in coreference resolution.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the Conference on Empirical {\ldots}},
number = {October},
pages = {1971--1982},
title = {{Easy victories and uphill battles in coreference resolution}},
url = {http://nlp.cs.berkeley.edu/pubs/Durrett-Klein{\_}2013{\_}Coreference{\_}paper.pdf},
year = {2013}
}
